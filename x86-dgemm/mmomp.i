# 1 "mmomp.c"
# 1 "<built-in>" 1
# 1 "<built-in>" 3
# 395 "<built-in>" 3
# 1 "<command line>" 1
# 1 "<built-in>" 2
# 1 "mmomp.c" 2
# 1 "./dgemm.h" 1
# 1 "/usr/lib/clang/18/include/x86intrin.h" 1 3
# 13 "/usr/lib/clang/18/include/x86intrin.h" 3
# 1 "/usr/lib/clang/18/include/ia32intrin.h" 1 3
# 41 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bsfd(int __A) {
  return __builtin_ctz((unsigned int)__A);
}
# 58 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bsrd(int __A) {
  return 31 - __builtin_clz((unsigned int)__A);
}
# 73 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bswapd(int __A) {
  return (int)__builtin_bswap32((unsigned int)__A);
}
# 88 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
_bswap(int __A) {
  return (int)__builtin_bswap32((unsigned int)__A);
}
# 141 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bsfq(long long __A) {
  return (long long)__builtin_ctzll((unsigned long long)__A);
}
# 157 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bsrq(long long __A) {
  return 63 - __builtin_clzll((unsigned long long)__A);
}
# 173 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__))
__bswapq(long long __A) {
  return (long long)__builtin_bswap64((unsigned long long)__A);
}
# 208 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__popcntd(unsigned int __A)
{
  return __builtin_popcount(__A);
}
# 245 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__))
__popcntq(unsigned long long __A)
{
  return __builtin_popcountll(__A);
}
# 279 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__readeflags(void)
{
  return __builtin_ia32_readeflags_u64();
}
# 294 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__))
__writeeflags(unsigned long long __f)
{
  __builtin_ia32_writeeflags_u64(__f);
}
# 341 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__))
_castf32_u32(float __A) {
  return __builtin_bit_cast(unsigned int, __A);
}
# 356 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__))
_castf64_u64(double __A) {
  return __builtin_bit_cast(unsigned long long, __A);
}
# 371 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ float __attribute__((__always_inline__))
_castu32_f32(unsigned int __A) {
  return __builtin_bit_cast(float, __A);
}
# 386 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ double __attribute__((__always_inline__))
_castu64_f64(unsigned long long __A) {
  return __builtin_bit_cast(double, __A);
}
# 405 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
__crc32b(unsigned int __C, unsigned char __D)
{
  return __builtin_ia32_crc32qi(__C, __D);
}
# 425 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
__crc32w(unsigned int __C, unsigned short __D)
{
  return __builtin_ia32_crc32hi(__C, __D);
}
# 445 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
__crc32d(unsigned int __C, unsigned int __D)
{
  return __builtin_ia32_crc32si(__C, __D);
}
# 466 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
__crc32q(unsigned long long __C, unsigned long long __D)
{
  return __builtin_ia32_crc32di(__C, __D);
}
# 485 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rdpmc(int __A) {
  return __builtin_ia32_rdpmc(__A);
}
# 500 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rdtscp(unsigned int *__A) {
  return __builtin_ia32_rdtscp(__A);
}
# 536 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__))
_wbinvd(void) {
  __builtin_ia32_wbinvd();
}
# 554 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__))
__rolb(unsigned char __X, int __C) {
  return __builtin_rotateleft8(__X, __C);
}
# 572 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__))
__rorb(unsigned char __X, int __C) {
  return __builtin_rotateright8(__X, __C);
}
# 591 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__))
__rolw(unsigned short __X, int __C) {
  return __builtin_rotateleft16(__X, __C);
}
# 610 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__))
__rorw(unsigned short __X, int __C) {
  return __builtin_rotateright16(__X, __C);
}
# 629 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__rold(unsigned int __X, int __C) {
  return __builtin_rotateleft32(__X, (unsigned int)__C);
}
# 648 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__rord(unsigned int __X, int __C) {
  return __builtin_rotateright32(__X, (unsigned int)__C);
}
# 667 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rolq(unsigned long long __X, int __C) {
  return __builtin_rotateleft64(__X, (unsigned long long)__C);
}
# 685 "/usr/lib/clang/18/include/ia32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rorq(unsigned long long __X, int __C) {
  return __builtin_rotateright64(__X, (unsigned long long)__C);
}
# 14 "/usr/lib/clang/18/include/x86intrin.h" 2 3

# 1 "/usr/lib/clang/18/include/immintrin.h" 1 3
# 17 "/usr/lib/clang/18/include/immintrin.h" 3
# 1 "/usr/lib/clang/18/include/x86gprintrin.h" 1 3
# 15 "/usr/lib/clang/18/include/x86gprintrin.h" 3
# 1 "/usr/lib/clang/18/include/hresetintrin.h" 1 3
# 39 "/usr/lib/clang/18/include/hresetintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("hreset")))
_hreset(int __eax)
{
  __asm__ ("hreset $0" :: "a"(__eax));
}
# 16 "/usr/lib/clang/18/include/x86gprintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/uintrintrin.h" 1 3
# 23 "/usr/lib/clang/18/include/uintrintrin.h" 3
struct __uintr_frame
{
  unsigned long long rip;
  unsigned long long rflags;
  unsigned long long rsp;
};
# 45 "/usr/lib/clang/18/include/uintrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("uintr")))
_clui (void)
{
  __builtin_ia32_clui();
}
# 66 "/usr/lib/clang/18/include/uintrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("uintr")))
_stui (void)
{
  __builtin_ia32_stui();
}
# 93 "/usr/lib/clang/18/include/uintrintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("uintr")))
_testui (void)
{
  return __builtin_ia32_testui();
}
# 147 "/usr/lib/clang/18/include/uintrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("uintr")))
_senduipi (unsigned long long __a)
{
  __builtin_ia32_senduipi(__a);
}
# 21 "/usr/lib/clang/18/include/x86gprintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/usermsrintrin.h" 1 3
# 26 "/usr/lib/clang/18/include/usermsrintrin.h" 3
static __inline__ unsigned long long
    __attribute__((__always_inline__, __nodebug__, __target__("usermsr")))
    _urdmsr(unsigned long long __A) {
  return __builtin_ia32_urdmsr(__A);
}
# 44 "/usr/lib/clang/18/include/usermsrintrin.h" 3
static __inline__ void
    __attribute__((__always_inline__, __nodebug__, __target__("usermsr")))
    _uwrmsr(unsigned long long __A, unsigned long long __B) {
  return __builtin_ia32_uwrmsr(__A, __B);
}
# 26 "/usr/lib/clang/18/include/x86gprintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/crc32intrin.h" 1 3
# 30 "/usr/lib/clang/18/include/crc32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
_mm_crc32_u8(unsigned int __C, unsigned char __D)
{
  return __builtin_ia32_crc32qi(__C, __D);
}
# 50 "/usr/lib/clang/18/include/crc32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
_mm_crc32_u16(unsigned int __C, unsigned short __D)
{
  return __builtin_ia32_crc32hi(__C, __D);
}
# 70 "/usr/lib/clang/18/include/crc32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
_mm_crc32_u32(unsigned int __C, unsigned int __D)
{
  return __builtin_ia32_crc32si(__C, __D);
}
# 91 "/usr/lib/clang/18/include/crc32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
_mm_crc32_u64(unsigned long long __C, unsigned long long __D)
{
  return __builtin_ia32_crc32di(__C, __D);
}
# 31 "/usr/lib/clang/18/include/x86gprintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/prfchiintrin.h" 1 3
# 31 "/usr/lib/clang/18/include/prfchiintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("prefetchi")))
_m_prefetchit0(volatile const void *__P) {
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wcast-qual"
  __builtin_ia32_prefetchi((const void *)__P, 3 );
#pragma clang diagnostic pop
}
# 51 "/usr/lib/clang/18/include/prfchiintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("prefetchi")))
_m_prefetchit1(volatile const void *__P) {
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wcast-qual"
  __builtin_ia32_prefetchi((const void *)__P, 2 );
#pragma clang diagnostic pop
}
# 36 "/usr/lib/clang/18/include/x86gprintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/raointintrin.h" 1 3
# 38 "/usr/lib/clang/18/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aadd_i32(int *__A, int __B) {
  __builtin_ia32_aadd32((int *)__A, __B);
}
# 60 "/usr/lib/clang/18/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aand_i32(int *__A, int __B) {
  __builtin_ia32_aand32((int *)__A, __B);
}
# 82 "/usr/lib/clang/18/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aor_i32(int *__A, int __B) {
  __builtin_ia32_aor32((int *)__A, __B);
}
# 104 "/usr/lib/clang/18/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _axor_i32(int *__A, int __B) {
  __builtin_ia32_axor32((int *)__A, __B);
}
# 127 "/usr/lib/clang/18/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aadd_i64(long long *__A,
                                                    long long __B) {
  __builtin_ia32_aadd64((long long *)__A, __B);
}
# 150 "/usr/lib/clang/18/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aand_i64(long long *__A,
                                                    long long __B) {
  __builtin_ia32_aand64((long long *)__A, __B);
}
# 173 "/usr/lib/clang/18/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aor_i64(long long *__A,
                                                   long long __B) {
  __builtin_ia32_aor64((long long *)__A, __B);
}
# 196 "/usr/lib/clang/18/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _axor_i64(long long *__A,
                                                    long long __B) {
  __builtin_ia32_axor64((long long *)__A, __B);
}
# 41 "/usr/lib/clang/18/include/x86gprintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/cmpccxaddintrin.h" 1 3
# 19 "/usr/lib/clang/18/include/cmpccxaddintrin.h" 3
typedef enum {
  _CMPCCX_O,
  _CMPCCX_NO,
  _CMPCCX_B,
  _CMPCCX_NB,
  _CMPCCX_Z,
  _CMPCCX_NZ,
  _CMPCCX_BE,
  _CMPCCX_NBE,
  _CMPCCX_S,
  _CMPCCX_NS,
  _CMPCCX_P,
  _CMPCCX_NP,
  _CMPCCX_L,
  _CMPCCX_NL,
  _CMPCCX_LE,
  _CMPCCX_NLE,
} _CMPCCX_ENUM;
# 46 "/usr/lib/clang/18/include/x86gprintrin.h" 2 3
# 18 "/usr/lib/clang/18/include/immintrin.h" 2 3



# 1 "/usr/lib/clang/18/include/mmintrin.h" 1 3
# 17 "/usr/lib/clang/18/include/mmintrin.h" 3
typedef long long __m64 __attribute__((__vector_size__(8), __aligned__(8)));

typedef long long __v1di __attribute__((__vector_size__(8)));
typedef int __v2si __attribute__((__vector_size__(8)));
typedef short __v4hi __attribute__((__vector_size__(8)));
typedef char __v8qi __attribute__((__vector_size__(8)));
# 36 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__,
                                      __target__("mmx,no-evex512")))
_mm_empty(void) {
  __builtin_ia32_emms();
}
# 53 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cvtsi32_si64(int __i)
{
    return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);
}
# 70 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cvtsi64_si32(__m64 __m)
{
    return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);
}
# 86 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cvtsi64_m64(long long __i)
{
    return (__m64)__i;
}
# 102 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cvtm64_si64(__m64 __m)
{
    return (long long)__m;
}
# 132 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_packs_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);
}
# 162 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_packs_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);
}
# 192 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_packs_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);
}
# 219 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_unpackhi_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);
}
# 242 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_unpackhi_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);
}
# 263 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_unpackhi_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);
}
# 290 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_unpacklo_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);
}
# 313 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_unpacklo_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);
}
# 334 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_unpacklo_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);
}
# 355 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_add_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
}
# 376 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_add_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
}
# 397 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_add_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
}
# 419 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_adds_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddsb((__v8qi)__m1, (__v8qi)__m2);
}
# 442 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_adds_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddsw((__v4hi)__m1, (__v4hi)__m2);
}
# 464 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_adds_pu8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddusb((__v8qi)__m1, (__v8qi)__m2);
}
# 486 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_adds_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddusw((__v4hi)__m1, (__v4hi)__m2);
}
# 507 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_sub_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubb((__v8qi)__m1, (__v8qi)__m2);
}
# 528 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_sub_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubw((__v4hi)__m1, (__v4hi)__m2);
}
# 549 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_sub_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubd((__v2si)__m1, (__v2si)__m2);
}
# 572 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_subs_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubsb((__v8qi)__m1, (__v8qi)__m2);
}
# 595 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_subs_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubsw((__v4hi)__m1, (__v4hi)__m2);
}
# 619 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_subs_pu8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubusb((__v8qi)__m1, (__v8qi)__m2);
}
# 643 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_subs_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubusw((__v4hi)__m1, (__v4hi)__m2);
}
# 670 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_madd_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmaddwd((__v4hi)__m1, (__v4hi)__m2);
}
# 691 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_mulhi_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmulhw((__v4hi)__m1, (__v4hi)__m2);
}
# 712 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_mullo_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmullw((__v4hi)__m1, (__v4hi)__m2);
}
# 735 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_sll_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psllw((__v4hi)__m, __count);
}
# 757 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_slli_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psllwi((__v4hi)__m, __count);
}
# 780 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_sll_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_pslld((__v2si)__m, __count);
}
# 802 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_slli_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_pslldi((__v2si)__m, __count);
}
# 822 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_sll_si64(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psllq((__v1di)__m, __count);
}
# 842 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_slli_si64(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psllqi((__v1di)__m, __count);
}
# 866 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_sra_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psraw((__v4hi)__m, __count);
}
# 889 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_srai_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrawi((__v4hi)__m, __count);
}
# 913 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_sra_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrad((__v2si)__m, __count);
}
# 936 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_srai_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psradi((__v2si)__m, __count);
}
# 959 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_srl_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrlw((__v4hi)__m, __count);
}
# 981 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_srli_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrlwi((__v4hi)__m, __count);
}
# 1004 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_srl_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrld((__v2si)__m, __count);
}
# 1026 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_srli_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrldi((__v2si)__m, __count);
}
# 1046 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_srl_si64(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrlq((__v1di)__m, __count);
}
# 1067 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_srli_si64(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrlqi((__v1di)__m, __count);
}
# 1085 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_and_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pand((__v1di)__m1, (__v1di)__m2);
}
# 1106 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_andnot_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pandn((__v1di)__m1, (__v1di)__m2);
}
# 1124 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_or_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_por((__v1di)__m1, (__v1di)__m2);
}
# 1142 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_xor_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pxor((__v1di)__m1, (__v1di)__m2);
}
# 1164 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cmpeq_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqb((__v8qi)__m1, (__v8qi)__m2);
}
# 1186 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cmpeq_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqw((__v4hi)__m1, (__v4hi)__m2);
}
# 1208 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cmpeq_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqd((__v2si)__m1, (__v2si)__m2);
}
# 1230 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cmpgt_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtb((__v8qi)__m1, (__v8qi)__m2);
}
# 1252 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cmpgt_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtw((__v4hi)__m1, (__v4hi)__m2);
}
# 1274 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_cmpgt_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtd((__v2si)__m1, (__v2si)__m2);
}
# 1287 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_setzero_si64(void)
{
    return __extension__ (__m64){ 0LL };
}
# 1308 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_set_pi32(int __i1, int __i0)
{
    return (__m64)__builtin_ia32_vec_init_v2si(__i0, __i1);
}
# 1331 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_set_pi16(short __s3, short __s2, short __s1, short __s0)
{
    return (__m64)__builtin_ia32_vec_init_v4hi(__s0, __s1, __s2, __s3);
}
# 1362 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_set_pi8(char __b7, char __b6, char __b5, char __b4, char __b3, char __b2,
            char __b1, char __b0)
{
    return (__m64)__builtin_ia32_vec_init_v8qi(__b0, __b1, __b2, __b3,
                                               __b4, __b5, __b6, __b7);
}
# 1383 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_set1_pi32(int __i)
{
    return _mm_set_pi32(__i, __i);
}
# 1402 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_set1_pi16(short __w)
{
    return _mm_set_pi16(__w, __w, __w, __w);
}
# 1420 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_set1_pi8(char __b)
{
    return _mm_set_pi8(__b, __b, __b, __b, __b, __b, __b, __b);
}
# 1441 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_setr_pi32(int __i0, int __i1)
{
    return _mm_set_pi32(__i1, __i0);
}
# 1464 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_setr_pi16(short __w0, short __w1, short __w2, short __w3)
{
    return _mm_set_pi16(__w3, __w2, __w1, __w0);
}
# 1495 "/usr/lib/clang/18/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,no-evex512"), __min_vector_width__(64)))
_mm_setr_pi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5,
             char __b6, char __b7)
{
    return _mm_set_pi8(__b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);
}
# 22 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/xmmintrin.h" 1 3
# 19 "/usr/lib/clang/18/include/xmmintrin.h" 3
typedef int __v4si __attribute__((__vector_size__(16)));
typedef float __v4sf __attribute__((__vector_size__(16)));
typedef float __m128 __attribute__((__vector_size__(16), __aligned__(16)));

typedef float __m128_u __attribute__((__vector_size__(16), __aligned__(1)));


typedef unsigned int __v4su __attribute__((__vector_size__(16)));




# 1 "/usr/lib/clang/18/include/mm_malloc.h" 1 3
# 13 "/usr/lib/clang/18/include/mm_malloc.h" 3
# 1 "/usr/include/stdlib.h" 1 3 4
# 37 "/usr/include/stdlib.h" 3 4
# 1 "/usr/include/sys/cdefs.h" 1 3 4
# 38 "/usr/include/stdlib.h" 2 3 4
# 1 "/usr/include/sys/_null.h" 1 3 4
# 39 "/usr/include/stdlib.h" 2 3 4
# 1 "/usr/include/sys/_types.h" 1 3 4
# 43 "/usr/include/sys/_types.h" 3 4
typedef signed char __int8_t;
typedef unsigned char __uint8_t;
typedef short __int16_t;
typedef unsigned short __uint16_t;
typedef int __int32_t;
typedef unsigned int __uint32_t;

typedef long __int64_t;
typedef unsigned long __uint64_t;
# 61 "/usr/include/sys/_types.h" 3 4
typedef __int8_t __int_least8_t;
typedef __int16_t __int_least16_t;
typedef __int32_t __int_least32_t;
typedef __int64_t __int_least64_t;
typedef __int64_t __intmax_t;
typedef __uint8_t __uint_least8_t;
typedef __uint16_t __uint_least16_t;
typedef __uint32_t __uint_least32_t;
typedef __uint64_t __uint_least64_t;
typedef __uint64_t __uintmax_t;


typedef __int64_t __intptr_t;
typedef __int64_t __intfptr_t;
typedef __uint64_t __uintptr_t;
typedef __uint64_t __uintfptr_t;
typedef __uint64_t __vm_offset_t;
typedef __uint64_t __vm_size_t;
# 91 "/usr/include/sys/_types.h" 3 4
typedef __uint64_t __size_t;
typedef __int64_t __ssize_t;
# 101 "/usr/include/sys/_types.h" 3 4
typedef __int64_t __ptrdiff_t;
# 111 "/usr/include/sys/_types.h" 3 4
# 1 "/usr/include/machine/_types.h" 1 3 4




# 1 "/usr/include/x86/_types.h" 1 3 4
# 47 "/usr/include/x86/_types.h" 3 4
# 1 "/usr/include/machine/_limits.h" 1 3 4




# 1 "/usr/include/x86/_limits.h" 1 3 4
# 6 "/usr/include/machine/_limits.h" 2 3 4
# 48 "/usr/include/x86/_types.h" 2 3 4







typedef __int32_t __clock_t;
typedef __int64_t __critical_t;

typedef double __double_t;
typedef float __float_t;
# 69 "/usr/include/x86/_types.h" 3 4
typedef __int32_t __int_fast8_t;
typedef __int32_t __int_fast16_t;
typedef __int32_t __int_fast32_t;
typedef __int64_t __int_fast64_t;

typedef __int64_t __register_t;
typedef __int64_t __segsz_t;
typedef __int64_t __time_t;





typedef __uint32_t __uint_fast8_t;
typedef __uint32_t __uint_fast16_t;
typedef __uint32_t __uint_fast32_t;
typedef __uint64_t __uint_fast64_t;

typedef __uint64_t __u_register_t;
typedef __uint64_t __vm_paddr_t;




typedef int ___wchar_t;
# 6 "/usr/include/machine/_types.h" 2 3 4
# 112 "/usr/include/sys/_types.h" 2 3 4




typedef __int32_t __blksize_t;
typedef __int64_t __blkcnt_t;
typedef __int32_t __clockid_t;
typedef __uint32_t __fflags_t;
typedef __uint64_t __fsblkcnt_t;
typedef __uint64_t __fsfilcnt_t;
typedef __uint32_t __gid_t;
typedef __int64_t __id_t;
typedef __uint64_t __ino_t;
typedef long __key_t;
typedef __int32_t __lwpid_t;
typedef __uint16_t __mode_t;
typedef int __accmode_t;
typedef int __nl_item;
typedef __uint64_t __nlink_t;
typedef __int64_t __off_t;
typedef __int64_t __off64_t;
typedef __int32_t __pid_t;
typedef __int64_t __sbintime_t;
typedef __int64_t __rlim_t;


typedef __uint8_t __sa_family_t;
typedef __uint32_t __socklen_t;
typedef long __suseconds_t;
typedef struct __timer *__timer_t;
typedef struct __mq *__mqd_t;
typedef __uint32_t __uid_t;
typedef unsigned int __useconds_t;
typedef int __cpuwhich_t;
typedef int __cpulevel_t;
typedef int __cpusetid_t;
typedef __int64_t __daddr_t;
# 168 "/usr/include/sys/_types.h" 3 4
typedef int __ct_rune_t;
typedef __ct_rune_t __rune_t;
typedef __ct_rune_t __wint_t;



typedef __uint_least16_t __char16_t;
typedef __uint_least32_t __char32_t;







typedef struct {
 long long __max_align1 __attribute__((__aligned__(_Alignof(long long))));

 long double __max_align2 __attribute__((__aligned__(_Alignof(long double))));

} __max_align_t;

typedef __uint64_t __dev_t;

typedef __uint32_t __fixpt_t;





typedef union {
 char __mbstate8[128];
 __int64_t _mbstateL;
} __mbstate_t;

typedef __uintmax_t __rman_res_t;





typedef __builtin_va_list __va_list;


typedef __va_list __gnuc_va_list;
# 40 "/usr/include/stdlib.h" 2 3 4

#pragma clang diagnostic push
# 41 "/usr/include/stdlib.h" 3 4
#pragma clang diagnostic ignored "-Wnullability-completeness"



typedef __rune_t rune_t;





typedef __size_t size_t;





typedef ___wchar_t wchar_t;




typedef struct {
 int quot;
 int rem;
} div_t;

typedef struct {
 long quot;
 long rem;
} ldiv_t;
# 85 "/usr/include/stdlib.h" 3 4
extern int __mb_cur_max;
extern int ___mb_cur_max(void);


_Noreturn void abort(void);
int abs(int) __attribute__((__const__));
int atexit(void (* _Nonnull)(void));
double atof(const char *);
int atoi(const char *);
long atol(const char *);
void *bsearch(const void *, const void *, size_t,
     size_t, int (*)(const void * _Nonnull, const void *));
void *calloc(size_t, size_t) __attribute__((__malloc__)) __attribute__((__warn_unused_result__))
      __attribute__((__alloc_size__(1, 2)));
div_t div(int, int) __attribute__((__const__));
_Noreturn void exit(int);
void free(void *);
char *getenv(const char *);
long labs(long) __attribute__((__const__));
ldiv_t ldiv(long, long) __attribute__((__const__));
void *malloc(size_t) __attribute__((__malloc__)) __attribute__((__warn_unused_result__)) __attribute__((__alloc_size__(1)));
int mblen(const char *, size_t);
size_t mbstowcs(wchar_t * restrict , const char * restrict, size_t);
int mbtowc(wchar_t * restrict, const char * restrict, size_t);
void qsort(void *, size_t, size_t,
     int (* _Nonnull)(const void *, const void *));
int rand(void);
void *realloc(void *, size_t) __attribute__((__warn_unused_result__)) __attribute__((__alloc_size__(2)));
void srand(unsigned);
double strtod(const char * restrict, char ** restrict);
float strtof(const char * restrict, char ** restrict);
long strtol(const char * restrict, char ** restrict, int);
long double
  strtold(const char * restrict, char ** restrict);
unsigned long
  strtoul(const char * restrict, char ** restrict, int);
int system(const char *);
int wctomb(char *, wchar_t);
size_t wcstombs(char * restrict, const wchar_t * restrict, size_t);
# 138 "/usr/include/stdlib.h" 3 4
typedef struct {
 long long quot;
 long long rem;
} lldiv_t;


long long
  atoll(const char *);

long long
  llabs(long long) __attribute__((__const__));

lldiv_t lldiv(long long, long long) __attribute__((__const__));

long long
  strtoll(const char * restrict, char ** restrict, int);

unsigned long long
  strtoull(const char * restrict, char ** restrict, int);


_Noreturn void _Exit(int);






void * aligned_alloc(size_t, size_t) __attribute__((__malloc__)) __attribute__((__alloc_align__(1)))
     __attribute__((__alloc_size__(2)));
int at_quick_exit(void (*)(void));
_Noreturn void
 quick_exit(int);





char *realpath(const char * restrict, char * restrict);


int rand_r(unsigned *);


int posix_memalign(void **, size_t, size_t);
int setenv(const char *, const char *, int);
int unsetenv(const char *);



int getsubopt(char **, char *const *, char **);

char *mkdtemp(char *);



int mkstemp(char *);
# 207 "/usr/include/stdlib.h" 3 4
long a64l(const char *);
double drand48(void);

double erand48(unsigned short[3]);


char *initstate(unsigned int, char *, size_t);
long jrand48(unsigned short[3]);
char *l64a(long);
void lcong48(unsigned short[7]);
long lrand48(void);

char *mktemp(char *);


long mrand48(void);
long nrand48(unsigned short[3]);
int putenv(char *);
long random(void);
unsigned short
 *seed48(unsigned short[3]);
char *setstate( char *);
void srand48(long);
void srandom(unsigned int);



int grantpt(int);
int posix_openpt(int);
char *ptsname(int);
int unlockpt(int);



int ptsname_r(int, char *, size_t);



extern const char *malloc_conf;
extern void (*malloc_message)(void *, const char *);
# 260 "/usr/include/stdlib.h" 3 4
void abort2(const char *, int, void **) __attribute__((__noreturn__));
__uint32_t
  arc4random(void);
void arc4random_buf(void *, size_t);
__uint32_t
  arc4random_uniform(__uint32_t);






char *getbsize(int *, long *);

char *cgetcap(char *, const char *, int);
int cgetclose(void);
int cgetent(char **, char **, const char *);
int cgetfirst(char **, char **);
int cgetmatch(const char *, const char *);
int cgetnext(char **, char **);
int cgetnum(char *, const char *, long *);
int cgetset(const char *);
int cgetstr(char *, const char *, char **);
int cgetustr(char *, const char *, char **);

int clearenv(void);

int daemon(int, int);
int daemonfd(int, int);
char *devname(__dev_t, __mode_t);
char *devname_r(__dev_t, __mode_t, char *, int);
char *fdevname(int);
char *fdevname_r(int, char *, int);
int getloadavg(double [], int);
const char *
  getprogname(void);

int heapsort(void *, size_t, size_t,
     int (* _Nonnull)(const void *, const void *));






int l64a_r(long, char *, int);
int mergesort(void *, size_t, size_t, int (*)(const void *, const void *));



int mkostemp(char *, int);
int mkostemps(char *, int, int);
int mkostempsat(int, char *, int, int);
void qsort_r(void *, size_t, size_t,
     int (*)(const void *, const void *, void *), void *);
int radixsort(const unsigned char **, int, const unsigned char *,
     unsigned);
void *reallocarray(void *, size_t, size_t) __attribute__((__warn_unused_result__))
     __attribute__((__alloc_size__(2, 3)));
void *reallocf(void *, size_t) __attribute__((__warn_unused_result__)) __attribute__((__alloc_size__(2)));
int rpmatch(const char *);
char *secure_getenv(const char *);
void setprogname(const char *);
int sradixsort(const unsigned char **, int, const unsigned char *,
     unsigned);
void srandomdev(void);
long long
 strtonum(const char *, long long, long long, const char **);


__int64_t
  strtoq(const char *, char **, int);
__uint64_t
  strtouq(const char *, char **, int);
# 350 "/usr/include/stdlib.h" 3 4
void __qsort_r_compat(void *, size_t, size_t, void *,
     int (*)(void *, const void *, const void *));
__asm__(".symver " "__qsort_r_compat" ", " "qsort_r" "@" "FBSD_1.0");
# 369 "/usr/include/stdlib.h" 3 4
extern char *suboptarg;






typedef size_t rsize_t;




typedef int errno_t;



typedef void (*constraint_handler_t)(const char * restrict,
    void * restrict, errno_t);

constraint_handler_t set_constraint_handler_s(constraint_handler_t handler);

_Noreturn void abort_handler_s(const char * restrict, void * restrict,
    errno_t);

void ignore_handler_s(const char * restrict, void * restrict, errno_t);

errno_t qsort_s(void *, rsize_t, rsize_t,
    int (*)(const void *, const void *, void *), void *);



#pragma clang diagnostic pop
# 14 "/usr/lib/clang/18/include/mm_malloc.h" 2 3





extern int posix_memalign(void **__memptr, size_t __alignment, size_t __size);
# 30 "/usr/lib/clang/18/include/mm_malloc.h" 3
static __inline__ void *__attribute__((__always_inline__, __nodebug__,
                                       __malloc__, __alloc_size__(1),
                                       __alloc_align__(2)))
_mm_malloc(size_t __size, size_t __align) {
  if (__align == 1) {
    return malloc(__size);
  }

  if (!(__align & (__align - 1)) && __align < sizeof(void *))
    __align = sizeof(void *);

  void *__mallocedMemory;





  if (posix_memalign(&__mallocedMemory, __align, __size))
    return 0;


  return __mallocedMemory;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_free(void *__p)
{





  free(__p);

}
# 32 "/usr/lib/clang/18/include/xmmintrin.h" 2 3
# 57 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_add_ss(__m128 __a, __m128 __b)
{
  __a[0] += __b[0];
  return __a;
}
# 77 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_add_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a + (__v4sf)__b);
}
# 99 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_sub_ss(__m128 __a, __m128 __b)
{
  __a[0] -= __b[0];
  return __a;
}
# 120 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_sub_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a - (__v4sf)__b);
}
# 142 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_mul_ss(__m128 __a, __m128 __b)
{
  __a[0] *= __b[0];
  return __a;
}
# 162 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_mul_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a * (__v4sf)__b);
}
# 184 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_div_ss(__m128 __a, __m128 __b)
{
  __a[0] /= __b[0];
  return __a;
}
# 203 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_div_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a / (__v4sf)__b);
}
# 221 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_sqrt_ss(__m128 __a)
{
  return (__m128)__builtin_ia32_sqrtss((__v4sf)__a);
}
# 238 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_sqrt_ps(__m128 __a)
{
  return __builtin_ia32_sqrtps((__v4sf)__a);
}
# 256 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_rcp_ss(__m128 __a)
{
  return (__m128)__builtin_ia32_rcpss((__v4sf)__a);
}
# 273 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_rcp_ps(__m128 __a)
{
  return (__m128)__builtin_ia32_rcpps((__v4sf)__a);
}
# 292 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_rsqrt_ss(__m128 __a)
{
  return __builtin_ia32_rsqrtss((__v4sf)__a);
}
# 309 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_rsqrt_ps(__m128 __a)
{
  return __builtin_ia32_rsqrtps((__v4sf)__a);
}
# 332 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_min_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_minss((__v4sf)__a, (__v4sf)__b);
}
# 351 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_min_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_minps((__v4sf)__a, (__v4sf)__b);
}
# 374 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_max_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_maxss((__v4sf)__a, (__v4sf)__b);
}
# 393 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_max_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_maxps((__v4sf)__a, (__v4sf)__b);
}
# 411 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_and_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4su)__a & (__v4su)__b);
}
# 433 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_andnot_ps(__m128 __a, __m128 __b)
{
  return (__m128)(~(__v4su)__a & (__v4su)__b);
}
# 451 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_or_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4su)__a | (__v4su)__b);
}
# 470 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_xor_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4su)__a ^ (__v4su)__b);
}
# 492 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpeq_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpeqss((__v4sf)__a, (__v4sf)__b);
}
# 510 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpeq_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpeqps((__v4sf)__a, (__v4sf)__b);
}
# 533 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmplt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpltss((__v4sf)__a, (__v4sf)__b);
}
# 552 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmplt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpltps((__v4sf)__a, (__v4sf)__b);
}
# 576 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmple_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpless((__v4sf)__a, (__v4sf)__b);
}
# 595 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmple_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpleps((__v4sf)__a, (__v4sf)__b);
}
# 618 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpgt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpltss((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}
# 639 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpgt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpltps((__v4sf)__b, (__v4sf)__a);
}
# 663 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpge_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpless((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}
# 684 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpge_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpleps((__v4sf)__b, (__v4sf)__a);
}
# 707 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpneq_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpneqss((__v4sf)__a, (__v4sf)__b);
}
# 726 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpneq_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpneqps((__v4sf)__a, (__v4sf)__b);
}
# 750 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpnlt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnltss((__v4sf)__a, (__v4sf)__b);
}
# 770 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpnlt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnltps((__v4sf)__a, (__v4sf)__b);
}
# 795 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpnle_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnless((__v4sf)__a, (__v4sf)__b);
}
# 815 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpnle_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnleps((__v4sf)__a, (__v4sf)__b);
}
# 840 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpngt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpnltss((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}
# 862 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpngt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnltps((__v4sf)__b, (__v4sf)__a);
}
# 887 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpnge_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpnless((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}
# 909 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpnge_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnleps((__v4sf)__b, (__v4sf)__a);
}
# 934 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpord_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpordss((__v4sf)__a, (__v4sf)__b);
}
# 954 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpord_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpordps((__v4sf)__a, (__v4sf)__b);
}
# 979 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpunord_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpunordss((__v4sf)__a, (__v4sf)__b);
}
# 999 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cmpunord_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpunordps((__v4sf)__a, (__v4sf)__b);
}
# 1023 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_comieq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comieq((__v4sf)__a, (__v4sf)__b);
}
# 1048 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_comilt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comilt((__v4sf)__a, (__v4sf)__b);
}
# 1072 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_comile_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comile((__v4sf)__a, (__v4sf)__b);
}
# 1096 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_comigt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comigt((__v4sf)__a, (__v4sf)__b);
}
# 1120 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_comige_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comige((__v4sf)__a, (__v4sf)__b);
}
# 1144 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_comineq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comineq((__v4sf)__a, (__v4sf)__b);
}
# 1168 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_ucomieq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomieq((__v4sf)__a, (__v4sf)__b);
}
# 1192 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_ucomilt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomilt((__v4sf)__a, (__v4sf)__b);
}
# 1217 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_ucomile_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomile((__v4sf)__a, (__v4sf)__b);
}
# 1242 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_ucomigt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomigt((__v4sf)__a, (__v4sf)__b);
}
# 1267 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_ucomige_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomige((__v4sf)__a, (__v4sf)__b);
}
# 1291 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_ucomineq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomineq((__v4sf)__a, (__v4sf)__b);
}
# 1309 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvtss_si32(__m128 __a)
{
  return __builtin_ia32_cvtss2si((__v4sf)__a);
}
# 1327 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvt_ss2si(__m128 __a)
{
  return _mm_cvtss_si32(__a);
}
# 1347 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvtss_si64(__m128 __a)
{
  return __builtin_ia32_cvtss2si64((__v4sf)__a);
}
# 1365 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtps_pi32(__m128 __a)
{
  return (__m64)__builtin_ia32_cvtps2pi((__v4sf)__a);
}
# 1381 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvt_ps2pi(__m128 __a)
{
  return _mm_cvtps_pi32(__a);
}
# 1400 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvttss_si32(__m128 __a)
{
  return __builtin_ia32_cvttss2si((__v4sf)__a);
}
# 1419 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvtt_ss2si(__m128 __a)
{
  return _mm_cvttss_si32(__a);
}
# 1439 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvttss_si64(__m128 __a)
{
  return __builtin_ia32_cvttss2si64((__v4sf)__a);
}
# 1458 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvttps_pi32(__m128 __a)
{
  return (__m64)__builtin_ia32_cvttps2pi((__v4sf)__a);
}
# 1475 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtt_ps2pi(__m128 __a)
{
  return _mm_cvttps_pi32(__a);
}
# 1497 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvtsi32_ss(__m128 __a, int __b)
{
  __a[0] = __b;
  return __a;
}
# 1520 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvt_si2ss(__m128 __a, int __b)
{
  return _mm_cvtsi32_ss(__a, __b);
}
# 1544 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvtsi64_ss(__m128 __a, long long __b)
{
  __a[0] = __b;
  return __a;
}
# 1570 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtpi32_ps(__m128 __a, __m64 __b)
{
  return __builtin_ia32_cvtpi2ps((__v4sf)__a, (__v2si)__b);
}
# 1593 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvt_pi2ps(__m128 __a, __m64 __b)
{
  return _mm_cvtpi32_ps(__a, __b);
}
# 1610 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_cvtss_f32(__m128 __a)
{
  return __a[0];
}
# 1631 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_loadh_pi(__m128 __a, const __m64 *__p)
{
  typedef float __mm_loadh_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_loadh_pi_struct {
    __mm_loadh_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  __mm_loadh_pi_v2f32 __b = ((const struct __mm_loadh_pi_struct*)__p)->__u;
  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);
  return __builtin_shufflevector(__a, __bb, 0, 1, 4, 5);
}
# 1658 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_loadl_pi(__m128 __a, const __m64 *__p)
{
  typedef float __mm_loadl_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_loadl_pi_struct {
    __mm_loadl_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  __mm_loadl_pi_v2f32 __b = ((const struct __mm_loadl_pi_struct*)__p)->__u;
  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);
  return __builtin_shufflevector(__a, __bb, 4, 5, 2, 3);
}
# 1685 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_load_ss(const float *__p)
{
  struct __mm_load_ss_struct {
    float __u;
  } __attribute__((__packed__, __may_alias__));
  float __u = ((const struct __mm_load_ss_struct*)__p)->__u;
  return __extension__ (__m128){ __u, 0, 0, 0 };
}
# 1707 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_load1_ps(const float *__p)
{
  struct __mm_load1_ps_struct {
    float __u;
  } __attribute__((__packed__, __may_alias__));
  float __u = ((const struct __mm_load1_ps_struct*)__p)->__u;
  return __extension__ (__m128){ __u, __u, __u, __u };
}
# 1730 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_load_ps(const float *__p)
{
  return *(const __m128*)__p;
}
# 1747 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_loadu_ps(const float *__p)
{
  struct __loadu_ps {
    __m128_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ps*)__p)->__v;
}
# 1769 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_loadr_ps(const float *__p)
{
  __m128 __a = _mm_load_ps(__p);
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 3, 2, 1, 0);
}
# 1783 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_undefined_ps(void)
{
  return (__m128)__builtin_ia32_undef128();
}
# 1803 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_set_ss(float __w)
{
  return __extension__ (__m128){ __w, 0, 0, 0 };
}
# 1821 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_set1_ps(float __w)
{
  return __extension__ (__m128){ __w, __w, __w, __w };
}
# 1840 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_set_ps1(float __w)
{
    return _mm_set1_ps(__w);
}
# 1867 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_set_ps(float __z, float __y, float __x, float __w)
{
  return __extension__ (__m128){ __w, __x, __y, __z };
}
# 1895 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_setr_ps(float __z, float __y, float __x, float __w)
{
  return __extension__ (__m128){ __z, __y, __x, __w };
}
# 1910 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_setzero_ps(void)
{
  return __extension__ (__m128){ 0.0f, 0.0f, 0.0f, 0.0f };
}
# 1927 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_storeh_pi(__m64 *__p, __m128 __a)
{
  typedef float __mm_storeh_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_storeh_pi_struct {
    __mm_storeh_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pi_struct*)__p)->__u = __builtin_shufflevector(__a, __a, 2, 3);
}
# 1948 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_storel_pi(__m64 *__p, __m128 __a)
{
  typedef float __mm_storeh_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_storeh_pi_struct {
    __mm_storeh_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pi_struct*)__p)->__u = __builtin_shufflevector(__a, __a, 0, 1);
}
# 1969 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_store_ss(float *__p, __m128 __a)
{
  struct __mm_store_ss_struct {
    float __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store_ss_struct*)__p)->__u = __a[0];
}
# 1990 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_storeu_ps(float *__p, __m128 __a)
{
  struct __storeu_ps {
    __m128_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ps*)__p)->__v = __a;
}
# 2011 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_store_ps(float *__p, __m128 __a)
{
  *(__m128*)__p = __a;
}
# 2030 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_store1_ps(float *__p, __m128 __a)
{
  __a = __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 0, 0, 0);
  _mm_store_ps(__p, __a);
}
# 2050 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_store_ps1(float *__p, __m128 __a)
{
  _mm_store1_ps(__p, __a);
}
# 2069 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_storer_ps(float *__p, __m128 __a)
{
  __a = __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 3, 2, 1, 0);
  _mm_store_ps(__p, __a);
}
# 2127 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_stream_pi(void *__p, __m64 __a)
{
  __builtin_ia32_movntq((__m64 *)__p, __a);
}
# 2146 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_stream_ps(void *__p, __m128 __a)
{
  __builtin_nontemporal_store((__v4sf)__a, (__v4sf*)__p);
}
# 2165 "/usr/lib/clang/18/include/xmmintrin.h" 3
void _mm_sfence(void);
# 2238 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_max_pi16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pmaxsw((__v4hi)__a, (__v4hi)__b);
}
# 2257 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_max_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pmaxub((__v8qi)__a, (__v8qi)__b);
}
# 2276 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_min_pi16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pminsw((__v4hi)__a, (__v4hi)__b);
}
# 2295 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_min_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pminub((__v8qi)__a, (__v8qi)__b);
}
# 2313 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_movemask_pi8(__m64 __a)
{
  return __builtin_ia32_pmovmskb((__v8qi)__a);
}
# 2332 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_mulhi_pu16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pmulhuw((__v4hi)__a, (__v4hi)__b);
}
# 2398 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_maskmove_si64(__m64 __d, __m64 __n, char *__p)
{
  __builtin_ia32_maskmovq((__v8qi)__d, (__v8qi)__n, __p);
}
# 2417 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_avg_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pavgb((__v8qi)__a, (__v8qi)__b);
}
# 2436 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_avg_pu16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pavgw((__v4hi)__a, (__v4hi)__b);
}
# 2458 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_sad_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_psadbw((__v8qi)__a, (__v8qi)__b);
}
# 2518 "/usr/lib/clang/18/include/xmmintrin.h" 3
unsigned int _mm_getcsr(void);
# 2572 "/usr/lib/clang/18/include/xmmintrin.h" 3
void _mm_setcsr(unsigned int __i);
# 2637 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_unpackhi_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 2, 6, 3, 7);
}
# 2659 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_unpacklo_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 0, 4, 1, 5);
}
# 2681 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_move_ss(__m128 __a, __m128 __b)
{
  __a[0] = __b[0];
  return __a;
}
# 2703 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_movehl_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 6, 7, 2, 3);
}
# 2724 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_movelh_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 0, 1, 4, 5);
}
# 2742 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtpi16_ps(__m64 __a)
{
  __m64 __b, __c;
  __m128 __r;

  __b = _mm_setzero_si64();
  __b = _mm_cmpgt_pi16(__b, __a);
  __c = _mm_unpackhi_pi16(__a, __b);
  __r = _mm_setzero_ps();
  __r = _mm_cvtpi32_ps(__r, __c);
  __r = _mm_movelh_ps(__r, __r);
  __c = _mm_unpacklo_pi16(__a, __b);
  __r = _mm_cvtpi32_ps(__r, __c);

  return __r;
}
# 2772 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtpu16_ps(__m64 __a)
{
  __m64 __b, __c;
  __m128 __r;

  __b = _mm_setzero_si64();
  __c = _mm_unpackhi_pi16(__a, __b);
  __r = _mm_setzero_ps();
  __r = _mm_cvtpi32_ps(__r, __c);
  __r = _mm_movelh_ps(__r, __r);
  __c = _mm_unpacklo_pi16(__a, __b);
  __r = _mm_cvtpi32_ps(__r, __c);

  return __r;
}
# 2801 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtpi8_ps(__m64 __a)
{
  __m64 __b;

  __b = _mm_setzero_si64();
  __b = _mm_cmpgt_pi8(__b, __a);
  __b = _mm_unpacklo_pi8(__a, __b);

  return _mm_cvtpi16_ps(__b);
}
# 2826 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtpu8_ps(__m64 __a)
{
  __m64 __b;

  __b = _mm_setzero_si64();
  __b = _mm_unpacklo_pi8(__a, __b);

  return _mm_cvtpi16_ps(__b);
}
# 2853 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtpi32x2_ps(__m64 __a, __m64 __b)
{
  __m128 __c;

  __c = _mm_setzero_ps();
  __c = _mm_cvtpi32_ps(__c, __b);
  __c = _mm_movelh_ps(__c, __c);

  return _mm_cvtpi32_ps(__c, __a);
}
# 2882 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtps_pi16(__m128 __a)
{
  __m64 __b, __c;

  __b = _mm_cvtps_pi32(__a);
  __a = _mm_movehl_ps(__a, __a);
  __c = _mm_cvtps_pi32(__a);

  return _mm_packs_pi32(__b, __c);
}
# 2912 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse,no-evex512"), __min_vector_width__(64)))
_mm_cvtps_pi8(__m128 __a)
{
  __m64 __b, __c;

  __b = _mm_cvtps_pi16(__a);
  __c = _mm_setzero_si64();

  return _mm_packs_pi16(__b, __c);
}
# 2937 "/usr/lib/clang/18/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse,no-evex512"), __min_vector_width__(128)))
_mm_movemask_ps(__m128 __a)
{
  return __builtin_ia32_movmskps((__v4sf)__a);
}
# 3018 "/usr/lib/clang/18/include/xmmintrin.h" 3
# 1 "/usr/lib/clang/18/include/emmintrin.h" 1 3
# 17 "/usr/lib/clang/18/include/emmintrin.h" 3
# 1 "/usr/lib/clang/18/include/xmmintrin.h" 1 3
# 18 "/usr/lib/clang/18/include/emmintrin.h" 2 3

typedef double __m128d __attribute__((__vector_size__(16), __aligned__(16)));
typedef long long __m128i __attribute__((__vector_size__(16), __aligned__(16)));

typedef double __m128d_u __attribute__((__vector_size__(16), __aligned__(1)));
typedef long long __m128i_u
    __attribute__((__vector_size__(16), __aligned__(1)));


typedef double __v2df __attribute__((__vector_size__(16)));
typedef long long __v2di __attribute__((__vector_size__(16)));
typedef short __v8hi __attribute__((__vector_size__(16)));
typedef char __v16qi __attribute__((__vector_size__(16)));


typedef unsigned long long __v2du __attribute__((__vector_size__(16)));
typedef unsigned short __v8hu __attribute__((__vector_size__(16)));
typedef unsigned char __v16qu __attribute__((__vector_size__(16)));



typedef signed char __v16qs __attribute__((__vector_size__(16)));



typedef _Float16 __v8hf __attribute__((__vector_size__(16), __aligned__(16)));
typedef _Float16 __m128h __attribute__((__vector_size__(16), __aligned__(16)));
typedef _Float16 __m128h_u __attribute__((__vector_size__(16), __aligned__(1)));

typedef __bf16 __v8bf __attribute__((__vector_size__(16), __aligned__(16)));
typedef __bf16 __m128bh __attribute__((__vector_size__(16), __aligned__(16)));
# 74 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_add_sd(__m128d __a,
                                                        __m128d __b) {
  __a[0] += __b[0];
  return __a;
}
# 92 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_add_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2df)__a + (__v2df)__b);
}
# 114 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sub_sd(__m128d __a,
                                                        __m128d __b) {
  __a[0] -= __b[0];
  return __a;
}
# 132 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sub_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2df)__a - (__v2df)__b);
}
# 153 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_mul_sd(__m128d __a,
                                                        __m128d __b) {
  __a[0] *= __b[0];
  return __a;
}
# 171 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_mul_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2df)__a * (__v2df)__b);
}
# 193 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_div_sd(__m128d __a,
                                                        __m128d __b) {
  __a[0] /= __b[0];
  return __a;
}
# 212 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_div_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2df)__a / (__v2df)__b);
}
# 236 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sqrt_sd(__m128d __a,
                                                         __m128d __b) {
  __m128d __c = __builtin_ia32_sqrtsd((__v2df)__b);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 253 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sqrt_pd(__m128d __a) {
  return __builtin_ia32_sqrtpd((__v2df)__a);
}
# 275 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_min_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_minsd((__v2df)__a, (__v2df)__b);
}
# 294 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_min_pd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_minpd((__v2df)__a, (__v2df)__b);
}
# 317 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_max_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_maxsd((__v2df)__a, (__v2df)__b);
}
# 336 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_max_pd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_maxpd((__v2df)__a, (__v2df)__b);
}
# 353 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_and_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2du)__a & (__v2du)__b);
}
# 373 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_andnot_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)(~(__v2du)__a & (__v2du)__b);
}
# 390 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_or_pd(__m128d __a,
                                                       __m128d __b) {
  return (__m128d)((__v2du)__a | (__v2du)__b);
}
# 407 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_xor_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2du)__a ^ (__v2du)__b);
}
# 425 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpeq_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpeqpd((__v2df)__a, (__v2df)__b);
}
# 444 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmplt_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpltpd((__v2df)__a, (__v2df)__b);
}
# 464 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmple_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmplepd((__v2df)__a, (__v2df)__b);
}
# 484 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpgt_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpltpd((__v2df)__b, (__v2df)__a);
}
# 504 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpge_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmplepd((__v2df)__b, (__v2df)__a);
}
# 526 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpord_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpordpd((__v2df)__a, (__v2df)__b);
}
# 549 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpunord_pd(__m128d __a,
                                                             __m128d __b) {
  return (__m128d)__builtin_ia32_cmpunordpd((__v2df)__a, (__v2df)__b);
}
# 569 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpneq_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpneqpd((__v2df)__a, (__v2df)__b);
}
# 589 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpnlt_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnltpd((__v2df)__a, (__v2df)__b);
}
# 609 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpnle_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnlepd((__v2df)__a, (__v2df)__b);
}
# 629 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpngt_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnltpd((__v2df)__b, (__v2df)__a);
}
# 649 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpnge_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnlepd((__v2df)__b, (__v2df)__a);
}
# 671 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpeq_sd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpeqsd((__v2df)__a, (__v2df)__b);
}
# 695 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmplt_sd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpltsd((__v2df)__a, (__v2df)__b);
}
# 719 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmple_sd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmplesd((__v2df)__a, (__v2df)__b);
}
# 743 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpgt_sd(__m128d __a,
                                                          __m128d __b) {
  __m128d __c = __builtin_ia32_cmpltsd((__v2df)__b, (__v2df)__a);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 768 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpge_sd(__m128d __a,
                                                          __m128d __b) {
  __m128d __c = __builtin_ia32_cmplesd((__v2df)__b, (__v2df)__a);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 795 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpord_sd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpordsd((__v2df)__a, (__v2df)__b);
}
# 822 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpunord_sd(__m128d __a,
                                                             __m128d __b) {
  return (__m128d)__builtin_ia32_cmpunordsd((__v2df)__a, (__v2df)__b);
}
# 846 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpneq_sd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpneqsd((__v2df)__a, (__v2df)__b);
}
# 870 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpnlt_sd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnltsd((__v2df)__a, (__v2df)__b);
}
# 894 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpnle_sd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnlesd((__v2df)__a, (__v2df)__b);
}
# 918 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpngt_sd(__m128d __a,
                                                           __m128d __b) {
  __m128d __c = __builtin_ia32_cmpnltsd((__v2df)__b, (__v2df)__a);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 943 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpnge_sd(__m128d __a,
                                                           __m128d __b) {
  __m128d __c = __builtin_ia32_cmpnlesd((__v2df)__b, (__v2df)__a);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 967 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_comieq_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdeq((__v2df)__a, (__v2df)__b);
}
# 992 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_comilt_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdlt((__v2df)__a, (__v2df)__b);
}
# 1017 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_comile_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdle((__v2df)__a, (__v2df)__b);
}
# 1042 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_comigt_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdgt((__v2df)__a, (__v2df)__b);
}
# 1067 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_comige_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdge((__v2df)__a, (__v2df)__b);
}
# 1092 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_comineq_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_comisdneq((__v2df)__a, (__v2df)__b);
}
# 1115 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_ucomieq_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdeq((__v2df)__a, (__v2df)__b);
}
# 1140 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_ucomilt_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdlt((__v2df)__a, (__v2df)__b);
}
# 1165 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_ucomile_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdle((__v2df)__a, (__v2df)__b);
}
# 1190 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_ucomigt_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdgt((__v2df)__a, (__v2df)__b);
}
# 1215 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_ucomige_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdge((__v2df)__a, (__v2df)__b);
}
# 1240 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_ucomineq_sd(__m128d __a,
                                                         __m128d __b) {
  return __builtin_ia32_ucomisdneq((__v2df)__a, (__v2df)__b);
}
# 1258 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtpd_ps(__m128d __a) {
  return __builtin_ia32_cvtpd2ps((__v2df)__a);
}
# 1276 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtps_pd(__m128 __a) {
  return (__m128d) __builtin_convertvector(
      __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 1), __v2df);
}
# 1297 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtepi32_pd(__m128i __a) {
  return (__m128d) __builtin_convertvector(
      __builtin_shufflevector((__v4si)__a, (__v4si)__a, 0, 1), __v2df);
}
# 1315 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtpd_epi32(__m128d __a) {
  return __builtin_ia32_cvtpd2dq((__v2df)__a);
}
# 1330 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsd_si32(__m128d __a) {
  return __builtin_ia32_cvtsd2si((__v2df)__a);
}
# 1353 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsd_ss(__m128 __a,
                                                         __m128d __b) {
  return (__m128)__builtin_ia32_cvtsd2ss((__v4sf)__a, (__v2df)__b);
}
# 1375 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsi32_sd(__m128d __a,
                                                            int __b) {
  __a[0] = __b;
  return __a;
}
# 1400 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtss_sd(__m128d __a,
                                                          __m128 __b) {
  __a[0] = __b[0];
  return __a;
}
# 1423 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvttpd_epi32(__m128d __a) {
  return (__m128i)__builtin_ia32_cvttpd2dq((__v2df)__a);
}
# 1439 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvttsd_si32(__m128d __a) {
  return __builtin_ia32_cvttsd2si((__v2df)__a);
}
# 1454 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2,no-evex512"), __min_vector_width__(64))) _mm_cvtpd_pi32(__m128d __a) {
  return (__m64)__builtin_ia32_cvtpd2pi((__v2df)__a);
}
# 1472 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2,no-evex512"), __min_vector_width__(64))) _mm_cvttpd_pi32(__m128d __a) {
  return (__m64)__builtin_ia32_cvttpd2pi((__v2df)__a);
}
# 1487 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2,no-evex512"), __min_vector_width__(64))) _mm_cvtpi32_pd(__m64 __a) {
  return __builtin_ia32_cvtpi2pd((__v2si)__a);
}
# 1502 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsd_f64(__m128d __a) {
  return __a[0];
}
# 1517 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_load_pd(double const *__dp) {
  return *(const __m128d *)__dp;
}
# 1533 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_load1_pd(double const *__dp) {
  struct __mm_load1_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((const struct __mm_load1_pd_struct *)__dp)->__u;
  return __extension__(__m128d){__u, __u};
}
# 1557 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_loadr_pd(double const *__dp) {
  __m128d __u = *(const __m128d *)__dp;
  return __builtin_shufflevector((__v2df)__u, (__v2df)__u, 1, 0);
}
# 1573 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_loadu_pd(double const *__dp) {
  struct __loadu_pd {
    __m128d_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_pd *)__dp)->__v;
}
# 1591 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_loadu_si64(void const *__a) {
  struct __loadu_si64 {
    long long __v;
  } __attribute__((__packed__, __may_alias__));
  long long __u = ((const struct __loadu_si64 *)__a)->__v;
  return __extension__(__m128i)(__v2di){__u, 0LL};
}
# 1610 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_loadu_si32(void const *__a) {
  struct __loadu_si32 {
    int __v;
  } __attribute__((__packed__, __may_alias__));
  int __u = ((const struct __loadu_si32 *)__a)->__v;
  return __extension__(__m128i)(__v4si){__u, 0, 0, 0};
}
# 1629 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_loadu_si16(void const *__a) {
  struct __loadu_si16 {
    short __v;
  } __attribute__((__packed__, __may_alias__));
  short __u = ((const struct __loadu_si16 *)__a)->__v;
  return __extension__(__m128i)(__v8hi){__u, 0, 0, 0, 0, 0, 0, 0};
}
# 1648 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_load_sd(double const *__dp) {
  struct __mm_load_sd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((const struct __mm_load_sd_struct *)__dp)->__u;
  return __extension__(__m128d){__u, 0};
}
# 1673 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_loadh_pd(__m128d __a,
                                                          double const *__dp) {
  struct __mm_loadh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((const struct __mm_loadh_pd_struct *)__dp)->__u;
  return __extension__(__m128d){__a[0], __u};
}
# 1699 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_loadl_pd(__m128d __a,
                                                          double const *__dp) {
  struct __mm_loadl_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((const struct __mm_loadl_pd_struct *)__dp)->__u;
  return __extension__(__m128d){__u, __a[1]};
}
# 1719 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_undefined_pd(void) {
  return (__m128d)__builtin_ia32_undef128();
}
# 1737 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set_sd(double __w) {
  return __extension__(__m128d){__w, 0};
}
# 1753 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set1_pd(double __w) {
  return __extension__(__m128d){__w, __w};
}
# 1769 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set_pd1(double __w) {
  return _mm_set1_pd(__w);
}
# 1787 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set_pd(double __w,
                                                        double __x) {
  return __extension__(__m128d){__x, __w};
}
# 1807 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_setr_pd(double __w,
                                                         double __x) {
  return __extension__(__m128d){__w, __x};
}
# 1821 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_setzero_pd(void) {
  return __extension__(__m128d){0.0, 0.0};
}
# 1840 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_move_sd(__m128d __a,
                                                         __m128d __b) {
  __a[0] = __b[0];
  return __a;
}
# 1857 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_store_sd(double *__dp,
                                                       __m128d __a) {
  struct __mm_store_sd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store_sd_struct *)__dp)->__u = __a[0];
}
# 1878 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_store_pd(double *__dp,
                                                       __m128d __a) {
  *(__m128d *)__dp = __a;
}
# 1897 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_store1_pd(double *__dp,
                                                        __m128d __a) {
  __a = __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);
  _mm_store_pd(__dp, __a);
}
# 1917 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_store_pd1(double *__dp,
                                                        __m128d __a) {
  _mm_store1_pd(__dp, __a);
}
# 1934 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_storeu_pd(double *__dp,
                                                        __m128d __a) {
  struct __storeu_pd {
    __m128d_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_pd *)__dp)->__v = __a;
}
# 1956 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_storer_pd(double *__dp,
                                                        __m128d __a) {
  __a = __builtin_shufflevector((__v2df)__a, (__v2df)__a, 1, 0);
  *(__m128d *)__dp = __a;
}
# 1973 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_storeh_pd(double *__dp,
                                                        __m128d __a) {
  struct __mm_storeh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pd_struct *)__dp)->__u = __a[1];
}
# 1992 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_storel_pd(double *__dp,
                                                        __m128d __a) {
  struct __mm_storeh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pd_struct *)__dp)->__u = __a[0];
}
# 2016 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_add_epi8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)((__v16qu)__a + (__v16qu)__b);
}
# 2037 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_add_epi16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v8hu)__a + (__v8hu)__b);
}
# 2058 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_add_epi32(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v4su)__a + (__v4su)__b);
}
# 2075 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2,no-evex512"), __min_vector_width__(64))) _mm_add_si64(__m64 __a,
                                                            __m64 __b) {
  return (__m64)__builtin_ia32_paddq((__v1di)__a, (__v1di)__b);
}
# 2096 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_add_epi64(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v2du)__a + (__v2du)__b);
}
# 2116 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_adds_epi8(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_add_sat((__v16qs)__a, (__v16qs)__b);
}
# 2137 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_adds_epi16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_elementwise_add_sat((__v8hi)__a, (__v8hi)__b);
}
# 2157 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_adds_epu8(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_add_sat((__v16qu)__a, (__v16qu)__b);
}
# 2177 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_adds_epu16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_elementwise_add_sat((__v8hu)__a, (__v8hu)__b);
}
# 2196 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_avg_epu8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)__builtin_ia32_pavgb128((__v16qi)__a, (__v16qi)__b);
}
# 2215 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_avg_epu16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_ia32_pavgw128((__v8hi)__a, (__v8hi)__b);
}
# 2240 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_madd_epi16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_ia32_pmaddwd128((__v8hi)__a, (__v8hi)__b);
}
# 2259 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_max_epi16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_max((__v8hi)__a, (__v8hi)__b);
}
# 2278 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_max_epu8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)__builtin_elementwise_max((__v16qu)__a, (__v16qu)__b);
}
# 2297 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_min_epi16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_min((__v8hi)__a, (__v8hi)__b);
}
# 2316 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_min_epu8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)__builtin_elementwise_min((__v16qu)__a, (__v16qu)__b);
}
# 2335 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_mulhi_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)__builtin_ia32_pmulhw128((__v8hi)__a, (__v8hi)__b);
}
# 2354 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_mulhi_epu16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)__builtin_ia32_pmulhuw128((__v8hi)__a, (__v8hi)__b);
}
# 2373 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_mullo_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v8hu)__a * (__v8hu)__b);
}
# 2391 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2,no-evex512"), __min_vector_width__(64))) _mm_mul_su32(__m64 __a,
                                                            __m64 __b) {
  return __builtin_ia32_pmuludq((__v2si)__a, (__v2si)__b);
}
# 2409 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_mul_epu32(__m128i __a,
                                                           __m128i __b) {
  return __builtin_ia32_pmuludq128((__v4si)__a, (__v4si)__b);
}
# 2430 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sad_epu8(__m128i __a,
                                                          __m128i __b) {
  return __builtin_ia32_psadbw128((__v16qi)__a, (__v16qi)__b);
}
# 2447 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sub_epi8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)((__v16qu)__a - (__v16qu)__b);
}
# 2464 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sub_epi16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v8hu)__a - (__v8hu)__b);
}
# 2481 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sub_epi32(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v4su)__a - (__v4su)__b);
}
# 2499 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2,no-evex512"), __min_vector_width__(64))) _mm_sub_si64(__m64 __a,
                                                            __m64 __b) {
  return (__m64)__builtin_ia32_psubq((__v1di)__a, (__v1di)__b);
}
# 2516 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sub_epi64(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v2du)__a - (__v2du)__b);
}
# 2536 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_subs_epi8(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_sub_sat((__v16qs)__a, (__v16qs)__b);
}
# 2556 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_subs_epi16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_elementwise_sub_sat((__v8hi)__a, (__v8hi)__b);
}
# 2575 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_subs_epu8(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_sub_sat((__v16qu)__a, (__v16qu)__b);
}
# 2594 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_subs_epu16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_elementwise_sub_sat((__v8hu)__a, (__v8hu)__b);
}
# 2611 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_and_si128(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v2du)__a & (__v2du)__b);
}
# 2630 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_andnot_si128(__m128i __a,
                                                              __m128i __b) {
  return (__m128i)(~(__v2du)__a & (__v2du)__b);
}
# 2646 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_or_si128(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)((__v2du)__a | (__v2du)__b);
}
# 2663 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_xor_si128(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v2du)__a ^ (__v2du)__b);
}
# 2706 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_slli_epi16(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psllwi128((__v8hi)__a, __count);
}
# 2724 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sll_epi16(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psllw128((__v8hi)__a, (__v8hi)__count);
}
# 2742 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_slli_epi32(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_pslldi128((__v4si)__a, __count);
}
# 2760 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sll_epi32(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_pslld128((__v4si)__a, (__v4si)__count);
}
# 2778 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_slli_epi64(__m128i __a,
                                                            int __count) {
  return __builtin_ia32_psllqi128((__v2di)__a, __count);
}
# 2796 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sll_epi64(__m128i __a,
                                                           __m128i __count) {
  return __builtin_ia32_psllq128((__v2di)__a, (__v2di)__count);
}
# 2815 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_srai_epi16(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psrawi128((__v8hi)__a, __count);
}
# 2834 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sra_epi16(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psraw128((__v8hi)__a, (__v8hi)__count);
}
# 2853 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_srai_epi32(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psradi128((__v4si)__a, __count);
}
# 2872 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_sra_epi32(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psrad128((__v4si)__a, (__v4si)__count);
}
# 2915 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_srli_epi16(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psrlwi128((__v8hi)__a, __count);
}
# 2933 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_srl_epi16(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psrlw128((__v8hi)__a, (__v8hi)__count);
}
# 2951 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_srli_epi32(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psrldi128((__v4si)__a, __count);
}
# 2969 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_srl_epi32(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psrld128((__v4si)__a, (__v4si)__count);
}
# 2987 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_srli_epi64(__m128i __a,
                                                            int __count) {
  return __builtin_ia32_psrlqi128((__v2di)__a, __count);
}
# 3005 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_srl_epi64(__m128i __a,
                                                           __m128i __count) {
  return __builtin_ia32_psrlq128((__v2di)__a, (__v2di)__count);
}
# 3023 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpeq_epi8(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)((__v16qi)__a == (__v16qi)__b);
}
# 3041 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpeq_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v8hi)__a == (__v8hi)__b);
}
# 3059 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpeq_epi32(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v4si)__a == (__v4si)__b);
}
# 3078 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpgt_epi8(__m128i __a,
                                                            __m128i __b) {


  return (__m128i)((__v16qs)__a > (__v16qs)__b);
}
# 3100 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpgt_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v8hi)__a > (__v8hi)__b);
}
# 3120 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmpgt_epi32(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v4si)__a > (__v4si)__b);
}
# 3140 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmplt_epi8(__m128i __a,
                                                            __m128i __b) {
  return _mm_cmpgt_epi8(__b, __a);
}
# 3160 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmplt_epi16(__m128i __a,
                                                             __m128i __b) {
  return _mm_cmpgt_epi16(__b, __a);
}
# 3180 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cmplt_epi32(__m128i __a,
                                                             __m128i __b) {
  return _mm_cmpgt_epi32(__b, __a);
}
# 3203 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsi64_sd(__m128d __a,
                                                            long long __b) {
  __a[0] = __b;
  return __a;
}
# 3220 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsd_si64(__m128d __a) {
  return __builtin_ia32_cvtsd2si64((__v2df)__a);
}
# 3236 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvttsd_si64(__m128d __a) {
  return __builtin_ia32_cvttsd2si64((__v2df)__a);
}
# 3250 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtepi32_ps(__m128i __a) {
  return (__m128) __builtin_convertvector((__v4si)__a, __v4sf);
}
# 3264 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtps_epi32(__m128 __a) {
  return (__m128i)__builtin_ia32_cvtps2dq((__v4sf)__a);
}
# 3279 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvttps_epi32(__m128 __a) {
  return (__m128i)__builtin_ia32_cvttps2dq((__v4sf)__a);
}
# 3293 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsi32_si128(int __a) {
  return __extension__(__m128i)(__v4si){__a, 0, 0, 0};
}
# 3308 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsi64_si128(long long __a) {
  return __extension__(__m128i)(__v2di){__a, 0};
}
# 3323 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsi128_si32(__m128i __a) {
  __v4si __b = (__v4si)__a;
  return __b[0];
}
# 3339 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_cvtsi128_si64(__m128i __a) {
  return __a[0];
}
# 3353 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128)))
_mm_load_si128(__m128i const *__p) {
  return *__p;
}
# 3368 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128)))
_mm_loadu_si128(__m128i_u const *__p) {
  struct __loadu_si128 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_si128 *)__p)->__v;
}
# 3388 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128)))
_mm_loadl_epi64(__m128i_u const *__p) {
  struct __mm_loadl_epi64_struct {
    long long __u;
  } __attribute__((__packed__, __may_alias__));
  return __extension__(__m128i){
      ((const struct __mm_loadl_epi64_struct *)__p)->__u, 0};
}
# 3406 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_undefined_si128(void) {
  return (__m128i)__builtin_ia32_undef128();
}
# 3426 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set_epi64x(long long __q1,
                                                            long long __q0) {
  return __extension__(__m128i)(__v2di){__q0, __q1};
}
# 3447 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set_epi64(__m64 __q1,
                                                           __m64 __q0) {
  return _mm_set_epi64x((long long)__q1, (long long)__q0);
}
# 3474 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set_epi32(int __i3, int __i2,
                                                           int __i1, int __i0) {
  return __extension__(__m128i)(__v4si){__i0, __i1, __i2, __i3};
}
# 3513 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128)))
_mm_set_epi16(short __w7, short __w6, short __w5, short __w4, short __w3,
              short __w2, short __w1, short __w0) {
  return __extension__(__m128i)(__v8hi){__w0, __w1, __w2, __w3,
                                        __w4, __w5, __w6, __w7};
}
# 3562 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128)))
_mm_set_epi8(char __b15, char __b14, char __b13, char __b12, char __b11,
             char __b10, char __b9, char __b8, char __b7, char __b6, char __b5,
             char __b4, char __b3, char __b2, char __b1, char __b0) {
  return __extension__(__m128i)(__v16qi){
      __b0, __b1, __b2, __b3, __b4, __b5, __b6, __b7,
      __b8, __b9, __b10, __b11, __b12, __b13, __b14, __b15};
}
# 3584 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set1_epi64x(long long __q) {
  return _mm_set_epi64x(__q, __q);
}
# 3601 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set1_epi64(__m64 __q) {
  return _mm_set_epi64(__q, __q);
}
# 3618 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set1_epi32(int __i) {
  return _mm_set_epi32(__i, __i, __i, __i);
}
# 3635 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set1_epi16(short __w) {
  return _mm_set_epi16(__w, __w, __w, __w, __w, __w, __w, __w);
}
# 3652 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_set1_epi8(char __b) {
  return _mm_set_epi8(__b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b,
                      __b, __b, __b, __b, __b);
}
# 3671 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_setr_epi64(__m64 __q0,
                                                            __m64 __q1) {
  return _mm_set_epi64(__q1, __q0);
}
# 3693 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_setr_epi32(int __i0, int __i1,
                                                            int __i2,
                                                            int __i3) {
  return _mm_set_epi32(__i3, __i2, __i1, __i0);
}
# 3724 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128)))
_mm_setr_epi16(short __w0, short __w1, short __w2, short __w3, short __w4,
               short __w5, short __w6, short __w7) {
  return _mm_set_epi16(__w7, __w6, __w5, __w4, __w3, __w2, __w1, __w0);
}
# 3771 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128)))
_mm_setr_epi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5,
              char __b6, char __b7, char __b8, char __b9, char __b10,
              char __b11, char __b12, char __b13, char __b14, char __b15) {
  return _mm_set_epi8(__b15, __b14, __b13, __b12, __b11, __b10, __b9, __b8,
                      __b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);
}
# 3787 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_setzero_si128(void) {
  return __extension__(__m128i)(__v2di){0LL, 0LL};
}
# 3803 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_store_si128(__m128i *__p,
                                                          __m128i __b) {
  *__p = __b;
}
# 3818 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_storeu_si128(__m128i_u *__p,
                                                           __m128i __b) {
  struct __storeu_si128 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si128 *)__p)->__v = __b;
}
# 3838 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_storeu_si64(void *__p,
                                                          __m128i __b) {
  struct __storeu_si64 {
    long long __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si64 *)__p)->__v = ((__v2di)__b)[0];
}
# 3858 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_storeu_si32(void *__p,
                                                          __m128i __b) {
  struct __storeu_si32 {
    int __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si32 *)__p)->__v = ((__v4si)__b)[0];
}
# 3878 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_storeu_si16(void *__p,
                                                          __m128i __b) {
  struct __storeu_si16 {
    short __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si16 *)__p)->__v = ((__v8hi)__b)[0];
}
# 3907 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_maskmoveu_si128(__m128i __d,
                                                              __m128i __n,
                                                              char *__p) {
  __builtin_ia32_maskmovdqu((__v16qi)__d, (__v16qi)__n, __p);
}
# 3926 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_storel_epi64(__m128i_u *__p,
                                                           __m128i __a) {
  struct __mm_storel_epi64_struct {
    long long __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storel_epi64_struct *)__p)->__u = __a[0];
}
# 3948 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_stream_pd(void *__p,
                                                        __m128d __a) {
  __builtin_nontemporal_store((__v2df)__a, (__v2df *)__p);
}
# 3966 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_stream_si128(void *__p,
                                                           __m128i __a) {
  __builtin_nontemporal_store((__v2di)__a, (__v2di *)__p);
}
# 3984 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void
    __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
    _mm_stream_si32(void *__p, int __a) {
  __builtin_ia32_movnti((int *)__p, __a);
}
# 4004 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ void
    __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
    _mm_stream_si64(void *__p, long long __a) {
  __builtin_ia32_movnti64((long long *)__p, __a);
}
# 4025 "/usr/lib/clang/18/include/emmintrin.h" 3
void _mm_clflush(void const *__p);
# 4036 "/usr/lib/clang/18/include/emmintrin.h" 3
void _mm_lfence(void);
# 4047 "/usr/lib/clang/18/include/emmintrin.h" 3
void _mm_mfence(void);
# 4075 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_packs_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)__builtin_ia32_packsswb128((__v8hi)__a, (__v8hi)__b);
}
# 4102 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_packs_epi32(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)__builtin_ia32_packssdw128((__v4si)__a, (__v4si)__b);
}
# 4129 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_packus_epi16(__m128i __a,
                                                              __m128i __b) {
  return (__m128i)__builtin_ia32_packuswb128((__v8hi)__a, (__v8hi)__b);
}
# 4204 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_movemask_epi8(__m128i __a) {
  return __builtin_ia32_pmovmskb128((__v16qi)__a);
}
# 4337 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpackhi_epi8(__m128i __a,
                                                               __m128i __b) {
  return (__m128i)__builtin_shufflevector(
      (__v16qi)__a, (__v16qi)__b, 8, 16 + 8, 9, 16 + 9, 10, 16 + 10, 11,
      16 + 11, 12, 16 + 12, 13, 16 + 13, 14, 16 + 14, 15, 16 + 15);
}
# 4365 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpackhi_epi16(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 4, 8 + 4, 5,
                                          8 + 5, 6, 8 + 6, 7, 8 + 7);
}
# 4388 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpackhi_epi32(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 2, 4 + 2, 3,
                                          4 + 3);
}
# 4409 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpackhi_epi64(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v2di)__a, (__v2di)__b, 1, 2 + 1);
}
# 4443 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpacklo_epi8(__m128i __a,
                                                               __m128i __b) {
  return (__m128i)__builtin_shufflevector(
      (__v16qi)__a, (__v16qi)__b, 0, 16 + 0, 1, 16 + 1, 2, 16 + 2, 3, 16 + 3, 4,
      16 + 4, 5, 16 + 5, 6, 16 + 6, 7, 16 + 7);
}
# 4472 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpacklo_epi16(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 0, 8 + 0, 1,
                                          8 + 1, 2, 8 + 2, 3, 8 + 3);
}
# 4495 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpacklo_epi32(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 0, 4 + 0, 1,
                                          4 + 1);
}
# 4516 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpacklo_epi64(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v2di)__a, (__v2di)__b, 0, 2 + 0);
}
# 4532 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_movepi64_pi64(__m128i __a) {
  return (__m64)__a[0];
}
# 4547 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_movpi64_epi64(__m64 __a) {
  return __extension__(__m128i)(__v2di){(long long)__a, 0};
}
# 4563 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_move_epi64(__m128i __a) {
  return __builtin_shufflevector((__v2di)__a, _mm_setzero_si128(), 0, 2);
}
# 4582 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpackhi_pd(__m128d __a,
                                                             __m128d __b) {
  return __builtin_shufflevector((__v2df)__a, (__v2df)__b, 1, 2 + 1);
}
# 4602 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_unpacklo_pd(__m128d __a,
                                                             __m128d __b) {
  return __builtin_shufflevector((__v2df)__a, (__v2df)__b, 0, 2 + 0);
}
# 4620 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_movemask_pd(__m128d __a) {
  return __builtin_ia32_movmskpd((__v2df)__a);
}
# 4666 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_castpd_ps(__m128d __a) {
  return (__m128)__a;
}
# 4681 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_castpd_si128(__m128d __a) {
  return (__m128i)__a;
}
# 4696 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_castps_pd(__m128 __a) {
  return (__m128d)__a;
}
# 4711 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_castps_si128(__m128 __a) {
  return (__m128i)__a;
}
# 4726 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_castsi128_ps(__m128i __a) {
  return (__m128)__a;
}
# 4741 "/usr/lib/clang/18/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2,no-evex512"), __min_vector_width__(128))) _mm_castsi128_pd(__m128i __a) {
  return (__m128d)__a;
}
# 4756 "/usr/lib/clang/18/include/emmintrin.h" 3
void _mm_pause(void);
# 3019 "/usr/lib/clang/18/include/xmmintrin.h" 2 3
# 27 "/usr/lib/clang/18/include/immintrin.h" 2 3
# 36 "/usr/lib/clang/18/include/immintrin.h" 3
# 1 "/usr/lib/clang/18/include/pmmintrin.h" 1 3
# 38 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_lddqu_si128(__m128i_u const *__p)
{
  return (__m128i)__builtin_ia32_lddqu((char const *)__p);
}
# 57 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_addsub_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_addsubps((__v4sf)__a, (__v4sf)__b);
}
# 80 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_hadd_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_haddps((__v4sf)__a, (__v4sf)__b);
}
# 103 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_hsub_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_hsubps((__v4sf)__a, (__v4sf)__b);
}
# 125 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_movehdup_ps(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 1, 1, 3, 3);
}
# 146 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_moveldup_ps(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 0, 2, 2);
}
# 165 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_addsub_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_addsubpd((__v2df)__a, (__v2df)__b);
}
# 188 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_hadd_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_haddpd((__v2df)__a, (__v2df)__b);
}
# 211 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_hsub_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_hsubpd((__v2df)__a, (__v2df)__b);
}
# 247 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_movedup_pd(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);
}
# 271 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_monitor(void const *__p, unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_monitor(__p, __extensions, __hints);
}
# 293 "/usr/lib/clang/18/include/pmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse3,no-evex512"), __min_vector_width__(128)))
_mm_mwait(unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_mwait(__extensions, __hints);
}
# 37 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/tmmintrin.h" 1 3
# 40 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_abs_pi8(__m64 __a)
{
    return (__m64)__builtin_ia32_pabsb((__v8qi)__a);
}
# 58 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_abs_epi8(__m128i __a)
{
    return (__m128i)__builtin_elementwise_abs((__v16qs)__a);
}
# 76 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_abs_pi16(__m64 __a)
{
    return (__m64)__builtin_ia32_pabsw((__v4hi)__a);
}
# 94 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_abs_epi16(__m128i __a)
{
    return (__m128i)__builtin_elementwise_abs((__v8hi)__a);
}
# 112 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_abs_pi32(__m64 __a)
{
    return (__m64)__builtin_ia32_pabsd((__v2si)__a);
}
# 130 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_abs_epi32(__m128i __a)
{
    return (__m128i)__builtin_elementwise_abs((__v4si)__a);
}
# 199 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hadd_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phaddw128((__v8hi)__a, (__v8hi)__b);
}
# 222 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hadd_epi32(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phaddd128((__v4si)__a, (__v4si)__b);
}
# 245 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hadd_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phaddw((__v4hi)__a, (__v4hi)__b);
}
# 268 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hadd_pi32(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phaddd((__v2si)__a, (__v2si)__b);
}
# 293 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hadds_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phaddsw128((__v8hi)__a, (__v8hi)__b);
}
# 318 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hadds_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phaddsw((__v4hi)__a, (__v4hi)__b);
}
# 341 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hsub_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phsubw128((__v8hi)__a, (__v8hi)__b);
}
# 364 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hsub_epi32(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phsubd128((__v4si)__a, (__v4si)__b);
}
# 387 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hsub_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phsubw((__v4hi)__a, (__v4hi)__b);
}
# 410 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hsub_pi32(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phsubd((__v2si)__a, (__v2si)__b);
}
# 435 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hsubs_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phsubsw128((__v8hi)__a, (__v8hi)__b);
}
# 460 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_hsubs_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phsubsw((__v4hi)__a, (__v4hi)__b);
}
# 494 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_maddubs_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_pmaddubsw128((__v16qi)__a, (__v16qi)__b);
}
# 524 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_maddubs_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_pmaddubsw((__v8qi)__a, (__v8qi)__b);
}
# 544 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_mulhrs_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_pmulhrsw128((__v8hi)__a, (__v8hi)__b);
}
# 564 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_mulhrs_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_pmulhrsw((__v4hi)__a, (__v4hi)__b);
}
# 590 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_shuffle_epi8(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_pshufb128((__v16qi)__a, (__v16qi)__b);
}
# 615 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_shuffle_pi8(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_pshufb((__v8qi)__a, (__v8qi)__b);
}
# 641 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_sign_epi8(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_psignb128((__v16qi)__a, (__v16qi)__b);
}
# 667 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_sign_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_psignw128((__v8hi)__a, (__v8hi)__b);
}
# 693 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3,no-evex512"), __min_vector_width__(64)))
_mm_sign_epi32(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_psignd128((__v4si)__a, (__v4si)__b);
}
# 719 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_sign_pi8(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_psignb((__v8qi)__a, (__v8qi)__b);
}
# 745 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_sign_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_psignw((__v4hi)__a, (__v4hi)__b);
}
# 771 "/usr/lib/clang/18/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3,no-evex512"), __min_vector_width__(64)))
_mm_sign_pi32(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_psignd((__v2si)__a, (__v2si)__b);
}
# 42 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/smmintrin.h" 1 3
# 436 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_blendv_pd(__m128d __V1,
                                                           __m128d __V2,
                                                           __m128d __M) {
  return (__m128d)__builtin_ia32_blendvpd((__v2df)__V1, (__v2df)__V2,
                                          (__v2df)__M);
}
# 463 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_blendv_ps(__m128 __V1,
                                                          __m128 __V2,
                                                          __m128 __M) {
  return (__m128)__builtin_ia32_blendvps((__v4sf)__V1, (__v4sf)__V2,
                                         (__v4sf)__M);
}
# 490 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_blendv_epi8(__m128i __V1,
                                                             __m128i __V2,
                                                             __m128i __M) {
  return (__m128i)__builtin_ia32_pblendvb128((__v16qi)__V1, (__v16qi)__V2,
                                             (__v16qi)__M);
}
# 539 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_mullo_epi32(__m128i __V1,
                                                             __m128i __V2) {
  return (__m128i)((__v4su)__V1 * (__v4su)__V2);
}
# 558 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_mul_epi32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_ia32_pmuldq128((__v4si)__V1, (__v4si)__V2);
}
# 647 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128)))
_mm_stream_load_si128(const void *__V) {
  return (__m128i)__builtin_nontemporal_load((const __v2di *)__V);
}
# 666 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_min_epi8(__m128i __V1,
                                                          __m128i __V2) {
  return (__m128i)__builtin_elementwise_min((__v16qs)__V1, (__v16qs)__V2);
}
# 684 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_max_epi8(__m128i __V1,
                                                          __m128i __V2) {
  return (__m128i)__builtin_elementwise_max((__v16qs)__V1, (__v16qs)__V2);
}
# 702 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_min_epu16(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_min((__v8hu)__V1, (__v8hu)__V2);
}
# 720 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_max_epu16(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_max((__v8hu)__V1, (__v8hu)__V2);
}
# 738 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_min_epi32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_min((__v4si)__V1, (__v4si)__V2);
}
# 756 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_max_epi32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_max((__v4si)__V1, (__v4si)__V2);
}
# 774 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_min_epu32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_min((__v4su)__V1, (__v4su)__V2);
}
# 792 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_max_epu32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_max((__v4su)__V1, (__v4su)__V2);
}
# 1093 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_testz_si128(__m128i __M,
                                                         __m128i __V) {
  return __builtin_ia32_ptestz128((__v2di)__M, (__v2di)__V);
}
# 1110 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_testc_si128(__m128i __M,
                                                         __m128i __V) {
  return __builtin_ia32_ptestc128((__v2di)__M, (__v2di)__V);
}
# 1128 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_testnzc_si128(__m128i __M,
                                                           __m128i __V) {
  return __builtin_ia32_ptestnzc128((__v2di)__M, (__v2di)__V);
}
# 1200 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cmpeq_epi64(__m128i __V1,
                                                             __m128i __V2) {
  return (__m128i)((__v2di)__V1 == (__v2di)__V2);
}
# 1219 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepi8_epi16(__m128i __V) {


  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3, 4, 5, 6,
                              7),
      __v8hi);
}
# 1241 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepi8_epi32(__m128i __V) {


  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3), __v4si);
}
# 1261 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepi8_epi64(__m128i __V) {


  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1), __v2di);
}
# 1281 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepi16_epi32(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1, 2, 3), __v4si);
}
# 1299 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepi16_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1), __v2di);
}
# 1317 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepi32_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v4si)__V, (__v4si)__V, 0, 1), __v2di);
}
# 1336 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepu8_epi16(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3, 4, 5, 6,
                              7),
      __v8hi);
}
# 1356 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepu8_epi32(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3), __v4si);
}
# 1374 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepu8_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1), __v2di);
}
# 1392 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepu16_epi32(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1, 2, 3), __v4si);
}
# 1410 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepu16_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1), __v2di);
}
# 1428 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_cvtepu32_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v4su)__V, (__v4su)__V, 0, 1), __v2di);
}
# 1456 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_packus_epi32(__m128i __V1,
                                                              __m128i __V2) {
  return (__m128i)__builtin_ia32_packusdw128((__v4si)__V1, (__v4si)__V2);
}
# 1514 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1,no-evex512"), __min_vector_width__(128))) _mm_minpos_epu16(__m128i __V) {
  return (__m128i)__builtin_ia32_phminposuw128((__v8hi)__V);
}
# 2317 "/usr/lib/clang/18/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.2"))) _mm_cmpgt_epi64(__m128i __V1,
                                                             __m128i __V2) {
  return (__m128i)((__v2di)__V1 > (__v2di)__V2);
}



# 1 "/usr/lib/clang/18/include/popcntintrin.h" 1 3
# 32 "/usr/lib/clang/18/include/popcntintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("popcnt")))
_mm_popcnt_u32(unsigned int __A)
{
  return __builtin_popcount(__A);
}
# 49 "/usr/lib/clang/18/include/popcntintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("popcnt")))
_mm_popcnt_u64(unsigned long long __A)
{
  return __builtin_popcountll(__A);
}
# 2325 "/usr/lib/clang/18/include/smmintrin.h" 2 3
# 47 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/wmmintrin.h" 1 3
# 19 "/usr/lib/clang/18/include/wmmintrin.h" 3
# 1 "/usr/lib/clang/18/include/__wmmintrin_aes.h" 1 3
# 34 "/usr/lib/clang/18/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesenc_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesenc128((__v2di)__V, (__v2di)__R);
}
# 54 "/usr/lib/clang/18/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesenclast_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesenclast128((__v2di)__V, (__v2di)__R);
}
# 74 "/usr/lib/clang/18/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesdec_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesdec128((__v2di)__V, (__v2di)__R);
}
# 94 "/usr/lib/clang/18/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesdeclast_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesdeclast128((__v2di)__V, (__v2di)__R);
}
# 111 "/usr/lib/clang/18/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesimc_si128(__m128i __V)
{
  return (__m128i)__builtin_ia32_aesimc128((__v2di)__V);
}
# 20 "/usr/lib/clang/18/include/wmmintrin.h" 2 3

# 1 "/usr/lib/clang/18/include/__wmmintrin_pclmul.h" 1 3
# 22 "/usr/lib/clang/18/include/wmmintrin.h" 2 3
# 52 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/clflushoptintrin.h" 1 3
# 29 "/usr/lib/clang/18/include/clflushoptintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("clflushopt")))
_mm_clflushopt(void const * __m) {
  __builtin_ia32_clflushopt(__m);
}
# 57 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/clwbintrin.h" 1 3
# 31 "/usr/lib/clang/18/include/clwbintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("clwb")))
_mm_clwb(void const *__p) {
  __builtin_ia32_clwb(__p);
}
# 62 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avxintrin.h" 1 3
# 17 "/usr/lib/clang/18/include/avxintrin.h" 3
typedef double __v4df __attribute__ ((__vector_size__ (32)));
typedef float __v8sf __attribute__ ((__vector_size__ (32)));
typedef long long __v4di __attribute__ ((__vector_size__ (32)));
typedef int __v8si __attribute__ ((__vector_size__ (32)));
typedef short __v16hi __attribute__ ((__vector_size__ (32)));
typedef char __v32qi __attribute__ ((__vector_size__ (32)));


typedef unsigned long long __v4du __attribute__ ((__vector_size__ (32)));
typedef unsigned int __v8su __attribute__ ((__vector_size__ (32)));
typedef unsigned short __v16hu __attribute__ ((__vector_size__ (32)));
typedef unsigned char __v32qu __attribute__ ((__vector_size__ (32)));



typedef signed char __v32qs __attribute__((__vector_size__(32)));

typedef float __m256 __attribute__ ((__vector_size__ (32), __aligned__(32)));
typedef double __m256d __attribute__((__vector_size__(32), __aligned__(32)));
typedef long long __m256i __attribute__((__vector_size__(32), __aligned__(32)));

typedef float __m256_u __attribute__ ((__vector_size__ (32), __aligned__(1)));
typedef double __m256d_u __attribute__((__vector_size__(32), __aligned__(1)));
typedef long long __m256i_u __attribute__((__vector_size__(32), __aligned__(1)));



typedef _Float16 __v16hf __attribute__((__vector_size__(32), __aligned__(32)));
typedef _Float16 __m256h __attribute__((__vector_size__(32), __aligned__(32)));
typedef _Float16 __m256h_u __attribute__((__vector_size__(32), __aligned__(1)));

typedef __bf16 __v16bf __attribute__((__vector_size__(32), __aligned__(32)));
typedef __bf16 __m256bh __attribute__((__vector_size__(32), __aligned__(32)));
# 73 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_add_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a+(__v4df)__b);
}
# 91 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_add_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a+(__v8sf)__b);
}
# 109 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_sub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a-(__v4df)__b);
}
# 127 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_sub_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a-(__v8sf)__b);
}
# 146 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_addsub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_addsubpd256((__v4df)__a, (__v4df)__b);
}
# 165 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_addsub_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_addsubps256((__v8sf)__a, (__v8sf)__b);
}
# 183 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_div_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a/(__v4df)__b);
}
# 201 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_div_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a/(__v8sf)__b);
}
# 220 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_max_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_maxpd256((__v4df)__a, (__v4df)__b);
}
# 239 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_max_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_maxps256((__v8sf)__a, (__v8sf)__b);
}
# 258 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_min_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_minpd256((__v4df)__a, (__v4df)__b);
}
# 277 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_min_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_minps256((__v8sf)__a, (__v8sf)__b);
}
# 295 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_mul_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a * (__v4df)__b);
}
# 313 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_mul_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a * (__v8sf)__b);
}
# 330 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_sqrt_pd(__m256d __a)
{
  return (__m256d)__builtin_ia32_sqrtpd256((__v4df)__a);
}
# 347 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_sqrt_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_sqrtps256((__v8sf)__a);
}
# 364 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_rsqrt_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_rsqrtps256((__v8sf)__a);
}
# 381 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_rcp_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_rcpps256((__v8sf)__a);
}
# 533 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_and_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4du)__a & (__v4du)__b);
}
# 551 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_and_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8su)__a & (__v8su)__b);
}
# 572 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_andnot_pd(__m256d __a, __m256d __b)
{
  return (__m256d)(~(__v4du)__a & (__v4du)__b);
}
# 593 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_andnot_ps(__m256 __a, __m256 __b)
{
  return (__m256)(~(__v8su)__a & (__v8su)__b);
}
# 611 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_or_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4du)__a | (__v4du)__b);
}
# 629 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_or_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8su)__a | (__v8su)__b);
}
# 647 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_xor_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4du)__a ^ (__v4du)__b);
}
# 665 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_xor_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8su)__a ^ (__v8su)__b);
}
# 689 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_hadd_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_haddpd256((__v4df)__a, (__v4df)__b);
}
# 712 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_hadd_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_haddps256((__v8sf)__a, (__v8sf)__b);
}
# 735 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_hsub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_hsubpd256((__v4df)__a, (__v4df)__b);
}
# 758 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_hsub_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_hsubps256((__v8sf)__a, (__v8sf)__b);
}
# 788 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_permutevar_pd(__m128d __a, __m128i __c)
{
  return (__m128d)__builtin_ia32_vpermilvarpd((__v2df)__a, (__v2di)__c);
}
# 827 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_permutevar_pd(__m256d __a, __m256i __c)
{
  return (__m256d)__builtin_ia32_vpermilvarpd256((__v4df)__a, (__v4di)__c);
}
# 881 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_permutevar_ps(__m128 __a, __m128i __c)
{
  return (__m128)__builtin_ia32_vpermilvarps((__v4sf)__a, (__v4si)__c);
}
# 972 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_permutevar_ps(__m256 __a, __m256i __c)
{
  return (__m256)__builtin_ia32_vpermilvarps256((__v8sf)__a, (__v8si)__c);
}
# 1396 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_blendv_pd(__m256d __a, __m256d __b, __m256d __c)
{
  return (__m256d)__builtin_ia32_blendvpd256(
    (__v4df)__a, (__v4df)__b, (__v4df)__c);
}
# 1424 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_blendv_ps(__m256 __a, __m256 __b, __m256 __c)
{
  return (__m256)__builtin_ia32_blendvps256(
    (__v8sf)__a, (__v8sf)__b, (__v8sf)__c);
}
# 2177 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi32_pd(__m128i __a)
{
  return (__m256d)__builtin_convertvector((__v4si)__a, __v4df);
}
# 2192 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi32_ps(__m256i __a)
{
  return (__m256)__builtin_convertvector((__v8si)__a, __v8sf);
}
# 2208 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvtpd_ps(__m256d __a)
{
  return (__m128)__builtin_ia32_cvtpd2ps256((__v4df) __a);
}
# 2223 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvtps_epi32(__m256 __a)
{
  return (__m256i)__builtin_ia32_cvtps2dq256((__v8sf) __a);
}
# 2239 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvtps_pd(__m128 __a)
{
  return (__m256d)__builtin_convertvector((__v4sf)__a, __v4df);
}
# 2256 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvttpd_epi32(__m256d __a)
{
  return (__m128i)__builtin_ia32_cvttpd2dq256((__v4df) __a);
}
# 2273 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvtpd_epi32(__m256d __a)
{
  return (__m128i)__builtin_ia32_cvtpd2dq256((__v4df) __a);
}
# 2289 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvttps_epi32(__m256 __a)
{
  return (__m256i)__builtin_ia32_cvttps2dq256((__v8sf) __a);
}
# 2305 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline double __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvtsd_f64(__m256d __a)
{
 return __a[0];
}
# 2321 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvtsi256_si32(__m256i __a)
{
 __v8si __b = (__v8si)__a;
 return __b[0];
}
# 2338 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline float __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_cvtss_f32(__m256 __a)
{
 return __a[0];
}
# 2364 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_movehdup_ps(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 1, 1, 3, 3, 5, 5, 7, 7);
}
# 2389 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_moveldup_ps(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 0, 0, 2, 2, 4, 4, 6, 6);
}
# 2411 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_movedup_pd(__m256d __a)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__a, 0, 0, 2, 2);
}
# 2434 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_unpackhi_pd(__m256d __a, __m256d __b)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__b, 1, 5, 1+2, 5+2);
}
# 2456 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_unpacklo_pd(__m256d __a, __m256d __b)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__b, 0, 4, 0+2, 4+2);
}
# 2483 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_unpackhi_ps(__m256 __a, __m256 __b)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__b, 2, 10, 2+1, 10+1, 6, 14, 6+1, 14+1);
}
# 2510 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_unpacklo_ps(__m256 __a, __m256 __b)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__b, 0, 8, 0+1, 8+1, 4, 12, 4+1, 12+1);
}
# 2540 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_testz_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestzpd((__v2df)__a, (__v2df)__b);
}
# 2569 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_testc_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestcpd((__v2df)__a, (__v2df)__b);
}
# 2599 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_testnzc_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestnzcpd((__v2df)__a, (__v2df)__b);
}
# 2628 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_testz_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestzps((__v4sf)__a, (__v4sf)__b);
}
# 2657 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_testc_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestcps((__v4sf)__a, (__v4sf)__b);
}
# 2687 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_testnzc_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestnzcps((__v4sf)__a, (__v4sf)__b);
}
# 2716 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_testz_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestzpd256((__v4df)__a, (__v4df)__b);
}
# 2745 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_testc_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestcpd256((__v4df)__a, (__v4df)__b);
}
# 2775 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_testnzc_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestnzcpd256((__v4df)__a, (__v4df)__b);
}
# 2804 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_testz_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestzps256((__v8sf)__a, (__v8sf)__b);
}
# 2833 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_testc_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestcps256((__v8sf)__a, (__v8sf)__b);
}
# 2863 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_testnzc_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestnzcps256((__v8sf)__a, (__v8sf)__b);
}
# 2889 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_testz_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestz256((__v4di)__a, (__v4di)__b);
}
# 2915 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_testc_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestc256((__v4di)__a, (__v4di)__b);
}
# 2942 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_testnzc_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestnzc256((__v4di)__a, (__v4di)__b);
}
# 2961 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_movemask_pd(__m256d __a)
{
  return __builtin_ia32_movmskpd256((__v4df)__a);
}
# 2979 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_movemask_ps(__m256 __a)
{
  return __builtin_ia32_movmskps256((__v8sf)__a);
}







static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_zeroall(void)
{
  __builtin_ia32_vzeroall();
}






static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_zeroupper(void)
{
  __builtin_ia32_vzeroupper();
}
# 3021 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_broadcast_ss(float const *__a)
{
  struct __mm_broadcast_ss_struct {
    float __f;
  } __attribute__((__packed__, __may_alias__));
  float __f = ((const struct __mm_broadcast_ss_struct*)__a)->__f;
  return __extension__ (__m128){ __f, __f, __f, __f };
}
# 3043 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_sd(double const *__a)
{
  struct __mm256_broadcast_sd_struct {
    double __d;
  } __attribute__((__packed__, __may_alias__));
  double __d = ((const struct __mm256_broadcast_sd_struct*)__a)->__d;
  return __extension__ (__m256d)(__v4df){ __d, __d, __d, __d };
}
# 3065 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_ss(float const *__a)
{
  struct __mm256_broadcast_ss_struct {
    float __f;
  } __attribute__((__packed__, __may_alias__));
  float __f = ((const struct __mm256_broadcast_ss_struct*)__a)->__f;
  return __extension__ (__m256)(__v8sf){ __f, __f, __f, __f, __f, __f, __f, __f };
}
# 3087 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_pd(__m128d const *__a)
{
  __m128d __b = _mm_loadu_pd((const double *)__a);
  return (__m256d)__builtin_shufflevector((__v2df)__b, (__v2df)__b,
                                          0, 1, 0, 1);
}
# 3107 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_ps(__m128 const *__a)
{
  __m128 __b = _mm_loadu_ps((const float *)__a);
  return (__m256)__builtin_shufflevector((__v4sf)__b, (__v4sf)__b,
                                         0, 1, 2, 3, 0, 1, 2, 3);
}
# 3127 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_load_pd(double const *__p)
{
  return *(const __m256d *)__p;
}
# 3143 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_load_ps(float const *__p)
{
  return *(const __m256 *)__p;
}
# 3160 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_loadu_pd(double const *__p)
{
  struct __loadu_pd {
    __m256d_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_pd*)__p)->__v;
}
# 3180 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_loadu_ps(float const *__p)
{
  struct __loadu_ps {
    __m256_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ps*)__p)->__v;
}
# 3200 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_load_si256(__m256i const *__p)
{
  return *__p;
}
# 3216 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_loadu_si256(__m256i_u const *__p)
{
  struct __loadu_si256 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_si256*)__p)->__v;
}
# 3237 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_lddqu_si256(__m256i_u const *__p)
{
  return (__m256i)__builtin_ia32_lddqu256((char const *)__p);
}
# 3257 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_store_pd(double *__p, __m256d __a)
{
  *(__m256d *)__p = __a;
}
# 3275 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_store_ps(float *__p, __m256 __a)
{
  *(__m256 *)__p = __a;
}
# 3293 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_storeu_pd(double *__p, __m256d __a)
{
  struct __storeu_pd {
    __m256d_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_pd*)__p)->__v = __a;
}
# 3313 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_storeu_ps(float *__p, __m256 __a)
{
  struct __storeu_ps {
    __m256_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ps*)__p)->__v = __a;
}
# 3334 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_store_si256(__m256i *__p, __m256i __a)
{
  *__p = __a;
}
# 3351 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_storeu_si256(__m256i_u *__p, __m256i __a)
{
  struct __storeu_si256 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si256*)__p)->__v = __a;
}
# 3379 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_maskload_pd(double const *__p, __m128i __m)
{
  return (__m128d)__builtin_ia32_maskloadpd((const __v2df *)__p, (__v2di)__m);
}
# 3403 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_maskload_pd(double const *__p, __m256i __m)
{
  return (__m256d)__builtin_ia32_maskloadpd256((const __v4df *)__p,
                                               (__v4di)__m);
}
# 3428 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_maskload_ps(float const *__p, __m128i __m)
{
  return (__m128)__builtin_ia32_maskloadps((const __v4sf *)__p, (__v4si)__m);
}
# 3452 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_maskload_ps(float const *__p, __m256i __m)
{
  return (__m256)__builtin_ia32_maskloadps256((const __v8sf *)__p, (__v8si)__m);
}
# 3477 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_maskstore_ps(float *__p, __m256i __m, __m256 __a)
{
  __builtin_ia32_maskstoreps256((__v8sf *)__p, (__v8si)__m, (__v8sf)__a);
}
# 3501 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_maskstore_pd(double *__p, __m128i __m, __m128d __a)
{
  __builtin_ia32_maskstorepd((__v2df *)__p, (__v2di)__m, (__v2df)__a);
}
# 3525 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_maskstore_pd(double *__p, __m256i __m, __m256d __a)
{
  __builtin_ia32_maskstorepd256((__v4df *)__p, (__v4di)__m, (__v4df)__a);
}
# 3549 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(128)))
_mm_maskstore_ps(float *__p, __m128i __m, __m128 __a)
{
  __builtin_ia32_maskstoreps((__v4sf *)__p, (__v4si)__m, (__v4sf)__a);
}
# 3569 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_stream_si256(void *__a, __m256i __b)
{
  typedef __v4di __v4di_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v4di_aligned)__b, (__v4di_aligned*)__a);
}
# 3589 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_stream_pd(void *__a, __m256d __b)
{
  typedef __v4df __v4df_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v4df_aligned)__b, (__v4df_aligned*)__a);
}
# 3610 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_stream_ps(void *__p, __m256 __a)
{
  typedef __v8sf __v8sf_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v8sf_aligned)__a, (__v8sf_aligned*)__p);
}
# 3625 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_undefined_pd(void)
{
  return (__m256d)__builtin_ia32_undef256();
}
# 3638 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_undefined_ps(void)
{
  return (__m256)__builtin_ia32_undef256();
}
# 3651 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_undefined_si256(void)
{
  return (__m256i)__builtin_ia32_undef256();
}
# 3678 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set_pd(double __a, double __b, double __c, double __d)
{
  return __extension__ (__m256d){ __d, __c, __b, __a };
}
# 3717 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set_ps(float __a, float __b, float __c, float __d,
              float __e, float __f, float __g, float __h)
{
  return __extension__ (__m256){ __h, __g, __f, __e, __d, __c, __b, __a };
}
# 3749 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set_epi32(int __i0, int __i1, int __i2, int __i3,
                 int __i4, int __i5, int __i6, int __i7)
{
  return __extension__ (__m256i)(__v8si){ __i7, __i6, __i5, __i4, __i3, __i2, __i1, __i0 };
}
# 3797 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set_epi16(short __w15, short __w14, short __w13, short __w12,
                 short __w11, short __w10, short __w09, short __w08,
                 short __w07, short __w06, short __w05, short __w04,
                 short __w03, short __w02, short __w01, short __w00)
{
  return __extension__ (__m256i)(__v16hi){ __w00, __w01, __w02, __w03, __w04, __w05, __w06,
    __w07, __w08, __w09, __w10, __w11, __w12, __w13, __w14, __w15 };
}
# 3880 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set_epi8(char __b31, char __b30, char __b29, char __b28,
                char __b27, char __b26, char __b25, char __b24,
                char __b23, char __b22, char __b21, char __b20,
                char __b19, char __b18, char __b17, char __b16,
                char __b15, char __b14, char __b13, char __b12,
                char __b11, char __b10, char __b09, char __b08,
                char __b07, char __b06, char __b05, char __b04,
                char __b03, char __b02, char __b01, char __b00)
{
  return __extension__ (__m256i)(__v32qi){
    __b00, __b01, __b02, __b03, __b04, __b05, __b06, __b07,
    __b08, __b09, __b10, __b11, __b12, __b13, __b14, __b15,
    __b16, __b17, __b18, __b19, __b20, __b21, __b22, __b23,
    __b24, __b25, __b26, __b27, __b28, __b29, __b30, __b31
  };
}
# 3915 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set_epi64x(long long __a, long long __b, long long __c, long long __d)
{
  return __extension__ (__m256i)(__v4di){ __d, __c, __b, __a };
}
# 3944 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setr_pd(double __a, double __b, double __c, double __d)
{
  return _mm256_set_pd(__d, __c, __b, __a);
}
# 3984 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setr_ps(float __a, float __b, float __c, float __d,
               float __e, float __f, float __g, float __h)
{
  return _mm256_set_ps(__h, __g, __f, __e, __d, __c, __b, __a);
}
# 4016 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setr_epi32(int __i0, int __i1, int __i2, int __i3,
                  int __i4, int __i5, int __i6, int __i7)
{
  return _mm256_set_epi32(__i7, __i6, __i5, __i4, __i3, __i2, __i1, __i0);
}
# 4064 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setr_epi16(short __w15, short __w14, short __w13, short __w12,
       short __w11, short __w10, short __w09, short __w08,
       short __w07, short __w06, short __w05, short __w04,
       short __w03, short __w02, short __w01, short __w00)
{
  return _mm256_set_epi16(__w00, __w01, __w02, __w03,
                          __w04, __w05, __w06, __w07,
                          __w08, __w09, __w10, __w11,
                          __w12, __w13, __w14, __w15);
}
# 4149 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setr_epi8(char __b31, char __b30, char __b29, char __b28,
                 char __b27, char __b26, char __b25, char __b24,
                 char __b23, char __b22, char __b21, char __b20,
                 char __b19, char __b18, char __b17, char __b16,
                 char __b15, char __b14, char __b13, char __b12,
                 char __b11, char __b10, char __b09, char __b08,
                 char __b07, char __b06, char __b05, char __b04,
                 char __b03, char __b02, char __b01, char __b00)
{
  return _mm256_set_epi8(__b00, __b01, __b02, __b03, __b04, __b05, __b06, __b07,
                         __b08, __b09, __b10, __b11, __b12, __b13, __b14, __b15,
                         __b16, __b17, __b18, __b19, __b20, __b21, __b22, __b23,
                         __b24, __b25, __b26, __b27, __b28, __b29, __b30, __b31);
}
# 4182 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setr_epi64x(long long __a, long long __b, long long __c, long long __d)
{
  return _mm256_set_epi64x(__d, __c, __b, __a);
}
# 4201 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set1_pd(double __w)
{
  return _mm256_set_pd(__w, __w, __w, __w);
}
# 4220 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set1_ps(float __w)
{
  return _mm256_set_ps(__w, __w, __w, __w, __w, __w, __w, __w);
}
# 4239 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set1_epi32(int __i)
{
  return _mm256_set_epi32(__i, __i, __i, __i, __i, __i, __i, __i);
}
# 4257 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set1_epi16(short __w)
{
  return _mm256_set_epi16(__w, __w, __w, __w, __w, __w, __w, __w,
                          __w, __w, __w, __w, __w, __w, __w, __w);
}
# 4275 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set1_epi8(char __b)
{
  return _mm256_set_epi8(__b, __b, __b, __b, __b, __b, __b, __b,
                         __b, __b, __b, __b, __b, __b, __b, __b,
                         __b, __b, __b, __b, __b, __b, __b, __b,
                         __b, __b, __b, __b, __b, __b, __b, __b);
}
# 4296 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set1_epi64x(long long __q)
{
  return _mm256_set_epi64x(__q, __q, __q, __q);
}
# 4311 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setzero_pd(void)
{
  return __extension__ (__m256d){ 0.0, 0.0, 0.0, 0.0 };
}
# 4325 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setzero_ps(void)
{
  return __extension__ (__m256){ 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f };
}
# 4338 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setzero_si256(void)
{
  return __extension__ (__m256i)(__v4di){ 0, 0, 0, 0 };
}
# 4356 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castpd_ps(__m256d __a)
{
  return (__m256)__a;
}
# 4373 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castpd_si256(__m256d __a)
{
  return (__m256i)__a;
}
# 4390 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castps_pd(__m256 __a)
{
  return (__m256d)__a;
}
# 4407 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castps_si256(__m256 __a)
{
  return (__m256i)__a;
}
# 4424 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castsi256_ps(__m256i __a)
{
  return (__m256)__a;
}
# 4441 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castsi256_pd(__m256i __a)
{
  return (__m256d)__a;
}
# 4458 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castpd256_pd128(__m256d __a)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__a, 0, 1);
}
# 4475 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castps256_ps128(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 0, 1, 2, 3);
}
# 4491 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castsi256_si128(__m256i __a)
{
  return __builtin_shufflevector((__v4di)__a, (__v4di)__a, 0, 1);
}
# 4512 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castpd128_pd256(__m128d __a)
{
  return __builtin_shufflevector(
      (__v2df)__a, (__v2df)__builtin_nondeterministic_value(__a), 0, 1, 2, 3);
}
# 4534 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castps128_ps256(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a,
                                 (__v4sf)__builtin_nondeterministic_value(__a),
                                 0, 1, 2, 3, 4, 5, 6, 7);
}
# 4555 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_castsi128_si256(__m128i __a)
{
  return __builtin_shufflevector(
      (__v2di)__a, (__v2di)__builtin_nondeterministic_value(__a), 0, 1, 2, 3);
}
# 4575 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_zextpd128_pd256(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)_mm_setzero_pd(), 0, 1, 2, 3);
}
# 4593 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_zextps128_ps256(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)_mm_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7);
}
# 4611 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_zextsi128_si256(__m128i __a)
{
  return __builtin_shufflevector((__v2di)__a, (__v2di)_mm_setzero_si128(), 0, 1, 2, 3);
}
# 4828 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set_m128 (__m128 __hi, __m128 __lo)
{
  return (__m256) __builtin_shufflevector((__v4sf)__lo, (__v4sf)__hi, 0, 1, 2, 3, 4, 5, 6, 7);
}
# 4849 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set_m128d (__m128d __hi, __m128d __lo)
{
  return (__m256d) __builtin_shufflevector((__v2df)__lo, (__v2df)__hi, 0, 1, 2, 3);
}
# 4869 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_set_m128i (__m128i __hi, __m128i __lo)
{
  return (__m256i) __builtin_shufflevector((__v2di)__lo, (__v2di)__hi, 0, 1, 2, 3);
}
# 4892 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setr_m128 (__m128 __lo, __m128 __hi)
{
  return _mm256_set_m128(__hi, __lo);
}
# 4915 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setr_m128d (__m128d __lo, __m128d __hi)
{
  return (__m256d)_mm256_set_m128d(__hi, __lo);
}
# 4936 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_setr_m128i (__m128i __lo, __m128i __hi)
{
  return (__m256i)_mm256_set_m128i(__hi, __lo);
}
# 4964 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_loadu2_m128(float const *__addr_hi, float const *__addr_lo)
{
  return _mm256_set_m128(_mm_loadu_ps(__addr_hi), _mm_loadu_ps(__addr_lo));
}
# 4991 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_loadu2_m128d(double const *__addr_hi, double const *__addr_lo)
{
  return _mm256_set_m128d(_mm_loadu_pd(__addr_hi), _mm_loadu_pd(__addr_lo));
}
# 5015 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_loadu2_m128i(__m128i_u const *__addr_hi, __m128i_u const *__addr_lo)
{
   return _mm256_set_m128i(_mm_loadu_si128(__addr_hi), _mm_loadu_si128(__addr_lo));
}
# 5040 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_storeu2_m128(float *__addr_hi, float *__addr_lo, __m256 __a)
{
  __m128 __v128;

  __v128 = _mm256_castps256_ps128(__a);
  _mm_storeu_ps(__addr_lo, __v128);
  __v128 = ((__m128)__builtin_ia32_vextractf128_ps256((__v8sf)(__m256)(__a), (int)(1)));
  _mm_storeu_ps(__addr_hi, __v128);
}
# 5069 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_storeu2_m128d(double *__addr_hi, double *__addr_lo, __m256d __a)
{
  __m128d __v128;

  __v128 = _mm256_castpd256_pd128(__a);
  _mm_storeu_pd(__addr_lo, __v128);
  __v128 = ((__m128d)__builtin_ia32_vextractf128_pd256((__v4df)(__m256d)(__a), (int)(1)));
  _mm_storeu_pd(__addr_hi, __v128);
}
# 5098 "/usr/lib/clang/18/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx,no-evex512"), __min_vector_width__(256)))
_mm256_storeu2_m128i(__m128i_u *__addr_hi, __m128i_u *__addr_lo, __m256i __a)
{
  __m128i __v128;

  __v128 = _mm256_castsi256_si128(__a);
  _mm_storeu_si128(__addr_lo, __v128);
  __v128 = ((__m128i)__builtin_ia32_vextractf128_si256((__v8si)(__m256i)(__a), (int)(1)));
  _mm_storeu_si128(__addr_hi, __v128);
}
# 67 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx2intrin.h" 1 3
# 98 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_abs_epi8(__m256i __a)
{
    return (__m256i)__builtin_elementwise_abs((__v32qs)__a);
}
# 115 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_abs_epi16(__m256i __a)
{
    return (__m256i)__builtin_elementwise_abs((__v16hi)__a);
}
# 132 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_abs_epi32(__m256i __a)
{
    return (__m256i)__builtin_elementwise_abs((__v8si)__a);
}
# 163 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_packs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packsswb256((__v16hi)__a, (__v16hi)__b);
}
# 195 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_packs_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packssdw256((__v8si)__a, (__v8si)__b);
}
# 226 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_packus_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packuswb256((__v16hi)__a, (__v16hi)__b);
}
# 258 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_packus_epi32(__m256i __V1, __m256i __V2)
{
  return (__m256i) __builtin_ia32_packusdw256((__v8si)__V1, (__v8si)__V2);
}
# 277 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_add_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qu)__a + (__v32qu)__b);
}
# 296 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_add_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hu)__a + (__v16hu)__b);
}
# 315 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_add_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a + (__v8su)__b);
}
# 334 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_add_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a + (__v4du)__b);
}
# 353 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_adds_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_add_sat((__v32qs)__a, (__v32qs)__b);
}
# 371 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_adds_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_add_sat((__v16hi)__a, (__v16hi)__b);
}
# 390 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_adds_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_add_sat((__v32qu)__a, (__v32qu)__b);
}
# 408 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_adds_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_add_sat((__v16hu)__a, (__v16hu)__b);
}
# 454 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_and_si256(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a & (__v4du)__b);
}
# 472 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_andnot_si256(__m256i __a, __m256i __b)
{
  return (__m256i)(~(__v4du)__a & (__v4du)__b);
}
# 498 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_avg_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pavgb256((__v32qi)__a, (__v32qi)__b);
}
# 524 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_avg_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pavgw256((__v16hi)__a, (__v16hi)__b);
}
# 559 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_blendv_epi8(__m256i __V1, __m256i __V2, __m256i __M)
{
  return (__m256i)__builtin_ia32_pblendvb256((__v32qi)__V1, (__v32qi)__V2,
                                              (__v32qi)__M);
}
# 627 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cmpeq_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qi)__a == (__v32qi)__b);
}
# 653 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cmpeq_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a == (__v16hi)__b);
}
# 679 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cmpeq_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a == (__v8si)__b);
}
# 705 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cmpeq_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4di)__a == (__v4di)__b);
}
# 731 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cmpgt_epi8(__m256i __a, __m256i __b)
{


  return (__m256i)((__v32qs)__a > (__v32qs)__b);
}
# 759 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cmpgt_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a > (__v16hi)__b);
}
# 785 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cmpgt_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a > (__v8si)__b);
}
# 811 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cmpgt_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4di)__a > (__v4di)__b);
}
# 847 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_hadd_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddw256((__v16hi)__a, (__v16hi)__b);
}
# 879 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_hadd_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddd256((__v8si)__a, (__v8si)__b);
}
# 914 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_hadds_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddsw256((__v16hi)__a, (__v16hi)__b);
}
# 950 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_hsub_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubw256((__v16hi)__a, (__v16hi)__b);
}
# 982 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_hsub_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubd256((__v8si)__a, (__v8si)__b);
}
# 1018 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_hsubs_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubsw256((__v16hi)__a, (__v16hi)__b);
}
# 1048 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_maddubs_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_pmaddubsw256((__v32qi)__a, (__v32qi)__b);
}
# 1080 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_madd_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaddwd256((__v16hi)__a, (__v16hi)__b);
}
# 1099 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_max_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v32qs)__a, (__v32qs)__b);
}
# 1118 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_max_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v16hi)__a, (__v16hi)__b);
}
# 1137 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_max_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v8si)__a, (__v8si)__b);
}
# 1156 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_max_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v32qu)__a, (__v32qu)__b);
}
# 1175 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_max_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v16hu)__a, (__v16hu)__b);
}
# 1194 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_max_epu32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v8su)__a, (__v8su)__b);
}
# 1213 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_min_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v32qs)__a, (__v32qs)__b);
}
# 1232 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_min_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v16hi)__a, (__v16hi)__b);
}
# 1251 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_min_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v8si)__a, (__v8si)__b);
}
# 1270 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_min_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v32qu)__a, (__v32qu)__b);
}
# 1289 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_min_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v16hu)__a, (__v16hu)__b);
}
# 1308 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_min_epu32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v8su)__a, (__v8su)__b);
}
# 1331 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_movemask_epi8(__m256i __a)
{
  return __builtin_ia32_pmovmskb256((__v32qi)__a);
}
# 1357 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi8_epi16(__m128i __V)
{


  return (__m256i)__builtin_convertvector((__v16qs)__V, __v16hi);
}
# 1385 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi8_epi32(__m128i __V)
{


  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8si);
}
# 1412 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi8_epi64(__m128i __V)
{


  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3), __v4di);
}
# 1440 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi16_epi32(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v8hi)__V, __v8si);
}
# 1465 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi16_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1, 2, 3), __v4di);
}
# 1490 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi32_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v4si)__V, __v4di);
}
# 1516 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu8_epi16(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v16qu)__V, __v16hi);
}
# 1542 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu8_epi32(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8si);
}
# 1567 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu8_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3), __v4di);
}
# 1593 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu16_epi32(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v8hu)__V, __v8si);
}
# 1618 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu16_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1, 2, 3), __v4di);
}
# 1643 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu32_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v4su)__V, __v4di);
}
# 1669 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_mul_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmuldq256((__v8si)__a, (__v8si)__b);
}
# 1696 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_mulhrs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhrsw256((__v16hi)__a, (__v16hi)__b);
}
# 1715 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_mulhi_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhuw256((__v16hi)__a, (__v16hi)__b);
}
# 1734 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_mulhi_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhw256((__v16hi)__a, (__v16hi)__b);
}
# 1753 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_mullo_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hu)__a * (__v16hu)__b);
}
# 1772 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_mullo_epi32 (__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a * (__v8su)__b);
}
# 1798 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_mul_epu32(__m256i __a, __m256i __b)
{
  return __builtin_ia32_pmuludq256((__v8si)__a, (__v8si)__b);
}
# 1816 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_or_si256(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a | (__v4du)__b);
}
# 1861 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sad_epu8(__m256i __a, __m256i __b)
{
  return __builtin_ia32_psadbw256((__v32qi)__a, (__v32qi)__b);
}
# 1900 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_shuffle_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pshufb256((__v32qi)__a, (__v32qi)__b);
}
# 2027 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sign_epi8(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignb256((__v32qi)__a, (__v32qi)__b);
}
# 2048 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sign_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignw256((__v16hi)__a, (__v16hi)__b);
}
# 2069 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sign_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignd256((__v8si)__a, (__v8si)__b);
}
# 2128 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_slli_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psllwi256((__v16hi)__a, __count);
}
# 2149 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sll_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psllw256((__v16hi)__a, (__v8hi)__count);
}
# 2168 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_slli_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_pslldi256((__v8si)__a, __count);
}
# 2189 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sll_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_pslld256((__v8si)__a, (__v4si)__count);
}
# 2208 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_slli_epi64(__m256i __a, int __count)
{
  return __builtin_ia32_psllqi256((__v4di)__a, __count);
}
# 2229 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sll_epi64(__m256i __a, __m128i __count)
{
  return __builtin_ia32_psllq256((__v4di)__a, __count);
}
# 2249 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srai_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrawi256((__v16hi)__a, __count);
}
# 2271 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sra_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psraw256((__v16hi)__a, (__v8hi)__count);
}
# 2291 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srai_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psradi256((__v8si)__a, __count);
}
# 2313 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sra_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrad256((__v8si)__a, (__v4si)__count);
}
# 2372 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srli_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrlwi256((__v16hi)__a, __count);
}
# 2393 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srl_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrlw256((__v16hi)__a, (__v8hi)__count);
}
# 2412 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srli_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrldi256((__v8si)__a, __count);
}
# 2433 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srl_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrld256((__v8si)__a, (__v4si)__count);
}
# 2452 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srli_epi64(__m256i __a, int __count)
{
  return __builtin_ia32_psrlqi256((__v4di)__a, __count);
}
# 2473 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srl_epi64(__m256i __a, __m128i __count)
{
  return __builtin_ia32_psrlq256((__v4di)__a, __count);
}
# 2500 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sub_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qu)__a - (__v32qu)__b);
}
# 2527 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sub_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hu)__a - (__v16hu)__b);
}
# 2553 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sub_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a - (__v8su)__b);
}
# 2579 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sub_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a - (__v4du)__b);
}
# 2605 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_subs_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_sub_sat((__v32qs)__a, (__v32qs)__b);
}
# 2631 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_subs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_sub_sat((__v16hi)__a, (__v16hi)__b);
}
# 2658 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_subs_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_sub_sat((__v32qu)__a, (__v32qu)__b);
}
# 2684 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_subs_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_sub_sat((__v16hu)__a, (__v16hu)__b);
}
# 2718 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_unpackhi_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 8, 32+8, 9, 32+9, 10, 32+10, 11, 32+11, 12, 32+12, 13, 32+13, 14, 32+14, 15, 32+15, 24, 32+24, 25, 32+25, 26, 32+26, 27, 32+27, 28, 32+28, 29, 32+29, 30, 32+30, 31, 32+31);
}
# 2753 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_unpackhi_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 4, 16+4, 5, 16+5, 6, 16+6, 7, 16+7, 12, 16+12, 13, 16+13, 14, 16+14, 15, 16+15);
}
# 2787 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_unpackhi_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 2, 8+2, 3, 8+3, 6, 8+6, 7, 8+7);
}
# 2817 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_unpackhi_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v4di)__a, (__v4di)__b, 1, 4+1, 3, 4+3);
}
# 2851 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_unpacklo_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 0, 32+0, 1, 32+1, 2, 32+2, 3, 32+3, 4, 32+4, 5, 32+5, 6, 32+6, 7, 32+7, 16, 32+16, 17, 32+17, 18, 32+18, 19, 32+19, 20, 32+20, 21, 32+21, 22, 32+22, 23, 32+23);
}
# 2886 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_unpacklo_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 0, 16+0, 1, 16+1, 2, 16+2, 3, 16+3, 8, 16+8, 9, 16+9, 10, 16+10, 11, 16+11);
}
# 2920 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_unpacklo_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 0, 8+0, 1, 8+1, 4, 8+4, 5, 8+5);
}
# 2950 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_unpacklo_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v4di)__a, (__v4di)__b, 0, 4+0, 2, 4+2);
}
# 2968 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_xor_si256(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a ^ (__v4du)__b);
}
# 2985 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_stream_load_si256(const void *__V)
{
  typedef __v4di __v4di_aligned __attribute__((aligned(32)));
  return (__m256i)__builtin_nontemporal_load((const __v4di_aligned *)__V);
}
# 3003 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_broadcastss_ps(__m128 __X)
{
  return (__m128)__builtin_shufflevector((__v4sf)__X, (__v4sf)__X, 0, 0, 0, 0);
}
# 3020 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_broadcastsd_pd(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);
}
# 3037 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_broadcastss_ps(__m128 __X)
{
  return (__m256)__builtin_shufflevector((__v4sf)__X, (__v4sf)__X, 0, 0, 0, 0, 0, 0, 0, 0);
}
# 3054 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_broadcastsd_pd(__m128d __X)
{
  return (__m256d)__builtin_shufflevector((__v2df)__X, (__v2df)__X, 0, 0, 0, 0);
}
# 3070 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_broadcastsi128_si256(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 1, 0, 1);
}
# 3162 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_broadcastb_epi8(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v16qi)__X, (__v16qi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}
# 3178 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_broadcastw_epi16(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v8hi)__X, (__v8hi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}
# 3194 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_broadcastd_epi32(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v4si)__X, (__v4si)__X, 0, 0, 0, 0, 0, 0, 0, 0);
}
# 3210 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_broadcastq_epi64(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 0, 0, 0);
}
# 3226 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_broadcastb_epi8(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v16qi)__X, (__v16qi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}
# 3242 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_broadcastw_epi16(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v8hi)__X, (__v8hi)__X, 0, 0, 0, 0, 0, 0, 0, 0);
}
# 3258 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_broadcastd_epi32(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v4si)__X, (__v4si)__X, 0, 0, 0, 0);
}
# 3274 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_broadcastq_epi64(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 0);
}
# 3302 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_permutevar8x32_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_permvarsi256((__v8si)__a, (__v8si)__b);
}
# 3360 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_permutevar8x32_ps(__m256 __a, __m256i __b)
{
  return (__m256)__builtin_ia32_permvarsf256((__v8sf)__a, (__v8si)__b);
}
# 3512 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_maskload_epi32(int const *__X, __m256i __M)
{
  return (__m256i)__builtin_ia32_maskloadd256((const __v8si *)__X, (__v8si)__M);
}
# 3544 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_maskload_epi64(long long const *__X, __m256i __M)
{
  return (__m256i)__builtin_ia32_maskloadq256((const __v4di *)__X, (__v4di)__M);
}
# 3576 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_maskload_epi32(int const *__X, __m128i __M)
{
  return (__m128i)__builtin_ia32_maskloadd((const __v4si *)__X, (__v4si)__M);
}
# 3608 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_maskload_epi64(long long const *__X, __m128i __M)
{
  return (__m128i)__builtin_ia32_maskloadq((const __v2di *)__X, (__v2di)__M);
}
# 3638 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_maskstore_epi32(int *__X, __m256i __M, __m256i __Y)
{
  __builtin_ia32_maskstored256((__v8si *)__X, (__v8si)__M, (__v8si)__Y);
}
# 3668 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_maskstore_epi64(long long *__X, __m256i __M, __m256i __Y)
{
  __builtin_ia32_maskstoreq256((__v4di *)__X, (__v4di)__M, (__v4di)__Y);
}
# 3698 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_maskstore_epi32(int *__X, __m128i __M, __m128i __Y)
{
  __builtin_ia32_maskstored((__v4si *)__X, (__v4si)__M, (__v4si)__Y);
}
# 3728 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_maskstore_epi64(long long *__X, __m128i __M, __m128i __Y)
{
  __builtin_ia32_maskstoreq(( __v2di *)__X, (__v2di)__M, (__v2di)__Y);
}
# 3750 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sllv_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psllv8si((__v8si)__X, (__v8si)__Y);
}
# 3772 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_sllv_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psllv4si((__v4si)__X, (__v4si)__Y);
}
# 3794 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_sllv_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psllv4di((__v4di)__X, (__v4di)__Y);
}
# 3816 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_sllv_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psllv2di((__v2di)__X, (__v2di)__Y);
}
# 3839 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srav_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrav8si((__v8si)__X, (__v8si)__Y);
}
# 3862 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_srav_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrav4si((__v4si)__X, (__v4si)__Y);
}
# 3884 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srlv_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrlv8si((__v8si)__X, (__v8si)__Y);
}
# 3906 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_srlv_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrlv4si((__v4si)__X, (__v4si)__Y);
}
# 3928 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(256)))
_mm256_srlv_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrlv4di((__v4di)__X, (__v4di)__Y);
}
# 3950 "/usr/lib/clang/18/include/avx2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2,no-evex512"), __min_vector_width__(128)))
_mm_srlv_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrlv2di((__v2di)__X, (__v2di)__Y);
}
# 72 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/f16cintrin.h" 1 3
# 38 "/usr/lib/clang/18/include/f16cintrin.h" 3
static __inline float __attribute__((__always_inline__, __nodebug__, __target__("f16c"), __min_vector_width__(128)))
_cvtsh_ss(unsigned short __a)
{
  __v8hi __v = {(short)__a, 0, 0, 0, 0, 0, 0, 0};
  __v4sf __r = __builtin_ia32_vcvtph2ps(__v);
  return __r[0];
}
# 109 "/usr/lib/clang/18/include/f16cintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("f16c"), __min_vector_width__(128)))
_mm_cvtph_ps(__m128i __a)
{
  return (__m128)__builtin_ia32_vcvtph2ps((__v8hi)__a);
}
# 153 "/usr/lib/clang/18/include/f16cintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("f16c"), __min_vector_width__(256)))
_mm256_cvtph_ps(__m128i __a)
{
  return (__m256)__builtin_ia32_vcvtph2ps256((__v8hi)__a);
}
# 77 "/usr/lib/clang/18/include/immintrin.h" 2 3



# 1 "/usr/lib/clang/18/include/bmiintrin.h" 1 3
# 33 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__))
__tzcnt_u16(unsigned short __X)
{
  return __builtin_ia32_tzcnt_u16(__X);
}
# 67 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__tzcnt_u32(unsigned int __X)
{
  return __builtin_ia32_tzcnt_u32(__X);
}
# 84 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_tzcnt_32(unsigned int __X)
{
  return (int)__builtin_ia32_tzcnt_u32(__X);
}
# 120 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__tzcnt_u64(unsigned long long __X)
{
  return __builtin_ia32_tzcnt_u64(__X);
}
# 137 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__))
_mm_tzcnt_64(unsigned long long __X)
{
  return (long long)__builtin_ia32_tzcnt_u64(__X);
}
# 184 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__andn_u32(unsigned int __X, unsigned int __Y)
{
  return ~__X & __Y;
}
# 227 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__bextr_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bextr_u32(__X, __Y);
}
# 252 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr_u32(unsigned int __X, unsigned int __Y, unsigned int __Z)
{
  return __builtin_ia32_bextr_u32 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}
# 275 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr2_u32(unsigned int __X, unsigned int __Y) {
  return __builtin_ia32_bextr_u32(__X, __Y);
}
# 292 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsi_u32(unsigned int __X)
{
  return __X & -__X;
}
# 328 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsmsk_u32(unsigned int __X)
{
  return __X ^ (__X - 1);
}
# 364 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsr_u32(unsigned int __X)
{
  return __X & (__X - 1);
}
# 404 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__andn_u64 (unsigned long long __X, unsigned long long __Y)
{
  return ~__X & __Y;
}
# 448 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__bextr_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bextr_u64(__X, __Y);
}
# 473 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr_u64(unsigned long long __X, unsigned int __Y, unsigned int __Z)
{
  return __builtin_ia32_bextr_u64 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}
# 496 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr2_u64(unsigned long long __X, unsigned long long __Y) {
  return __builtin_ia32_bextr_u64(__X, __Y);
}
# 513 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsi_u64(unsigned long long __X)
{
  return __X & -__X;
}
# 549 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsmsk_u64(unsigned long long __X)
{
  return __X ^ (__X - 1);
}
# 585 "/usr/lib/clang/18/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsr_u64(unsigned long long __X)
{
  return __X & (__X - 1);
}
# 81 "/usr/lib/clang/18/include/immintrin.h" 2 3



# 1 "/usr/lib/clang/18/include/bmi2intrin.h" 1 3
# 40 "/usr/lib/clang/18/include/bmi2intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_bzhi_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bzhi_si(__X, __Y);
}
# 70 "/usr/lib/clang/18/include/bmi2intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pdep_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pdep_si(__X, __Y);
}
# 100 "/usr/lib/clang/18/include/bmi2intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pext_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pext_si(__X, __Y);
}
# 126 "/usr/lib/clang/18/include/bmi2intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_mulx_u32(unsigned int __X, unsigned int __Y, unsigned int *__P)
{
  unsigned long long __res = (unsigned long long) __X * __Y;
  *__P = (unsigned int)(__res >> 32);
  return (unsigned int)__res;
}
# 156 "/usr/lib/clang/18/include/bmi2intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_bzhi_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bzhi_di(__X, __Y);
}
# 186 "/usr/lib/clang/18/include/bmi2intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pdep_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pdep_di(__X, __Y);
}
# 216 "/usr/lib/clang/18/include/bmi2intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pext_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pext_di(__X, __Y);
}
# 242 "/usr/lib/clang/18/include/bmi2intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_mulx_u64 (unsigned long long __X, unsigned long long __Y,
    unsigned long long *__P)
{
  unsigned __int128 __res = (unsigned __int128) __X * __Y;
  *__P = (unsigned long long) (__res >> 64);
  return (unsigned long long) __res;
}
# 85 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/lzcntintrin.h" 1 3
# 45 "/usr/lib/clang/18/include/lzcntintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
__lzcnt32(unsigned int __X)
{
  return __builtin_ia32_lzcnt_u32(__X);
}
# 62 "/usr/lib/clang/18/include/lzcntintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
_lzcnt_u32(unsigned int __X)
{
  return __builtin_ia32_lzcnt_u32(__X);
}
# 95 "/usr/lib/clang/18/include/lzcntintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
_lzcnt_u64(unsigned long long __X)
{
  return __builtin_ia32_lzcnt_u64(__X);
}
# 90 "/usr/lib/clang/18/include/immintrin.h" 2 3
# 99 "/usr/lib/clang/18/include/immintrin.h" 3
# 1 "/usr/lib/clang/18/include/fmaintrin.h" 1 3
# 35 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}
# 55 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}
# 83 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmadd_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}
# 111 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmadd_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, (__v2df)__B, (__v2df)__C);
}
# 131 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}
# 151 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}
# 179 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}
# 207 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}
# 227 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}
# 247 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);
}
# 275 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmadd_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, -(__v4sf)__B, (__v4sf)__C);
}
# 303 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmadd_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, -(__v2df)__B, (__v2df)__C);
}
# 323 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}
# 343 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);
}
# 371 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, -(__v4sf)__B, -(__v4sf)__C);
}
# 399 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, -(__v2df)__B, -(__v2df)__C);
}
# 425 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmaddsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}
# 449 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmaddsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}
# 475 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsubadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}
# 499 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsubadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}
# 519 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}
# 539 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}
# 559 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}
# 579 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}
# 599 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fnmadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}
# 619 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fnmadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, (__v4df)__C);
}
# 639 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fnmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}
# 659 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fnmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, -(__v4df)__C);
}
# 689 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmaddsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}
# 715 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmaddsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}
# 745 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmsubadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}
# 771 "/usr/lib/clang/18/include/fmaintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmsubadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}
# 100 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512fintrin.h" 1 3
# 16 "/usr/lib/clang/18/include/avx512fintrin.h" 3
typedef char __v64qi __attribute__((__vector_size__(64)));
typedef short __v32hi __attribute__((__vector_size__(64)));
typedef double __v8df __attribute__((__vector_size__(64)));
typedef float __v16sf __attribute__((__vector_size__(64)));
typedef long long __v8di __attribute__((__vector_size__(64)));
typedef int __v16si __attribute__((__vector_size__(64)));


typedef unsigned char __v64qu __attribute__((__vector_size__(64)));
typedef unsigned short __v32hu __attribute__((__vector_size__(64)));
typedef unsigned long long __v8du __attribute__((__vector_size__(64)));
typedef unsigned int __v16su __attribute__((__vector_size__(64)));



typedef signed char __v64qs __attribute__((__vector_size__(64)));

typedef float __m512 __attribute__((__vector_size__(64), __aligned__(64)));
typedef double __m512d __attribute__((__vector_size__(64), __aligned__(64)));
typedef long long __m512i __attribute__((__vector_size__(64), __aligned__(64)));

typedef float __m512_u __attribute__((__vector_size__(64), __aligned__(1)));
typedef double __m512d_u __attribute__((__vector_size__(64), __aligned__(1)));
typedef long long __m512i_u __attribute__((__vector_size__(64), __aligned__(1)));

typedef unsigned char __mmask8;
typedef unsigned short __mmask16;
# 52 "/usr/lib/clang/18/include/avx512fintrin.h" 3
typedef enum {
    _MM_CMPINT_EQ,
    _MM_CMPINT_LT,
    _MM_CMPINT_LE,
    _MM_CMPINT_UNUSED,
    _MM_CMPINT_NE,
    _MM_CMPINT_NLT,

    _MM_CMPINT_NLE

} _MM_CMPINT_ENUM;

typedef enum
{
  _MM_PERM_AAAA = 0x00, _MM_PERM_AAAB = 0x01, _MM_PERM_AAAC = 0x02,
  _MM_PERM_AAAD = 0x03, _MM_PERM_AABA = 0x04, _MM_PERM_AABB = 0x05,
  _MM_PERM_AABC = 0x06, _MM_PERM_AABD = 0x07, _MM_PERM_AACA = 0x08,
  _MM_PERM_AACB = 0x09, _MM_PERM_AACC = 0x0A, _MM_PERM_AACD = 0x0B,
  _MM_PERM_AADA = 0x0C, _MM_PERM_AADB = 0x0D, _MM_PERM_AADC = 0x0E,
  _MM_PERM_AADD = 0x0F, _MM_PERM_ABAA = 0x10, _MM_PERM_ABAB = 0x11,
  _MM_PERM_ABAC = 0x12, _MM_PERM_ABAD = 0x13, _MM_PERM_ABBA = 0x14,
  _MM_PERM_ABBB = 0x15, _MM_PERM_ABBC = 0x16, _MM_PERM_ABBD = 0x17,
  _MM_PERM_ABCA = 0x18, _MM_PERM_ABCB = 0x19, _MM_PERM_ABCC = 0x1A,
  _MM_PERM_ABCD = 0x1B, _MM_PERM_ABDA = 0x1C, _MM_PERM_ABDB = 0x1D,
  _MM_PERM_ABDC = 0x1E, _MM_PERM_ABDD = 0x1F, _MM_PERM_ACAA = 0x20,
  _MM_PERM_ACAB = 0x21, _MM_PERM_ACAC = 0x22, _MM_PERM_ACAD = 0x23,
  _MM_PERM_ACBA = 0x24, _MM_PERM_ACBB = 0x25, _MM_PERM_ACBC = 0x26,
  _MM_PERM_ACBD = 0x27, _MM_PERM_ACCA = 0x28, _MM_PERM_ACCB = 0x29,
  _MM_PERM_ACCC = 0x2A, _MM_PERM_ACCD = 0x2B, _MM_PERM_ACDA = 0x2C,
  _MM_PERM_ACDB = 0x2D, _MM_PERM_ACDC = 0x2E, _MM_PERM_ACDD = 0x2F,
  _MM_PERM_ADAA = 0x30, _MM_PERM_ADAB = 0x31, _MM_PERM_ADAC = 0x32,
  _MM_PERM_ADAD = 0x33, _MM_PERM_ADBA = 0x34, _MM_PERM_ADBB = 0x35,
  _MM_PERM_ADBC = 0x36, _MM_PERM_ADBD = 0x37, _MM_PERM_ADCA = 0x38,
  _MM_PERM_ADCB = 0x39, _MM_PERM_ADCC = 0x3A, _MM_PERM_ADCD = 0x3B,
  _MM_PERM_ADDA = 0x3C, _MM_PERM_ADDB = 0x3D, _MM_PERM_ADDC = 0x3E,
  _MM_PERM_ADDD = 0x3F, _MM_PERM_BAAA = 0x40, _MM_PERM_BAAB = 0x41,
  _MM_PERM_BAAC = 0x42, _MM_PERM_BAAD = 0x43, _MM_PERM_BABA = 0x44,
  _MM_PERM_BABB = 0x45, _MM_PERM_BABC = 0x46, _MM_PERM_BABD = 0x47,
  _MM_PERM_BACA = 0x48, _MM_PERM_BACB = 0x49, _MM_PERM_BACC = 0x4A,
  _MM_PERM_BACD = 0x4B, _MM_PERM_BADA = 0x4C, _MM_PERM_BADB = 0x4D,
  _MM_PERM_BADC = 0x4E, _MM_PERM_BADD = 0x4F, _MM_PERM_BBAA = 0x50,
  _MM_PERM_BBAB = 0x51, _MM_PERM_BBAC = 0x52, _MM_PERM_BBAD = 0x53,
  _MM_PERM_BBBA = 0x54, _MM_PERM_BBBB = 0x55, _MM_PERM_BBBC = 0x56,
  _MM_PERM_BBBD = 0x57, _MM_PERM_BBCA = 0x58, _MM_PERM_BBCB = 0x59,
  _MM_PERM_BBCC = 0x5A, _MM_PERM_BBCD = 0x5B, _MM_PERM_BBDA = 0x5C,
  _MM_PERM_BBDB = 0x5D, _MM_PERM_BBDC = 0x5E, _MM_PERM_BBDD = 0x5F,
  _MM_PERM_BCAA = 0x60, _MM_PERM_BCAB = 0x61, _MM_PERM_BCAC = 0x62,
  _MM_PERM_BCAD = 0x63, _MM_PERM_BCBA = 0x64, _MM_PERM_BCBB = 0x65,
  _MM_PERM_BCBC = 0x66, _MM_PERM_BCBD = 0x67, _MM_PERM_BCCA = 0x68,
  _MM_PERM_BCCB = 0x69, _MM_PERM_BCCC = 0x6A, _MM_PERM_BCCD = 0x6B,
  _MM_PERM_BCDA = 0x6C, _MM_PERM_BCDB = 0x6D, _MM_PERM_BCDC = 0x6E,
  _MM_PERM_BCDD = 0x6F, _MM_PERM_BDAA = 0x70, _MM_PERM_BDAB = 0x71,
  _MM_PERM_BDAC = 0x72, _MM_PERM_BDAD = 0x73, _MM_PERM_BDBA = 0x74,
  _MM_PERM_BDBB = 0x75, _MM_PERM_BDBC = 0x76, _MM_PERM_BDBD = 0x77,
  _MM_PERM_BDCA = 0x78, _MM_PERM_BDCB = 0x79, _MM_PERM_BDCC = 0x7A,
  _MM_PERM_BDCD = 0x7B, _MM_PERM_BDDA = 0x7C, _MM_PERM_BDDB = 0x7D,
  _MM_PERM_BDDC = 0x7E, _MM_PERM_BDDD = 0x7F, _MM_PERM_CAAA = 0x80,
  _MM_PERM_CAAB = 0x81, _MM_PERM_CAAC = 0x82, _MM_PERM_CAAD = 0x83,
  _MM_PERM_CABA = 0x84, _MM_PERM_CABB = 0x85, _MM_PERM_CABC = 0x86,
  _MM_PERM_CABD = 0x87, _MM_PERM_CACA = 0x88, _MM_PERM_CACB = 0x89,
  _MM_PERM_CACC = 0x8A, _MM_PERM_CACD = 0x8B, _MM_PERM_CADA = 0x8C,
  _MM_PERM_CADB = 0x8D, _MM_PERM_CADC = 0x8E, _MM_PERM_CADD = 0x8F,
  _MM_PERM_CBAA = 0x90, _MM_PERM_CBAB = 0x91, _MM_PERM_CBAC = 0x92,
  _MM_PERM_CBAD = 0x93, _MM_PERM_CBBA = 0x94, _MM_PERM_CBBB = 0x95,
  _MM_PERM_CBBC = 0x96, _MM_PERM_CBBD = 0x97, _MM_PERM_CBCA = 0x98,
  _MM_PERM_CBCB = 0x99, _MM_PERM_CBCC = 0x9A, _MM_PERM_CBCD = 0x9B,
  _MM_PERM_CBDA = 0x9C, _MM_PERM_CBDB = 0x9D, _MM_PERM_CBDC = 0x9E,
  _MM_PERM_CBDD = 0x9F, _MM_PERM_CCAA = 0xA0, _MM_PERM_CCAB = 0xA1,
  _MM_PERM_CCAC = 0xA2, _MM_PERM_CCAD = 0xA3, _MM_PERM_CCBA = 0xA4,
  _MM_PERM_CCBB = 0xA5, _MM_PERM_CCBC = 0xA6, _MM_PERM_CCBD = 0xA7,
  _MM_PERM_CCCA = 0xA8, _MM_PERM_CCCB = 0xA9, _MM_PERM_CCCC = 0xAA,
  _MM_PERM_CCCD = 0xAB, _MM_PERM_CCDA = 0xAC, _MM_PERM_CCDB = 0xAD,
  _MM_PERM_CCDC = 0xAE, _MM_PERM_CCDD = 0xAF, _MM_PERM_CDAA = 0xB0,
  _MM_PERM_CDAB = 0xB1, _MM_PERM_CDAC = 0xB2, _MM_PERM_CDAD = 0xB3,
  _MM_PERM_CDBA = 0xB4, _MM_PERM_CDBB = 0xB5, _MM_PERM_CDBC = 0xB6,
  _MM_PERM_CDBD = 0xB7, _MM_PERM_CDCA = 0xB8, _MM_PERM_CDCB = 0xB9,
  _MM_PERM_CDCC = 0xBA, _MM_PERM_CDCD = 0xBB, _MM_PERM_CDDA = 0xBC,
  _MM_PERM_CDDB = 0xBD, _MM_PERM_CDDC = 0xBE, _MM_PERM_CDDD = 0xBF,
  _MM_PERM_DAAA = 0xC0, _MM_PERM_DAAB = 0xC1, _MM_PERM_DAAC = 0xC2,
  _MM_PERM_DAAD = 0xC3, _MM_PERM_DABA = 0xC4, _MM_PERM_DABB = 0xC5,
  _MM_PERM_DABC = 0xC6, _MM_PERM_DABD = 0xC7, _MM_PERM_DACA = 0xC8,
  _MM_PERM_DACB = 0xC9, _MM_PERM_DACC = 0xCA, _MM_PERM_DACD = 0xCB,
  _MM_PERM_DADA = 0xCC, _MM_PERM_DADB = 0xCD, _MM_PERM_DADC = 0xCE,
  _MM_PERM_DADD = 0xCF, _MM_PERM_DBAA = 0xD0, _MM_PERM_DBAB = 0xD1,
  _MM_PERM_DBAC = 0xD2, _MM_PERM_DBAD = 0xD3, _MM_PERM_DBBA = 0xD4,
  _MM_PERM_DBBB = 0xD5, _MM_PERM_DBBC = 0xD6, _MM_PERM_DBBD = 0xD7,
  _MM_PERM_DBCA = 0xD8, _MM_PERM_DBCB = 0xD9, _MM_PERM_DBCC = 0xDA,
  _MM_PERM_DBCD = 0xDB, _MM_PERM_DBDA = 0xDC, _MM_PERM_DBDB = 0xDD,
  _MM_PERM_DBDC = 0xDE, _MM_PERM_DBDD = 0xDF, _MM_PERM_DCAA = 0xE0,
  _MM_PERM_DCAB = 0xE1, _MM_PERM_DCAC = 0xE2, _MM_PERM_DCAD = 0xE3,
  _MM_PERM_DCBA = 0xE4, _MM_PERM_DCBB = 0xE5, _MM_PERM_DCBC = 0xE6,
  _MM_PERM_DCBD = 0xE7, _MM_PERM_DCCA = 0xE8, _MM_PERM_DCCB = 0xE9,
  _MM_PERM_DCCC = 0xEA, _MM_PERM_DCCD = 0xEB, _MM_PERM_DCDA = 0xEC,
  _MM_PERM_DCDB = 0xED, _MM_PERM_DCDC = 0xEE, _MM_PERM_DCDD = 0xEF,
  _MM_PERM_DDAA = 0xF0, _MM_PERM_DDAB = 0xF1, _MM_PERM_DDAC = 0xF2,
  _MM_PERM_DDAD = 0xF3, _MM_PERM_DDBA = 0xF4, _MM_PERM_DDBB = 0xF5,
  _MM_PERM_DDBC = 0xF6, _MM_PERM_DDBD = 0xF7, _MM_PERM_DDCA = 0xF8,
  _MM_PERM_DDCB = 0xF9, _MM_PERM_DDCC = 0xFA, _MM_PERM_DDCD = 0xFB,
  _MM_PERM_DDDA = 0xFC, _MM_PERM_DDDB = 0xFD, _MM_PERM_DDDC = 0xFE,
  _MM_PERM_DDDD = 0xFF
} _MM_PERM_ENUM;

typedef enum
{
  _MM_MANT_NORM_1_2,
  _MM_MANT_NORM_p5_2,
  _MM_MANT_NORM_p5_1,
  _MM_MANT_NORM_p75_1p5
} _MM_MANTISSA_NORM_ENUM;

typedef enum
{
  _MM_MANT_SIGN_src,
  _MM_MANT_SIGN_zero,
  _MM_MANT_SIGN_nan
} _MM_MANTISSA_SIGN_ENUM;
# 180 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_setzero_si512(void)
{
  return __extension__ (__m512i)(__v8di){ 0, 0, 0, 0, 0, 0, 0, 0 };
}



static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_undefined_pd(void)
{
  return (__m512d)__builtin_ia32_undef512();
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_undefined(void)
{
  return (__m512)__builtin_ia32_undef512();
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_undefined_ps(void)
{
  return (__m512)__builtin_ia32_undef512();
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_undefined_epi32(void)
{
  return (__m512i)__builtin_ia32_undef512();
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_broadcastd_epi32 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v4si) __A, (__v4si) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcastd_epi32 (__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__M,
                                             (__v16si) _mm512_broadcastd_epi32(__A),
                                             (__v16si) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcastd_epi32 (__mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__M,
                                             (__v16si) _mm512_broadcastd_epi32(__A),
                                             (__v16si) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_broadcastq_epi64 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v2di) __A, (__v2di) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcastq_epi64 (__m512i __O, __mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                             (__v8di) _mm512_broadcastq_epi64(__A),
                                             (__v8di) __O);

}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                             (__v8di) _mm512_broadcastq_epi64(__A),
                                             (__v8di) _mm512_setzero_si512());
}


static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_setzero_ps(void)
{
  return __extension__ (__m512){ 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,
                                 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f };
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_setzero_pd(void)
{
  return __extension__ (__m512d){ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set1_ps(float __w)
{
  return __extension__ (__m512){ __w, __w, __w, __w, __w, __w, __w, __w,
                                 __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set1_pd(double __w)
{
  return __extension__ (__m512d){ __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set1_epi8(char __w)
{
  return __extension__ (__m512i)(__v64qi){
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set1_epi16(short __w)
{
  return __extension__ (__m512i)(__v32hi){
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set1_epi32(int __s)
{
  return __extension__ (__m512i)(__v16si){
    __s, __s, __s, __s, __s, __s, __s, __s,
    __s, __s, __s, __s, __s, __s, __s, __s };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_set1_epi32(__mmask16 __M, int __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__M,
                                             (__v16si)_mm512_set1_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set1_epi64(long long __d)
{
  return __extension__(__m512i)(__v8di){ __d, __d, __d, __d, __d, __d, __d, __d };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_set1_epi64(__mmask8 __M, long long __A)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                             (__v8di)_mm512_set1_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_broadcastss_ps(__m128 __A)
{
  return (__m512)__builtin_shufflevector((__v4sf) __A, (__v4sf) __A,
                                         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set4_epi32 (int __A, int __B, int __C, int __D)
{
  return __extension__ (__m512i)(__v16si)
   { __D, __C, __B, __A, __D, __C, __B, __A,
     __D, __C, __B, __A, __D, __C, __B, __A };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set4_epi64 (long long __A, long long __B, long long __C,
       long long __D)
{
  return __extension__ (__m512i) (__v8di)
   { __D, __C, __B, __A, __D, __C, __B, __A };
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set4_pd (double __A, double __B, double __C, double __D)
{
  return __extension__ (__m512d)
   { __D, __C, __B, __A, __D, __C, __B, __A };
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set4_ps (float __A, float __B, float __C, float __D)
{
  return __extension__ (__m512)
   { __D, __C, __B, __A, __D, __C, __B, __A,
     __D, __C, __B, __A, __D, __C, __B, __A };
}
# 392 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_broadcastsd_pd(__m128d __A)
{
  return (__m512d)__builtin_shufflevector((__v2df) __A, (__v2df) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0);
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castpd256_pd512(__m256d __a)
{
  return __builtin_shufflevector(__a, __builtin_nondeterministic_value(__a), 0,
                                 1, 2, 3, 4, 5, 6, 7);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castps256_ps512(__m256 __a)
{
  return __builtin_shufflevector(__a, __builtin_nondeterministic_value(__a), 0,
                                 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}

static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castpd512_pd128(__m512d __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castpd512_pd256 (__m512d __A)
{
  return __builtin_shufflevector(__A, __A, 0, 1, 2, 3);
}

static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castps512_ps128(__m512 __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castps512_ps256 (__m512 __A)
{
  return __builtin_shufflevector(__A, __A, 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castpd_ps (__m512d __A)
{
  return (__m512) (__A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castpd_si512 (__m512d __A)
{
  return (__m512i) (__A);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castpd128_pd512 (__m128d __A)
{
  __m256d __B = __builtin_nondeterministic_value(__B);
  return __builtin_shufflevector(
      __builtin_shufflevector(__A, __builtin_nondeterministic_value(__A), 0, 1, 2, 3),
      __B, 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castps_pd (__m512 __A)
{
  return (__m512d) (__A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castps_si512 (__m512 __A)
{
  return (__m512i) (__A);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castps128_ps512 (__m128 __A)
{
  __m256 __B = __builtin_nondeterministic_value(__B);
  return __builtin_shufflevector(
      __builtin_shufflevector(__A, __builtin_nondeterministic_value(__A), 0, 1, 2, 3, 4, 5, 6, 7),
      __B, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castsi128_si512 (__m128i __A)
{
  __m256i __B = __builtin_nondeterministic_value(__B);
  return __builtin_shufflevector(
      __builtin_shufflevector(__A, __builtin_nondeterministic_value(__A), 0, 1, 2, 3),
      __B, 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castsi256_si512 (__m256i __A)
{
   return __builtin_shufflevector( __A, __builtin_nondeterministic_value(__A), 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castsi512_ps (__m512i __A)
{
  return (__m512) (__A);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castsi512_pd (__m512i __A)
{
  return (__m512d) (__A);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castsi512_si128 (__m512i __A)
{
  return (__m128i)__builtin_shufflevector(__A, __A , 0, 1);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_castsi512_si256 (__m512i __A)
{
  return (__m256i)__builtin_shufflevector(__A, __A , 0, 1, 2, 3);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_int2mask(int __a)
{
  return (__mmask16)__a;
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_mask2int(__mmask16 __a)
{
  return (int)__a;
}
# 545 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_zextpd128_pd512(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)_mm_setzero_pd(), 0, 1, 2, 3, 2, 3, 2, 3);
}
# 564 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_zextpd256_pd512(__m256d __a)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)_mm256_setzero_pd(), 0, 1, 2, 3, 4, 5, 6, 7);
}
# 582 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_zextps128_ps512(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)_mm_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7);
}
# 600 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_zextps256_ps512(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)_mm256_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}
# 618 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_zextsi128_si512(__m128i __a)
{
  return __builtin_shufflevector((__v2di)__a, (__v2di)_mm_setzero_si128(), 0, 1, 2, 3, 2, 3, 2, 3);
}
# 636 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_zextsi256_si512(__m256i __a)
{
  return __builtin_shufflevector((__v4di)__a, (__v4di)_mm256_setzero_si256(), 0, 1, 2, 3, 4, 5, 6, 7);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_and_epi32(__m512i __a, __m512i __b)
{
  return (__m512i)((__v16su)__a & (__v16su)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_and_epi32(__m512i __src, __mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__k,
                (__v16si) _mm512_and_epi32(__a, __b),
                (__v16si) __src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_and_epi32(__mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i) _mm512_mask_and_epi32(_mm512_setzero_si512 (),
                                         __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_and_epi64(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a & (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_and_epi64(__m512i __src, __mmask8 __k, __m512i __a, __m512i __b)
{
    return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __k,
                (__v8di) _mm512_and_epi64(__a, __b),
                (__v8di) __src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_and_epi64(__mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i) _mm512_mask_and_epi64(_mm512_setzero_si512 (),
                                         __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_andnot_si512 (__m512i __A, __m512i __B)
{
  return (__m512i)(~(__v8du)__A & (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_andnot_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i)(~(__v16su)__A & (__v16su)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_andnot_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_andnot_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_andnot_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)_mm512_mask_andnot_epi32(_mm512_setzero_si512(),
                                           __U, __A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_andnot_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)(~(__v8du)__A & (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_andnot_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_andnot_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_andnot_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)_mm512_mask_andnot_epi64(_mm512_setzero_si512(),
                                           __U, __A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_or_epi32(__m512i __a, __m512i __b)
{
  return (__m512i)((__v16su)__a | (__v16su)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_or_epi32(__m512i __src, __mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__k,
                                             (__v16si)_mm512_or_epi32(__a, __b),
                                             (__v16si)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_or_epi32(__mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_or_epi32(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_or_epi64(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a | (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_or_epi64(__m512i __src, __mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__k,
                                             (__v8di)_mm512_or_epi64(__a, __b),
                                             (__v8di)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_or_epi64(__mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_or_epi64(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_xor_epi32(__m512i __a, __m512i __b)
{
  return (__m512i)((__v16su)__a ^ (__v16su)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_xor_epi32(__m512i __src, __mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__k,
                                            (__v16si)_mm512_xor_epi32(__a, __b),
                                            (__v16si)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_xor_epi32(__mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_xor_epi32(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_xor_epi64(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a ^ (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_xor_epi64(__m512i __src, __mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__k,
                                             (__v8di)_mm512_xor_epi64(__a, __b),
                                             (__v8di)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_xor_epi64(__mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_xor_epi64(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_and_si512(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a & (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_or_si512(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a | (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_xor_si512(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a ^ (__v8du)__b);
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_add_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a + (__v8df)__b);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_add_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a + (__v16sf)__b);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mul_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a * (__v8df)__b);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mul_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a * (__v16sf)__b);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sub_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a - (__v8df)__b);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sub_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a - (__v16sf)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_add_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A + (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_add_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_add_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_add_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_add_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sub_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A - (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sub_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_sub_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sub_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_sub_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_add_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A + (__v16su) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_add_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_add_epi32(__A, __B),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_add_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_add_epi32(__A, __B),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sub_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A - (__v16su) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sub_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_sub_epi32(__A, __B),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sub_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_sub_epi32(__A, __B),
                                             (__v16si)_mm512_setzero_si512());
}
# 971 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_max_pd(__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_maxpd512((__v8df) __A, (__v8df) __B,
                                           0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_max_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_max_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_max_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}
# 1008 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_max_ps(__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_maxps512((__v16sf) __A, (__v16sf) __B,
                                          0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_max_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_max_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_max_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_maxss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_maxss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}
# 1067 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_maxsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_maxsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 1103 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512i
__attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_max_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_max_epi32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_max_epi32(__A, __B),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_epi32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_max_epi32(__A, __B),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_max_epu32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v16su)__A, (__v16su)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_max_epu32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_max_epu32(__A, __B),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_epu32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_max_epu32(__A, __B),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_max_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v8di)__A, (__v8di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_max_epi64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_max_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_epi64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_max_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_max_epu64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v8du)__A, (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_max_epu64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_max_epu64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_epu64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_max_epu64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}
# 1206 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_min_pd(__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_minpd512((__v8df) __A, (__v8df) __B,
                                           0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_min_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_min_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_min_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}
# 1243 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_min_ps(__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_minps512((__v16sf) __A, (__v16sf) __B,
                                          0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_min_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_min_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_min_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_minss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_minss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}
# 1302 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_minsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_minsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 1338 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512i
__attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_min_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_min_epi32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_min_epi32(__A, __B),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_epi32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_min_epi32(__A, __B),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_min_epu32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v16su)__A, (__v16su)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_min_epu32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_min_epu32(__A, __B),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_epu32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_min_epu32(__A, __B),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_min_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v8di)__A, (__v8di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_min_epi64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_min_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_epi64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_min_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_min_epu64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v8du)__A, (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_min_epu64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_min_epu64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_epu64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_min_epu64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mul_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_pmuldq512((__v16si)__X, (__v16si) __Y);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mul_epi32(__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epi32(__X, __Y),
                                             (__v8di)__W);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_mul_epi32(__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epi32(__X, __Y),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mul_epu32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_pmuludq512((__v16si)__X, (__v16si)__Y);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mul_epu32(__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epu32(__X, __Y),
                                             (__v8di)__W);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_mul_epu32(__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epu32(__X, __Y),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mullo_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A * (__v16su) __B);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_mullo_epi32(__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_mullo_epi32(__A, __B),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mullo_epi32(__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_mullo_epi32(__A, __B),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mullox_epi64 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v8du) __A * (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mullox_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_mullox_epi64(__A, __B),
                                             (__v8di)__W);
}
# 1518 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sqrt_pd(__m512d __A)
{
  return (__m512d)__builtin_ia32_sqrtpd512((__v8df)__A,
                                           0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sqrt_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_sqrt_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sqrt_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_sqrt_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}
# 1554 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sqrt_ps(__m512 __A)
{
  return (__m512)__builtin_ia32_sqrtps512((__v16sf)__A,
                                          0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sqrt_ps(__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_sqrt_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sqrt_ps( __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_sqrt_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_rsqrt14_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
                 (__v8df)
                 _mm512_setzero_pd (),
                 (__mmask8) -1);}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_rsqrt14_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
                  (__v8df) __W,
                  (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_rsqrt14_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
                  (__v8df)
                  _mm512_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_rsqrt14_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
                (__v16sf)
                _mm512_setzero_ps (),
                (__mmask16) -1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_rsqrt14_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
                 (__v16sf) __W,
                 (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_rsqrt14_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
                 (__v16sf)
                 _mm512_setzero_ps (),
                 (__mmask16) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_rsqrt14_ss(__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __A,
             (__v4sf) __B,
             (__v4sf)
             _mm_setzero_ps (),
             (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_rsqrt14_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rsqrt14_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) _mm_setzero_ps (),
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_rsqrt14_sd(__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rsqrt14sd_mask ((__v2df) __A,
              (__v2df) __B,
              (__v2df)
              _mm_setzero_pd (),
              (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_rsqrt14_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rsqrt14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rsqrt14_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rsqrt14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) _mm_setzero_pd (),
          (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_rcp14_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
               (__v8df)
               _mm512_setzero_pd (),
               (__mmask8) -1);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_rcp14_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
                (__v8df) __W,
                (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_rcp14_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
                (__v8df)
                _mm512_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_rcp14_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
              (__v16sf)
              _mm512_setzero_ps (),
              (__mmask16) -1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_rcp14_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_rcp14_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
                   (__v16sf)
                   _mm512_setzero_ps (),
                   (__mmask16) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_rcp14_ss(__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __A,
                 (__v4sf) __B,
                 (__v4sf)
                 _mm_setzero_ps (),
                 (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_rcp14_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rcp14_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) _mm_setzero_ps (),
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_rcp14_sd(__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rcp14sd_mask ((__v2df) __A,
            (__v2df) __B,
            (__v2df)
            _mm_setzero_pd (),
            (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_rcp14_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rcp14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rcp14_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rcp14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) _mm_setzero_pd (),
          (__mmask8) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_floor_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                                                  (0x00 | 0x01),
                                                  (__v16sf) __A, (unsigned short)-1,
                                                  0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_floor_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                   (0x00 | 0x01),
                   (__v16sf) __W, __U,
                   0x04);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_floor_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                                                   (0x00 | 0x01),
                                                   (__v8df) __A, (unsigned char)-1,
                                                   0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_floor_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                (0x00 | 0x01),
                (__v8df) __W, __U,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_ceil_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                   (0x00 | 0x02),
                   (__v16sf) __W, __U,
                   0x04);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_ceil_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                                                  (0x00 | 0x02),
                                                  (__v16sf) __A, (unsigned short)-1,
                                                  0x04);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_ceil_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                                                   (0x00 | 0x02),
                                                   (__v8df) __A, (unsigned char)-1,
                                                   0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_ceil_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                (0x00 | 0x02),
                (__v8df) __W, __U,
                0x04);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_abs_epi64(__m512i __A)
{
  return (__m512i)__builtin_elementwise_abs((__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_abs_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_abs_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_abs_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_abs_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_abs_epi32(__m512i __A)
{
  return (__m512i)__builtin_elementwise_abs((__v16si) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_abs_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                             (__v16si)_mm512_abs_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_abs_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                             (__v16si)_mm512_abs_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_add_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_add_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_add_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_add_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, _mm_setzero_ps());
}
# 1938 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_add_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_add_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_add_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_add_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, _mm_setzero_pd());
}
# 1967 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_add_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_add_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_add_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_add_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_add_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_add_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_add_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_add_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}
# 2023 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_sub_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_sub_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sub_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_sub_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, _mm_setzero_ps());
}
# 2052 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_sub_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_sub_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sub_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_sub_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, _mm_setzero_pd());
}
# 2082 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sub_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_sub_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sub_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_sub_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sub_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_sub_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sub_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_sub_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}
# 2138 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_mul_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_mul_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mul_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_mul_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, _mm_setzero_ps());
}
# 2167 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_mul_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_mul_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mul_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_mul_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, _mm_setzero_pd());
}
# 2197 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mul_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_mul_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_mul_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_mul_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mul_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_mul_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_mul_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_mul_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}
# 2253 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_div_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_div_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_div_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_div_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, _mm_setzero_ps());
}
# 2283 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_div_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_div_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_div_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_div_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, _mm_setzero_pd());
}
# 2313 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_div_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a/(__v8df)__b);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_div_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_div_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_div_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_div_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_div_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a/(__v16sf)__b);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_div_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_div_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_div_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_div_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}
# 2529 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fmadd_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fmadd_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmadd_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask3 ((__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmadd_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz ((__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fmsub_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fmsub_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmsub_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz ((__v8df) __A,
                                                     (__v8df) __B,
                                                     -(__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fnmadd_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    -(__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fnmadd_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask3 (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fnmadd_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fnmsub_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    -(__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fnmsub_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     -(__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}
# 2733 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fmadd_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fmadd_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmadd_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask3 ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmadd_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fmsub_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fmsub_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmsub_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    -(__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fnmadd_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   -(__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fnmadd_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask3 (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fnmadd_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fnmsub_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   -(__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fnmsub_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    -(__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}
# 2902 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fmaddsub_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                      (__v8df) __B,
                                                      (__v8df) __C,
                                                      (__mmask8) -1,
                                                      0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fmaddsub_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                      (__v8df) __B,
                                                      (__v8df) __C,
                                                      (__mmask8) __U,
                                                      0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmaddsub_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask3 ((__v8df) __A,
                                                       (__v8df) __B,
                                                       (__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmaddsub_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
                                                       (__v8df) __B,
                                                       (__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fmsubadd_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                       (__v8df) __B,
                                                       -(__v8df) __C,
                                                       (__mmask8) -1,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fmsubadd_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                       (__v8df) __B,
                                                       -(__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmsubadd_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
                                                        (__v8df) __B,
                                                        -(__v8df) __C,
                                                        (__mmask8) __U,
                                                        0x04);
}
# 3021 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fmaddsub_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      (__v16sf) __C,
                                                      (__mmask16) -1,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fmaddsub_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      (__v16sf) __C,
                                                      (__mmask16) __U,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmaddsub_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask3 ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       (__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmaddsub_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       (__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_fmsubadd_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      -(__v16sf) __C,
                                                      (__mmask16) -1,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fmsubadd_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      -(__v16sf) __C,
                                                      (__mmask16) __U,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmsubadd_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       -(__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}
# 3098 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmsub_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d)__builtin_ia32_vfmsubpd512_mask3 ((__v8df) __A,
                                                    (__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}







static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmsub_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512)__builtin_ia32_vfmsubps512_mask3 ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}
# 3131 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmsubadd_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d)__builtin_ia32_vfmsubaddpd512_mask3 ((__v8df) __A,
                                                       (__v8df) __B,
                                                       (__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}
# 3148 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmsubadd_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512)__builtin_ia32_vfmsubaddps512_mask3 ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      (__v16sf) __C,
                                                      (__mmask16) __U,
                                                      0x04);
}
# 3165 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fnmadd_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    -(__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}
# 3182 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fnmadd_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   -(__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}
# 3206 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fnmsub_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    -(__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fnmsub_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_mask3 (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}
# 3240 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_fnmsub_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   -(__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask3_fnmsub_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmsubps512_mask3 (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}





static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutex2var_epi32(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpermi2vard512((__v16si)__A, (__v16si) __I,
                                                (__v16si) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutex2var_epi32(__m512i __A, __mmask16 __U, __m512i __I,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                              (__v16si)_mm512_permutex2var_epi32(__A, __I, __B),
                              (__v16si)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_epi32(__m512i __A, __m512i __I, __mmask16 __U,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                              (__v16si)_mm512_permutex2var_epi32(__A, __I, __B),
                              (__v16si)__I);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_epi32(__mmask16 __U, __m512i __A, __m512i __I,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                              (__v16si)_mm512_permutex2var_epi32(__A, __I, __B),
                              (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutex2var_epi64(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpermi2varq512((__v8di)__A, (__v8di) __I,
                                                (__v8di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutex2var_epi64(__m512i __A, __mmask8 __U, __m512i __I,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                               (__v8di)_mm512_permutex2var_epi64(__A, __I, __B),
                               (__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_epi64(__m512i __A, __m512i __I, __mmask8 __U,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                               (__v8di)_mm512_permutex2var_epi64(__A, __I, __B),
                               (__v8di)__I);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_epi64(__mmask8 __U, __m512i __A, __m512i __I,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                               (__v8di)_mm512_permutex2var_epi64(__A, __I, __B),
                               (__v8di)_mm512_setzero_si512());
}
# 3393 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_blend_pd(__mmask8 __U, __m512d __A, __m512d __W)
{
  return (__m512d) __builtin_ia32_selectpd_512 ((__mmask8) __U,
                 (__v8df) __W,
                 (__v8df) __A);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_blend_ps(__mmask16 __U, __m512 __A, __m512 __W)
{
  return (__m512) __builtin_ia32_selectps_512 ((__mmask16) __U,
                (__v16sf) __W,
                (__v16sf) __A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_blend_epi64(__mmask8 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __U,
                (__v8di) __W,
                (__v8di) __A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_blend_epi32(__mmask16 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectd_512 ((__mmask16) __U,
                (__v16si) __W,
                (__v16si) __A);
}
# 3555 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvttps_epu32(__m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) -1,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttps_epu32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
                   (__v16si) __W,
                   (__mmask16) __U,
                   0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttps_epu32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
                   (__v16si) _mm512_setzero_si512 (),
                   (__mmask16) __U,
                   0x04);
}
# 3613 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepu32_ps (__m512i __A)
{
  return (__m512)__builtin_convertvector((__v16su)__A, __v16sf);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu32_ps (__m512 __W, __mmask16 __U, __m512i __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_cvtepu32_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu32_ps (__mmask16 __U, __m512i __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_cvtepu32_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi32_pd(__m256i __A)
{
  return (__m512d)__builtin_convertvector((__v8si)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_pd (__m512d __W, __mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepi32_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_pd (__mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepi32_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi32lo_pd(__m512i __A)
{
  return (__m512d) _mm512_cvtepi32_pd(_mm512_castsi512_si256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi32lo_pd(__m512d __W, __mmask8 __U,__m512i __A)
{
  return (__m512d) _mm512_mask_cvtepi32_pd(__W, __U, _mm512_castsi512_si256(__A));
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi32_ps (__m512i __A)
{
  return (__m512)__builtin_convertvector((__v16si)__A, __v16sf);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_ps (__m512 __W, __mmask16 __U, __m512i __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_cvtepi32_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_ps (__mmask16 __U, __m512i __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_cvtepi32_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepu32_pd(__m256i __A)
{
  return (__m512d)__builtin_convertvector((__v8su)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu32_pd (__m512d __W, __mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepu32_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu32_pd (__mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepu32_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepu32lo_pd(__m512i __A)
{
  return (__m512d) _mm512_cvtepu32_pd(_mm512_castsi512_si256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu32lo_pd(__m512d __W, __mmask8 __U,__m512i __A)
{
  return (__m512d) _mm512_mask_cvtepu32_pd(__W, __U, _mm512_castsi512_si256(__A));
}
# 3740 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtpd_ps (__m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
                (__v8sf) _mm256_undefined_ps (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtpd_ps (__m256 __W, __mmask8 __U, __m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
                (__v8sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_ps (__mmask8 __U, __m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
                (__v8sf) _mm256_setzero_ps (),
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtpd_pslo (__m512d __A)
{
  return (__m512) __builtin_shufflevector((__v8sf) _mm512_cvtpd_ps(__A),
                (__v8sf) _mm256_setzero_ps (),
                0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtpd_pslo (__m512 __W, __mmask8 __U,__m512d __A)
{
  return (__m512) __builtin_shufflevector (
                (__v8sf) _mm512_mask_cvtpd_ps (_mm512_castps512_ps256(__W),
                                               __U, __A),
                (__v8sf) _mm256_setzero_ps (),
                0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}
# 3820 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtph_ps(__m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
                (__v16sf)
                _mm512_setzero_ps (),
                (__mmask16) -1,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtph_ps (__m512 __W, __mmask16 __U, __m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
                 (__v16sf) __W,
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtph_ps (__mmask16 __U, __m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
                 (__v16sf) _mm512_setzero_ps (),
                 (__mmask16) __U,
                 0x04);
}
# 3863 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvttpd_epi32(__m512d __a)
{
  return (__m256i)__builtin_ia32_cvttpd2dq512_mask((__v8df) __a,
                                                   (__v8si)_mm256_setzero_si256(),
                                                   (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
                  (__v8si) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttpd_epi32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
                  (__v8si) _mm256_setzero_si256 (),
                  (__mmask8) __U,
                  0x04);
}
# 3905 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvttps_epi32(__m512 __a)
{
  return (__m512i)
    __builtin_ia32_cvttps2dq512_mask((__v16sf) __a,
                                     (__v16si) _mm512_setzero_si512 (),
                                     (__mmask16) -1, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttps_epi32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
                  (__v16si) __W,
                  (__mmask16) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttps_epi32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
                  (__v16si) _mm512_setzero_si512 (),
                  (__mmask16) __U,
                  0x04);
}
# 3947 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtps_epi32 (__m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
                 (__v16si) _mm512_undefined_epi32 (),
                 (__mmask16) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtps_epi32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
                 (__v16si) __W,
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtps_epi32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
                 (__v16si)
                 _mm512_setzero_si512 (),
                 (__mmask16) __U,
                 0x04);
}
# 3990 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtpd_epi32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
                 (__v8si)
                 _mm256_undefined_si256 (),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
                 (__v8si) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_epi32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U,
                 0x04);
}
# 4034 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtps_epu32 ( __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A, (__v16si) _mm512_undefined_epi32 (),


                  (__mmask16) -1, 0x04);

}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtps_epu32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
                  (__v16si) __W,
                  (__mmask16) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtps_epu32 ( __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) __U ,
                  0x04);
}
# 4078 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtpd_epu32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_undefined_si256 (),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
                  (__v8si) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_epu32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U,
                  0x04);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtsd_f64(__m512d __a)
{
  return __a[0];
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtss_f32(__m512 __a)
{
  return __a[0];
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_unpackhi_pd(__m512d __a, __m512d __b)
{
  return (__m512d)__builtin_shufflevector((__v8df)__a, (__v8df)__b,
                                          1, 9, 1+2, 9+2, 1+4, 9+4, 1+6, 9+6);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_unpackhi_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpackhi_pd(__A, __B),
                                           (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_pd(__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpackhi_pd(__A, __B),
                                           (__v8df)_mm512_setzero_pd());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_unpacklo_pd(__m512d __a, __m512d __b)
{
  return (__m512d)__builtin_shufflevector((__v8df)__a, (__v8df)__b,
                                          0, 8, 0+2, 8+2, 0+4, 8+4, 0+6, 8+6);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_unpacklo_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpacklo_pd(__A, __B),
                                           (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpacklo_pd(__A, __B),
                                           (__v8df)_mm512_setzero_pd());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_unpackhi_ps(__m512 __a, __m512 __b)
{
  return (__m512)__builtin_shufflevector((__v16sf)__a, (__v16sf)__b,
                                         2, 18, 3, 19,
                                         2+4, 18+4, 3+4, 19+4,
                                         2+8, 18+8, 3+8, 19+8,
                                         2+12, 18+12, 3+12, 19+12);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_unpackhi_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpackhi_ps(__A, __B),
                                          (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpackhi_ps(__A, __B),
                                          (__v16sf)_mm512_setzero_ps());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_unpacklo_ps(__m512 __a, __m512 __b)
{
  return (__m512)__builtin_shufflevector((__v16sf)__a, (__v16sf)__b,
                                         0, 16, 1, 17,
                                         0+4, 16+4, 1+4, 17+4,
                                         0+8, 16+8, 1+8, 17+8,
                                         0+12, 16+12, 1+12, 17+12);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_unpacklo_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpacklo_ps(__A, __B),
                                          (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpacklo_ps(__A, __B),
                                          (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_unpackhi_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v16si)__A, (__v16si)__B,
                                          2, 18, 3, 19,
                                          2+4, 18+4, 3+4, 19+4,
                                          2+8, 18+8, 3+8, 19+8,
                                          2+12, 18+12, 3+12, 19+12);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_unpackhi_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpackhi_epi32(__A, __B),
                                       (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpackhi_epi32(__A, __B),
                                       (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_unpacklo_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v16si)__A, (__v16si)__B,
                                          0, 16, 1, 17,
                                          0+4, 16+4, 1+4, 17+4,
                                          0+8, 16+8, 1+8, 17+8,
                                          0+12, 16+12, 1+12, 17+12);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_unpacklo_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpacklo_epi32(__A, __B),
                                       (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpacklo_epi32(__A, __B),
                                       (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_unpackhi_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v8di)__A, (__v8di)__B,
                                          1, 9, 1+2, 9+2, 1+4, 9+4, 1+6, 9+6);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_unpackhi_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpackhi_epi64(__A, __B),
                                        (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpackhi_epi64(__A, __B),
                                        (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_unpacklo_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v8di)__A, (__v8di)__B,
                                          0, 8, 0+2, 8+2, 0+4, 8+4, 0+6, 8+6);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_unpacklo_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpacklo_epi64(__A, __B),
                                        (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpacklo_epi64(__A, __B),
                                        (__v8di)_mm512_setzero_si512());
}




static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_loadu_si512 (void const *__P)
{
  struct __loadu_si512 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_si512*)__P)->__v;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_loadu_epi32 (void const *__P)
{
  struct __loadu_epi32 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi32*)__P)->__v;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_loadu_epi32 (__m512i __W, __mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const int *) __P,
                  (__v16si) __W,
                  (__mmask16) __U);
}


static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_loadu_epi32(__mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const int *)__P,
                                                     (__v16si)
                                                     _mm512_setzero_si512 (),
                                                     (__mmask16) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_loadu_epi64 (void const *__P)
{
  struct __loadu_epi64 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi64*)__P)->__v;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_loadu_epi64 (__m512i __W, __mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const long long *) __P,
                  (__v8di) __W,
                  (__mmask8) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_loadu_epi64(__mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const long long *)__P,
                                                     (__v8di)
                                                     _mm512_setzero_si512 (),
                                                     (__mmask8) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_loadu_ps (__m512 __W, __mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadups512_mask ((const float *) __P,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_loadu_ps(__mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadups512_mask ((const float *)__P,
                                                  (__v16sf)
                                                  _mm512_setzero_ps (),
                                                  (__mmask16) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_loadu_pd (__m512d __W, __mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadupd512_mask ((const double *) __P,
                (__v8df) __W,
                (__mmask8) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_loadu_pd(__mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadupd512_mask ((const double *)__P,
                                                   (__v8df)
                                                   _mm512_setzero_pd (),
                                                   (__mmask8) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_loadu_pd(void const *__p)
{
  struct __loadu_pd {
    __m512d_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_pd*)__p)->__v;
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_loadu_ps(void const *__p)
{
  struct __loadu_ps {
    __m512_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ps*)__p)->__v;
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_load_ps(void const *__p)
{
  return *(const __m512*)__p;
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_load_ps (__m512 __W, __mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadaps512_mask ((const __v16sf *) __P,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_load_ps(__mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadaps512_mask ((const __v16sf *)__P,
                                                  (__v16sf)
                                                  _mm512_setzero_ps (),
                                                  (__mmask16) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_load_pd(void const *__p)
{
  return *(const __m512d*)__p;
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_load_pd (__m512d __W, __mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadapd512_mask ((const __v8df *) __P,
                          (__v8df) __W,
                          (__mmask8) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_load_pd(__mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadapd512_mask ((const __v8df *)__P,
                                                   (__v8df)
                                                   _mm512_setzero_pd (),
                                                   (__mmask8) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_load_si512 (void const *__P)
{
  return *(const __m512i *) __P;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_load_epi32 (void const *__P)
{
  return *(const __m512i *) __P;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_load_epi64 (void const *__P)
{
  return *(const __m512i *) __P;
}



static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_storeu_epi64 (void *__P, __m512i __A)
{
  struct __storeu_epi64 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi64*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_storeu_epi64(void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_storedqudi512_mask ((long long *)__P, (__v8di) __A,
                                     (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_storeu_si512 (void *__P, __m512i __A)
{
  struct __storeu_si512 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si512*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_storeu_epi32 (void *__P, __m512i __A)
{
  struct __storeu_epi32 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi32*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_storeu_epi32(void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_storedqusi512_mask ((int *)__P, (__v16si) __A,
                                     (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_storeu_pd(void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_storeupd512_mask ((double *)__P, (__v8df) __A, (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_storeu_pd(void *__P, __m512d __A)
{
  struct __storeu_pd {
    __m512d_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_pd*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_storeu_ps(void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_storeups512_mask ((float *)__P, (__v16sf) __A,
                                   (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_storeu_ps(void *__P, __m512 __A)
{
  struct __storeu_ps {
    __m512_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ps*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_store_pd(void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_storeapd512_mask ((__v8df *)__P, (__v8df) __A, (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_store_pd(void *__P, __m512d __A)
{
  *(__m512d*)__P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_store_ps(void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_storeaps512_mask ((__v16sf *)__P, (__v16sf) __A,
                                   (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_store_ps(void *__P, __m512 __A)
{
  *(__m512*)__P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_store_si512 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_store_epi32 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_store_epi64 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}



static __inline __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_knot(__mmask16 __M)
{
  return __builtin_ia32_knothi(__M);
}
# 4725 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi8_epi32(__m128i __A)
{


  return (__m512i)__builtin_convertvector((__v16qs)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi8_epi32(__m512i __W, __mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepi8_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi8_epi32(__mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepi8_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi8_epi64(__m128i __A)
{


  return (__m512i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__A, (__v16qs)__A, 0, 1, 2, 3, 4, 5, 6, 7), __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi8_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi8_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi8_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi8_epi64(__A),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi32_epi64(__m256i __X)
{
  return (__m512i)__builtin_convertvector((__v8si)__X, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_epi64(__m512i __W, __mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi32_epi64(__X),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_epi64(__mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi32_epi64(__X),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi16_epi32(__m256i __A)
{
  return (__m512i)__builtin_convertvector((__v16hi)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_epi32(__m512i __W, __mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepi16_epi32(__A),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi16_epi32(__mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepi16_epi32(__A),
                                            (__v16si)_mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi16_epi64(__m128i __A)
{
  return (__m512i)__builtin_convertvector((__v8hi)__A, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi16_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi16_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepu8_epi32(__m128i __A)
{
  return (__m512i)__builtin_convertvector((__v16qu)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu8_epi32(__m512i __W, __mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepu8_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu8_epi32(__mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepu8_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepu8_epi64(__m128i __A)
{
  return (__m512i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__A, (__v16qu)__A, 0, 1, 2, 3, 4, 5, 6, 7), __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu8_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu8_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu8_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu8_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepu32_epi64(__m256i __X)
{
  return (__m512i)__builtin_convertvector((__v8su)__X, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu32_epi64(__m512i __W, __mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu32_epi64(__X),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu32_epi64(__mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu32_epi64(__X),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepu16_epi32(__m256i __A)
{
  return (__m512i)__builtin_convertvector((__v16hu)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu16_epi32(__m512i __W, __mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepu16_epi32(__A),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu16_epi32(__mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepu16_epi32(__A),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepu16_epi64(__m128i __A)
{
  return (__m512i)__builtin_convertvector((__v8hu)__A, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu16_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu16_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu16_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_rorv_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_prorvd512((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_rorv_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                           (__v16si)_mm512_rorv_epi32(__A, __B),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_rorv_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                           (__v16si)_mm512_rorv_epi32(__A, __B),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_rorv_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_prorvq512((__v8di)__A, (__v8di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_rorv_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                            (__v8di)_mm512_rorv_epi64(__A, __B),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_rorv_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                            (__v8di)_mm512_rorv_epi64(__A, __B),
                                            (__v8di)_mm512_setzero_si512());
}
# 5061 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_rolv_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_prolvd512((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_rolv_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                           (__v16si)_mm512_rolv_epi32(__A, __B),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_rolv_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                           (__v16si)_mm512_rolv_epi32(__A, __B),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_rolv_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_prolvq512((__v8di)__A, (__v8di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_rolv_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                            (__v8di)_mm512_rolv_epi64(__A, __B),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_rolv_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                            (__v8di)_mm512_rolv_epi64(__A, __B),
                                            (__v8di)_mm512_setzero_si512());
}
# 5131 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_slli_epi32(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_pslldi512((__v16si)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_slli_epi32(__m512i __W, __mmask16 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_slli_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_slli_epi32(__mmask16 __U, __m512i __A, unsigned int __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_slli_epi32(__A, __B),
                                         (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_slli_epi64(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psllqi512((__v8di)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_slli_epi64(__m512i __W, __mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_slli_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_slli_epi64(__mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_slli_epi64(__A, __B),
                                          (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srli_epi32(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psrldi512((__v16si)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srli_epi32(__m512i __W, __mmask16 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srli_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srli_epi32(__mmask16 __U, __m512i __A, unsigned int __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srli_epi32(__A, __B),
                                         (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srli_epi64(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psrlqi512((__v8di)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srli_epi64(__m512i __W, __mmask8 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srli_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srli_epi64(__mmask8 __U, __m512i __A,
                        unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srli_epi64(__A, __B),
                                          (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_load_epi32 (__m512i __W, __mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa32load512_mask ((const __v16si *) __P,
              (__v16si) __W,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_load_epi32 (__mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa32load512_mask ((const __v16si *) __P,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_store_epi32 (void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_movdqa32store512_mask ((__v16si *) __P, (__v16si) __A,
          (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mov_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectd_512 ((__mmask16) __U,
                 (__v16si) __A,
                 (__v16si) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_mov_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectd_512 ((__mmask16) __U,
                 (__v16si) __A,
                 (__v16si) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mov_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __U,
                 (__v8di) __A,
                 (__v8di) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_mov_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __U,
                 (__v8di) __A,
                 (__v8di) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_load_epi64 (__m512i __W, __mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa64load512_mask ((const __v8di *) __P,
              (__v8di) __W,
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_load_epi64 (__mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa64load512_mask ((const __v8di *) __P,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_store_epi64 (void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_movdqa64store512_mask ((__v8di *) __P, (__v8di) __A,
          (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_movedup_pd (__m512d __A)
{
  return (__m512d)__builtin_shufflevector((__v8df)__A, (__v8df)__A,
                                          0, 0, 2, 2, 4, 4, 6, 6);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_movedup_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_movedup_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_movedup_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_movedup_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}
# 5489 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_getexp_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_getexpsd128_round_mask ((__v2df) __A,
                 (__v2df) __B, (__v2df) _mm_setzero_pd(), (__mmask8) -1, 0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_getexp_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_getexpsd128_round_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_getexp_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_getexpsd128_round_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) _mm_setzero_pd (),
          (__mmask8) __U,
          0x04);
}
# 5534 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_getexp_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_getexpss128_round_mask ((__v4sf) __A,
                (__v4sf) __B, (__v4sf) _mm_setzero_ps(), (__mmask8) -1, 0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_getexp_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_getexpss128_round_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_getexp_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_getexpss128_round_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) _mm_setzero_ps (),
          (__mmask8) __U,
          0x04);
}
# 5663 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_kmov (__mmask16 __A)
{
  return __A;
}
# 5682 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sll_epi32(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_pslld512((__v16si) __A, (__v4si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sll_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sll_epi32(__A, __B),
                                          (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sll_epi32(__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sll_epi32(__A, __B),
                                          (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sll_epi64(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psllq512((__v8di)__A, (__v2di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sll_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_sll_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sll_epi64(__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_sll_epi64(__A, __B),
                                           (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sllv_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psllv16si((__v16si)__X, (__v16si)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sllv_epi32(__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_sllv_epi32(__X, __Y),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sllv_epi32(__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_sllv_epi32(__X, __Y),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sllv_epi64(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psllv8di((__v8di)__X, (__v8di)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sllv_epi64(__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_sllv_epi64(__X, __Y),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sllv_epi64(__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_sllv_epi64(__X, __Y),
                                            (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sra_epi32(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrad512((__v16si) __A, (__v4si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sra_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sra_epi32(__A, __B),
                                          (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sra_epi32(__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sra_epi32(__A, __B),
                                          (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_sra_epi64(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psraq512((__v8di)__A, (__v2di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_sra_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_sra_epi64(__A, __B),
                                           (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_sra_epi64(__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_sra_epi64(__A, __B),
                                           (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srav_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrav16si((__v16si)__X, (__v16si)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srav_epi32(__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srav_epi32(__X, __Y),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srav_epi32(__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srav_epi32(__X, __Y),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srav_epi64(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrav8di((__v8di)__X, (__v8di)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srav_epi64(__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srav_epi64(__X, __Y),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srav_epi64(__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srav_epi64(__X, __Y),
                                            (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srl_epi32(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrld512((__v16si) __A, (__v4si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srl_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_srl_epi32(__A, __B),
                                          (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srl_epi32(__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_srl_epi32(__A, __B),
                                          (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srl_epi64(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrlq512((__v8di)__A, (__v2di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srl_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_srl_epi64(__A, __B),
                                           (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srl_epi64(__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_srl_epi64(__A, __B),
                                           (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srlv_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrlv16si((__v16si)__X, (__v16si)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srlv_epi32(__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srlv_epi32(__X, __Y),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srlv_epi32(__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srlv_epi32(__X, __Y),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srlv_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrlv8di((__v8di)__X, (__v8di)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srlv_epi64(__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srlv_epi64(__X, __Y),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srlv_epi64(__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srlv_epi64(__X, __Y),
                                            (__v8di)_mm512_setzero_si512());
}




typedef enum {
  _MM_TERNLOG_A = 0xF0,
  _MM_TERNLOG_B = 0xCC,
  _MM_TERNLOG_C = 0xAA
} _MM_TERNLOG_ENUM;
# 5999 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvtsd_u32 (__m128d __A)
{
  return (unsigned) __builtin_ia32_vcvtsd2usi32 ((__v2df) __A,
             0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvtsd_u64 (__m128d __A)
{
  return (unsigned long long) __builtin_ia32_vcvtsd2usi64 ((__v2df)
                 __A,
                 0x04);
}
# 6037 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvtss_u32 (__m128 __A)
{
  return (unsigned) __builtin_ia32_vcvtss2usi32 ((__v4sf) __A,
             0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvtss_u64 (__m128 __A)
{
  return (unsigned long long) __builtin_ia32_vcvtss2usi64 ((__v4sf)
                 __A,
                 0x04);
}
# 6064 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvttsd_i32 (__m128d __A)
{
  return (int) __builtin_ia32_vcvttsd2si32 ((__v2df) __A,
              0x04);
}
# 6078 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvttsd_i64 (__m128d __A)
{
  return (long long) __builtin_ia32_vcvttsd2si64 ((__v2df) __A,
              0x04);
}





static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvttsd_u32 (__m128d __A)
{
  return (unsigned) __builtin_ia32_vcvttsd2usi32 ((__v2df) __A,
              0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvttsd_u64 (__m128d __A)
{
  return (unsigned long long) __builtin_ia32_vcvttsd2usi64 ((__v2df)
                  __A,
                  0x04);
}
# 6116 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvttss_i32 (__m128 __A)
{
  return (int) __builtin_ia32_vcvttss2si32 ((__v4sf) __A,
              0x04);
}
# 6130 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvttss_i64 (__m128 __A)
{
  return (long long) __builtin_ia32_vcvttss2si64 ((__v4sf) __A,
              0x04);
}





static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvttss_u32 (__m128 __A)
{
  return (unsigned) __builtin_ia32_vcvttss2usi32 ((__v4sf) __A,
              0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvttss_u64 (__m128 __A)
{
  return (unsigned long long) __builtin_ia32_vcvttss2usi64 ((__v4sf)
                  __A,
                  0x04);
}
# 6188 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutevar_pd(__m512d __A, __m512i __C)
{
  return (__m512d)__builtin_ia32_vpermilvarpd512((__v8df)__A, (__v8di)__C);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutevar_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512i __C)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                         (__v8df)_mm512_permutevar_pd(__A, __C),
                                         (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutevar_pd(__mmask8 __U, __m512d __A, __m512i __C)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                         (__v8df)_mm512_permutevar_pd(__A, __C),
                                         (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutevar_ps(__m512 __A, __m512i __C)
{
  return (__m512)__builtin_ia32_vpermilvarps512((__v16sf)__A, (__v16si)__C);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutevar_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512i __C)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                        (__v16sf)_mm512_permutevar_ps(__A, __C),
                                        (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutevar_ps(__mmask16 __U, __m512 __A, __m512i __C)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                        (__v16sf)_mm512_permutevar_ps(__A, __C),
                                        (__v16sf)_mm512_setzero_ps());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutex2var_pd(__m512d __A, __m512i __I, __m512d __B)
{
  return (__m512d)__builtin_ia32_vpermi2varpd512((__v8df)__A, (__v8di)__I,
                                                 (__v8df)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutex2var_pd(__m512d __A, __mmask8 __U, __m512i __I, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                  (__v8df)_mm512_permutex2var_pd(__A, __I, __B),
                                  (__v8df)__A);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_pd(__m512d __A, __m512i __I, __mmask8 __U,
                             __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                  (__v8df)_mm512_permutex2var_pd(__A, __I, __B),
                                  (__v8df)(__m512d)__I);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_pd(__mmask8 __U, __m512d __A, __m512i __I,
                             __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                  (__v8df)_mm512_permutex2var_pd(__A, __I, __B),
                                  (__v8df)_mm512_setzero_pd());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutex2var_ps(__m512 __A, __m512i __I, __m512 __B)
{
  return (__m512)__builtin_ia32_vpermi2varps512((__v16sf)__A, (__v16si)__I,
                                                (__v16sf) __B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutex2var_ps(__m512 __A, __mmask16 __U, __m512i __I, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                 (__v16sf)_mm512_permutex2var_ps(__A, __I, __B),
                                 (__v16sf)__A);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_ps(__m512 __A, __m512i __I, __mmask16 __U, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                 (__v16sf)_mm512_permutex2var_ps(__A, __I, __B),
                                 (__v16sf)(__m512)__I);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_ps(__mmask16 __U, __m512 __A, __m512i __I, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                 (__v16sf)_mm512_permutex2var_ps(__A, __I, __B),
                                 (__v16sf)_mm512_setzero_ps());
}
# 6312 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvttpd_epu32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_undefined_si256 (),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
                  (__v8si) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttpd_epu32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U,
                  0x04);
}
# 6443 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_scalef_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
                (__v8df) __B,
                (__v8df)
                _mm512_undefined_pd (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_scalef_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
                (__v8df) __B,
                (__v8df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_scalef_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
                (__v8df) __B,
                (__v8df)
                _mm512_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 6493 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_scalef_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
               (__v16sf) __B,
               (__v16sf)
               _mm512_undefined_ps (),
               (__mmask16) -1,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_scalef_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
               (__v16sf) __B,
               (__v16sf) __W,
               (__mmask16) __U,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_scalef_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
               (__v16sf) __B,
               (__v16sf)
               _mm512_setzero_ps (),
               (__mmask16) __U,
               0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_scalef_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_scalefsd_round_mask ((__v2df) __A,
              (__v2df)( __B), (__v2df) _mm_setzero_pd(),
              (__mmask8) -1,
              0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_scalef_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_scalefsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_scalef_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_scalefsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 6578 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_scalef_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_scalefss_round_mask ((__v4sf) __A,
             (__v4sf)( __B), (__v4sf) _mm_setzero_ps(),
             (__mmask8) -1,
             0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_scalef_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_scalefss_round_mask ( (__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_scalef_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_scalefss_round_mask ( (__v4sf) __A,
                 (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}
# 6620 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srai_epi32(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psradi512((__v16si)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srai_epi32(__m512i __W, __mmask16 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srai_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srai_epi32(__mmask16 __U, __m512i __A,
                        unsigned int __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srai_epi32(__A, __B),
                                         (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_srai_epi64(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psraqi512((__v8di)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_srai_epi64(__m512i __W, __mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srai_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_srai_epi64(__mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srai_epi64(__A, __B),
                                          (__v8di)_mm512_setzero_si512());
}
# 6755 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_sqrt_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_sqrtsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sqrt_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_sqrtsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 6793 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_sqrt_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_sqrtss_round_mask ( (__v4sf) __A,
                 (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sqrt_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_sqrtss_round_mask ( (__v4sf) __A,
                 (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}







static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_broadcast_f32x4(__m128 __A)
{
  return (__m512)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 2, 3, 0, 1, 2, 3,
                                         0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_f32x4(__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x4(__A),
                                           (__v16sf)__O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f32x4(__mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x4(__A),
                                           (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_broadcast_f64x4(__m256d __A)
{
  return (__m512d)__builtin_shufflevector((__v4df)__A, (__v4df)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_f64x4(__m512d __O, __mmask8 __M, __m256d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x4(__A),
                                            (__v8df)__O);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f64x4(__mmask8 __M, __m256d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x4(__A),
                                            (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_broadcast_i32x4(__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_i32x4(__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x4(__A),
                                           (__v16si)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i32x4(__mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x4(__A),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_broadcast_i64x4(__m256i __A)
{
  return (__m512i)__builtin_shufflevector((__v4di)__A, (__v4di)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_i64x4(__m512i __O, __mmask8 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x4(__A),
                                            (__v8di)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i64x4(__mmask8 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x4(__A),
                                            (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcastsd_pd (__m512d __O, __mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__M,
                                              (__v8df) _mm512_broadcastsd_pd(__A),
                                              (__v8df) __O);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcastsd_pd (__mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__M,
                                              (__v8df) _mm512_broadcastsd_pd(__A),
                                              (__v8df) _mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcastss_ps (__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__M,
                                             (__v16sf) _mm512_broadcastss_ps(__A),
                                             (__v16sf) __O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcastss_ps (__mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__M,
                                             (__v16sf) _mm512_broadcastss_ps(__A),
                                             (__v16sf) _mm512_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtsepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
               (__v16qi) _mm_undefined_si128 (),
               (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovsdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtsepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
               (__v16hi) _mm256_undefined_si256 (),
               (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
               (__v16hi) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
               (__v16hi) _mm256_setzero_si256 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi32_storeu_epi16 (void *__P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovsdw512mem_mask ((__v16hi*) __P, (__v16si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtsepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
               (__v16qi) _mm_undefined_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtsepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
               (__v8si) _mm256_undefined_si256 (),
               (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
               (__v8si) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
               (__v8si) _mm256_setzero_si256 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_storeu_epi32 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqd512mem_mask ((__v8si *) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtsepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
               (__v8hi) _mm_undefined_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqw512mem_mask ((__v8hi *) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtusepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
                (__v16qi) _mm_undefined_si128 (),
                (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovusdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtusepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
                (__v16hi) _mm256_undefined_si256 (),
                (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
                (__v16hi) __O,
                __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
                (__v16hi) _mm256_setzero_si256 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi32_storeu_epi16 (void *__P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovusdw512mem_mask ((__v16hi*) __P, (__v16si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtusepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
                (__v16qi) _mm_undefined_si128 (),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtusepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
                (__v8si) _mm256_undefined_si256 (),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
                (__v8si) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
                (__v8si) _mm256_setzero_si256 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_storeu_epi32 (void* __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqd512mem_mask ((__v8si*) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtusepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
                (__v8hi) _mm_undefined_si128 (),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_storeu_epi16 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqw512mem_mask ((__v8hi*) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
              (__v16qi) _mm_undefined_si128 (),
              (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
              (__v16hi) _mm256_undefined_si256 (),
              (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
              (__v16hi) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
              (__v16hi) _mm256_setzero_si256 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_storeu_epi16 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovdw512mem_mask ((__v16hi *) __P, (__v16si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
              (__v16qi) _mm_undefined_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
              (__v8si) _mm256_undefined_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
              (__v8si) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
              (__v8si) _mm256_setzero_si256 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_storeu_epi32 (void* __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqd512mem_mask ((__v8si *) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
              (__v8hi) _mm_undefined_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_storeu_epi16 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqw512mem_mask ((__v8hi *) __P, (__v8di) __A, __M);
}
# 7568 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_getexp_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
                (__v8df) _mm512_undefined_pd (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_getexp_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
                (__v8df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_getexp_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
                (__v8df) _mm512_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 7610 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_getexp_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
               (__v16sf) _mm512_undefined_ps (),
               (__mmask16) -1,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_getexp_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
               (__v16sf) __W,
               (__mmask16) __U,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_getexp_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
               (__v16sf) _mm512_setzero_ps (),
               (__mmask16) __U,
               0x04);
}
# 7813 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmadd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_vfmaddss3_mask((__v4sf)__W,
                                       (__v4sf)__A,
                                       (__v4sf)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 7835 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmadd_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return __builtin_ia32_vfmaddss3_maskz((__v4sf)__A,
                                        (__v4sf)__B,
                                        (__v4sf)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmadd_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmaddss3_mask3((__v4sf)__W,
                                        (__v4sf)__X,
                                        (__v4sf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmsub_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_vfmaddss3_mask((__v4sf)__W,
                                       (__v4sf)__A,
                                       -(__v4sf)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 7889 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmsub_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return __builtin_ia32_vfmaddss3_maskz((__v4sf)__A,
                                        (__v4sf)__B,
                                        -(__v4sf)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmsub_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmsubss3_mask3((__v4sf)__W,
                                        (__v4sf)__X,
                                        (__v4sf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmadd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_vfmaddss3_mask((__v4sf)__W,
                                       -(__v4sf)__A,
                                       (__v4sf)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 7943 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmadd_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return __builtin_ia32_vfmaddss3_maskz((__v4sf)__A,
                                        -(__v4sf)__B,
                                        (__v4sf)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmadd_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmaddss3_mask3((__v4sf)__W,
                                        -(__v4sf)__X,
                                        (__v4sf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmsub_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_vfmaddss3_mask((__v4sf)__W,
                                       -(__v4sf)__A,
                                       -(__v4sf)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 7997 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmsub_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return __builtin_ia32_vfmaddss3_maskz((__v4sf)__A,
                                        -(__v4sf)__B,
                                        -(__v4sf)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmsub_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmsubss3_mask3((__v4sf)__W,
                                        -(__v4sf)__X,
                                        (__v4sf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmadd_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_vfmaddsd3_mask((__v2df)__W,
                                       (__v2df)__A,
                                       (__v2df)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 8051 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmadd_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return __builtin_ia32_vfmaddsd3_maskz((__v2df)__A,
                                        (__v2df)__B,
                                        (__v2df)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmadd_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmaddsd3_mask3((__v2df)__W,
                                        (__v2df)__X,
                                        (__v2df)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmsub_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_vfmaddsd3_mask((__v2df)__W,
                                       (__v2df)__A,
                                       -(__v2df)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 8105 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmsub_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return __builtin_ia32_vfmaddsd3_maskz((__v2df)__A,
                                        (__v2df)__B,
                                        -(__v2df)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmsub_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmsubsd3_mask3((__v2df)__W,
                                        (__v2df)__X,
                                        (__v2df)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmadd_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_vfmaddsd3_mask((__v2df)__W,
                                       -(__v2df)__A,
                                       (__v2df)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 8159 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmadd_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return __builtin_ia32_vfmaddsd3_maskz((__v2df)__A,
                                        -(__v2df)__B,
                                        (__v2df)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmadd_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmaddsd3_mask3((__v2df)__W,
                                        -(__v2df)__X,
                                        (__v2df)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmsub_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_vfmaddsd3_mask((__v2df)__W,
                                       -(__v2df)__A,
                                       -(__v2df)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 8213 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmsub_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return __builtin_ia32_vfmaddsd3_maskz((__v2df)__A,
                                        -(__v2df)__B,
                                        -(__v2df)__C,
                                        (__mmask8)__U,
                                        0x04);
}
# 8230 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmsub_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmsubsd3_mask3((__v2df)__W,
                                        -(__v2df)__X,
                                        (__v2df)__Y,
                                        (__mmask8)__U,
                                        0x04);
}
# 8272 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutexvar_pd (__m512i __X, __m512d __Y)
{
  return (__m512d)__builtin_ia32_permvardf512((__v8df) __Y, (__v8di) __X);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutexvar_pd (__m512d __W, __mmask8 __U, __m512i __X, __m512d __Y)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                        (__v8df)_mm512_permutexvar_pd(__X, __Y),
                                        (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_pd (__mmask8 __U, __m512i __X, __m512d __Y)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                        (__v8df)_mm512_permutexvar_pd(__X, __Y),
                                        (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutexvar_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_permvardi512((__v8di)__Y, (__v8di)__X);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_epi64 (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                     (__v8di)_mm512_permutexvar_epi64(__X, __Y),
                                     (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutexvar_epi64 (__m512i __W, __mmask8 __M, __m512i __X,
             __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                     (__v8di)_mm512_permutexvar_epi64(__X, __Y),
                                     (__v8di)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutexvar_ps (__m512i __X, __m512 __Y)
{
  return (__m512)__builtin_ia32_permvarsf512((__v16sf)__Y, (__v16si)__X);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutexvar_ps (__m512 __W, __mmask16 __U, __m512i __X, __m512 __Y)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                       (__v16sf)_mm512_permutexvar_ps(__X, __Y),
                                       (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_ps (__mmask16 __U, __m512i __X, __m512 __Y)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                       (__v16sf)_mm512_permutexvar_ps(__X, __Y),
                                       (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_permutexvar_epi32 (__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_permvarsi512((__v16si)__Y, (__v16si)__X);
}



static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_epi32 (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                    (__v16si)_mm512_permutexvar_epi32(__X, __Y),
                                    (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_permutexvar_epi32 (__m512i __W, __mmask16 __M, __m512i __X,
             __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                    (__v16si)_mm512_permutexvar_epi32(__X, __Y),
                                    (__v16si)__W);
}



static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_kand (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kandhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_kandn (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kandnhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_kor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_korhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_kortestc (__mmask16 __A, __mmask16 __B)
{
  return __builtin_ia32_kortestchi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_kortestz (__mmask16 __A, __mmask16 __B)
{
  return __builtin_ia32_kortestzhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_kortestc_mask16_u8(__mmask16 __A, __mmask16 __B)
{
  return (unsigned char)__builtin_ia32_kortestchi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_kortestz_mask16_u8(__mmask16 __A, __mmask16 __B)
{
  return (unsigned char)__builtin_ia32_kortestzhi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_kortest_mask16_u8(__mmask16 __A, __mmask16 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_kortestchi(__A, __B);
  return (unsigned char)__builtin_ia32_kortestzhi(__A, __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_kunpackb (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kunpckhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_kxnor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kxnorhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_mm512_kxor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kxorhi ((__mmask16) __A, (__mmask16) __B);
}
# 8445 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_cvtmask16_u32(__mmask16 __A) {
  return (unsigned int)__builtin_ia32_kmovw((__mmask16)__A);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_cvtu32_mask16(unsigned int __A) {
  return (__mmask16)__builtin_ia32_kmovw((__mmask16)__A);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_load_mask16(__mmask16 *__A) {
  return (__mmask16)__builtin_ia32_kmovw(*(__mmask16 *)__A);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512")))
_store_mask16(__mmask16 *__A, __mmask16 __B) {
  *(__mmask16 *)__A = __builtin_ia32_kmovw((__mmask16)__B);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_stream_si512 (void * __P, __m512i __A)
{
  typedef __v8di __v8di_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v8di_aligned)__A, (__v8di_aligned*)__P);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_stream_load_si512 (void const *__P)
{
  typedef __v8di __v8di_aligned __attribute__((aligned(64)));
  return (__m512i) __builtin_nontemporal_load((const __v8di_aligned *)__P);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_stream_pd (void *__P, __m512d __A)
{
  typedef __v8df __v8df_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v8df_aligned)__A, (__v8df_aligned*)__P);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_stream_ps (void *__P, __m512 __A)
{
  typedef __v16sf __v16sf_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v16sf_aligned)__A, (__v16sf_aligned*)__P);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_compress_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_compressdf512_mask ((__v8df) __A,
                  (__v8df) __W,
                  (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_compress_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_compressdf512_mask ((__v8df) __A,
                  (__v8df)
                  _mm512_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_compress_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compressdi512_mask ((__v8di) __A,
                  (__v8di) __W,
                  (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_compress_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compressdi512_mask ((__v8di) __A,
                  (__v8di)
                  _mm512_setzero_si512 (),
                  (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_compress_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_compresssf512_mask ((__v16sf) __A,
                 (__v16sf) __W,
                 (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_compress_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_compresssf512_mask ((__v16sf) __A,
                 (__v16sf)
                 _mm512_setzero_ps (),
                 (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_compress_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compresssi512_mask ((__v16si) __A,
                  (__v16si) __W,
                  (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_compress_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compresssi512_mask ((__v16si) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) __U);
}
# 8607 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_test_epi32_mask (__m512i __A, __m512i __B)
{
  return ((__mmask16)__builtin_ia32_cmpd512_mask((__v16si)(__m512i)((_mm512_and_epi32(__A, __B))), (__v16si)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask16)-1));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_test_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return ((__mmask16)__builtin_ia32_cmpd512_mask((__v16si)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v16si)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask16)((__U))));

}

static __inline __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_test_epi64_mask (__m512i __A, __m512i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq512_mask((__v8di)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v8di)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_test_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq512_mask((__v8di)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v8di)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_testn_epi32_mask (__m512i __A, __m512i __B)
{
  return ((__mmask16)__builtin_ia32_cmpd512_mask((__v16si)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v16si)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask16)-1));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_testn_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return ((__mmask16)__builtin_ia32_cmpd512_mask((__v16si)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v16si)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask16)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_testn_epi64_mask (__m512i __A, __m512i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq512_mask((__v8di)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v8di)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_testn_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq512_mask((__v8di)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v8di)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_movehdup_ps (__m512 __A)
{
  return (__m512)__builtin_shufflevector((__v16sf)__A, (__v16sf)__A,
                         1, 1, 3, 3, 5, 5, 7, 7, 9, 9, 11, 11, 13, 13, 15, 15);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_movehdup_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_movehdup_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_movehdup_ps (__mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_movehdup_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_moveldup_ps (__m512 __A)
{
  return (__m512)__builtin_shufflevector((__v16sf)__A, (__v16sf)__A,
                         0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_moveldup_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_moveldup_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_moveldup_ps (__mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_moveldup_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_move_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_selectss_128(__U, _mm_move_ss(__A, __B), __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_move_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_selectss_128(__U, _mm_move_ss(__A, __B),
                                     _mm_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_move_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_selectsd_128(__U, _mm_move_sd(__A, __B), __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_move_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_selectsd_128(__U, _mm_move_sd(__A, __B),
                                     _mm_setzero_pd());
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_store_ss (float * __W, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storess128_mask ((__v4sf *)__W, __A, __U & 1);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_store_sd (double * __W, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storesd128_mask ((__v2df *)__W, __A, __U & 1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_load_ss (__m128 __W, __mmask8 __U, const float* __A)
{
  __m128 src = (__v4sf) __builtin_shufflevector((__v4sf) __W,
                                                (__v4sf)_mm_setzero_ps(),
                                                0, 4, 4, 4);

  return (__m128) __builtin_ia32_loadss128_mask ((const __v4sf *) __A, src, __U & 1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_load_ss (__mmask8 __U, const float* __A)
{
  return (__m128)__builtin_ia32_loadss128_mask ((const __v4sf *) __A,
                                                (__v4sf) _mm_setzero_ps(),
                                                __U & 1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_load_sd (__m128d __W, __mmask8 __U, const double* __A)
{
  __m128d src = (__v2df) __builtin_shufflevector((__v2df) __W,
                                                 (__v2df)_mm_setzero_pd(),
                                                 0, 2);

  return (__m128d) __builtin_ia32_loadsd128_mask ((const __v2df *) __A, src, __U & 1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_load_sd (__mmask8 __U, const double* __A)
{
  return (__m128d) __builtin_ia32_loadsd128_mask ((const __v2df *) __A,
                                                  (__v2df) _mm_setzero_pd(),
                                                  __U & 1);
}
# 8796 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_expand_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_expanddf512_mask ((__v8df) __A,
                (__v8df) __W,
                (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_expand_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_expanddf512_mask ((__v8df) __A,
                (__v8df) _mm512_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_expand_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expanddi512_mask ((__v8di) __A,
                (__v8di) __W,
                (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_expand_epi64 ( __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expanddi512_mask ((__v8di) __A,
                (__v8di) _mm512_setzero_si512 (),
                (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_expandloadu_pd(__m512d __W, __mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_expandloaddf512_mask ((const __v8df *)__P,
              (__v8df) __W,
              (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_pd(__mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_expandloaddf512_mask ((const __v8df *)__P,
              (__v8df) _mm512_setzero_pd(),
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_expandloadu_epi64(__m512i __W, __mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloaddi512_mask ((const __v8di *)__P,
              (__v8di) __W,
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_epi64(__mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloaddi512_mask ((const __v8di *)__P,
              (__v8di) _mm512_setzero_si512(),
              (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_expandloadu_ps(__m512 __W, __mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_expandloadsf512_mask ((const __v16sf *)__P,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_ps(__mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_expandloadsf512_mask ((const __v16sf *)__P,
                   (__v16sf) _mm512_setzero_ps(),
                   (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_expandloadu_epi32(__m512i __W, __mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadsi512_mask ((const __v16si *)__P,
              (__v16si) __W,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_epi32(__mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadsi512_mask ((const __v16si *)__P,
              (__v16si) _mm512_setzero_si512(),
              (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_expand_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_expandsf512_mask ((__v16sf) __A,
               (__v16sf) __W,
               (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_expand_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_expandsf512_mask ((__v16sf) __A,
               (__v16sf) _mm512_setzero_ps(),
               (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_expand_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expandsi512_mask ((__v16si) __A,
                (__v16si) __W,
                (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_expand_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expandsi512_mask ((__v16si) __A,
                (__v16si) _mm512_setzero_si512(),
                (__mmask16) __U);
}
# 8939 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtps_pd (__m256 __A)
{
  return (__m512d) __builtin_convertvector((__v8sf)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtps_pd (__m512d __W, __mmask8 __U, __m256 __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtps_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtps_pd (__mmask8 __U, __m256 __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtps_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtpslo_pd (__m512 __A)
{
  return (__m512d) _mm512_cvtps_pd(_mm512_castps512_ps256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtpslo_pd (__m512d __W, __mmask8 __U, __m512 __A)
{
  return (__m512d) _mm512_mask_cvtps_pd(__W, __U, _mm512_castps512_ps256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mov_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_selectpd_512 ((__mmask8) __U,
              (__v8df) __A,
              (__v8df) __W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_mov_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_selectpd_512 ((__mmask8) __U,
              (__v8df) __A,
              (__v8df) _mm512_setzero_pd ());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_mov_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_selectps_512 ((__mmask16) __U,
             (__v16sf) __A,
             (__v16sf) __W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_maskz_mov_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_selectps_512 ((__mmask16) __U,
             (__v16sf) __A,
             (__v16sf) _mm512_setzero_ps ());
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_compressstoredf512_mask ((__v8df *) __P, (__v8df) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_compressstoredi512_mask ((__v8di *) __P, (__v8di) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_ps (void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_compressstoresf512_mask ((__v16sf *) __P, (__v16sf) __A,
            (__mmask16) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_epi32 (void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_compressstoresi512_mask ((__v16si *) __P, (__v16si) __A,
            (__mmask16) __U);
}
# 9051 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128d __B)
{
  return __builtin_ia32_cvtsd2ss_round_mask ((__v4sf)__A,
                                             (__v2df)__B,
                                             (__v4sf)__W,
                                             (__mmask8)__U, 0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtsd_ss (__mmask8 __U, __m128 __A, __m128d __B)
{
  return __builtin_ia32_cvtsd2ss_round_mask ((__v4sf)__A,
                                             (__v2df)__B,
                                             (__v4sf)_mm_setzero_ps(),
                                             (__mmask8)__U, 0x04);
}
# 9124 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtss_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128 __B)
{
  return __builtin_ia32_cvtss2sd_round_mask((__v2df)__A,
                                            (__v4sf)__B,
                                            (__v2df)__W,
                                            (__mmask8)__U, 0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtss_sd (__mmask8 __U, __m128d __A, __m128 __B)
{
  return __builtin_ia32_cvtss2sd_round_mask((__v2df)__A,
                                            (__v4sf)__B,
                                            (__v2df)_mm_setzero_pd(),
                                            (__mmask8)__U, 0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvtu32_sd (__m128d __A, unsigned __B)
{
  __A[0] = __B;
  return __A;
}






static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvtu64_sd (__m128d __A, unsigned long long __B)
{
  __A[0] = __B;
  return __A;
}






static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvtu32_ss (__m128 __A, unsigned __B)
{
  __A[0] = __B;
  return __A;
}






static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,no-evex512"), __min_vector_width__(128)))
_mm_cvtu64_ss (__m128 __A, unsigned long long __B)
{
  __A[0] = __B;
  return __A;
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_set1_epi32 (__m512i __O, __mmask16 __M, int __A)
{
  return (__m512i) __builtin_ia32_selectd_512(__M,
                                              (__v16si) _mm512_set1_epi32(__A),
                                              (__v16si) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_set1_epi64 (__m512i __O, __mmask8 __M, long long __A)
{
  return (__m512i) __builtin_ia32_selectq_512(__M,
                                              (__v8di) _mm512_set1_epi64(__A),
                                              (__v8di) __O);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set_epi8 (char __e63, char __e62, char __e61, char __e60, char __e59,
    char __e58, char __e57, char __e56, char __e55, char __e54, char __e53,
    char __e52, char __e51, char __e50, char __e49, char __e48, char __e47,
    char __e46, char __e45, char __e44, char __e43, char __e42, char __e41,
    char __e40, char __e39, char __e38, char __e37, char __e36, char __e35,
    char __e34, char __e33, char __e32, char __e31, char __e30, char __e29,
    char __e28, char __e27, char __e26, char __e25, char __e24, char __e23,
    char __e22, char __e21, char __e20, char __e19, char __e18, char __e17,
    char __e16, char __e15, char __e14, char __e13, char __e12, char __e11,
    char __e10, char __e9, char __e8, char __e7, char __e6, char __e5,
    char __e4, char __e3, char __e2, char __e1, char __e0) {

  return __extension__ (__m512i)(__v64qi)
    {__e0, __e1, __e2, __e3, __e4, __e5, __e6, __e7,
     __e8, __e9, __e10, __e11, __e12, __e13, __e14, __e15,
     __e16, __e17, __e18, __e19, __e20, __e21, __e22, __e23,
     __e24, __e25, __e26, __e27, __e28, __e29, __e30, __e31,
     __e32, __e33, __e34, __e35, __e36, __e37, __e38, __e39,
     __e40, __e41, __e42, __e43, __e44, __e45, __e46, __e47,
     __e48, __e49, __e50, __e51, __e52, __e53, __e54, __e55,
     __e56, __e57, __e58, __e59, __e60, __e61, __e62, __e63};
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set_epi16(short __e31, short __e30, short __e29, short __e28,
    short __e27, short __e26, short __e25, short __e24, short __e23,
    short __e22, short __e21, short __e20, short __e19, short __e18,
    short __e17, short __e16, short __e15, short __e14, short __e13,
    short __e12, short __e11, short __e10, short __e9, short __e8,
    short __e7, short __e6, short __e5, short __e4, short __e3,
    short __e2, short __e1, short __e0) {
  return __extension__ (__m512i)(__v32hi)
    {__e0, __e1, __e2, __e3, __e4, __e5, __e6, __e7,
     __e8, __e9, __e10, __e11, __e12, __e13, __e14, __e15,
     __e16, __e17, __e18, __e19, __e20, __e21, __e22, __e23,
     __e24, __e25, __e26, __e27, __e28, __e29, __e30, __e31 };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set_epi32 (int __A, int __B, int __C, int __D,
     int __E, int __F, int __G, int __H,
     int __I, int __J, int __K, int __L,
     int __M, int __N, int __O, int __P)
{
  return __extension__ (__m512i)(__v16si)
  { __P, __O, __N, __M, __L, __K, __J, __I,
    __H, __G, __F, __E, __D, __C, __B, __A };
}






static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set_epi64 (long long __A, long long __B, long long __C,
     long long __D, long long __E, long long __F,
     long long __G, long long __H)
{
  return __extension__ (__m512i) (__v8di)
  { __H, __G, __F, __E, __D, __C, __B, __A };
}




static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set_pd (double __A, double __B, double __C, double __D,
        double __E, double __F, double __G, double __H)
{
  return __extension__ (__m512d)
  { __H, __G, __F, __E, __D, __C, __B, __A };
}




static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_set_ps (float __A, float __B, float __C, float __D,
        float __E, float __F, float __G, float __H,
        float __I, float __J, float __K, float __L,
        float __M, float __N, float __O, float __P)
{
  return __extension__ (__m512)
  { __P, __O, __N, __M, __L, __K, __J, __I,
    __H, __G, __F, __E, __D, __C, __B, __A };
}





static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_abs_ps(__m512 __A)
{
  return (__m512)_mm512_and_epi32(_mm512_set1_epi32(0x7FFFFFFF),(__m512i)__A) ;
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_abs_ps(__m512 __W, __mmask16 __K, __m512 __A)
{
  return (__m512)_mm512_mask_and_epi32((__m512i)__W, __K, _mm512_set1_epi32(0x7FFFFFFF),(__m512i)__A) ;
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_abs_pd(__m512d __A)
{
  return (__m512d)_mm512_and_epi64(_mm512_set1_epi64(0x7FFFFFFFFFFFFFFF),(__v8di)__A) ;
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_abs_pd(__m512d __W, __mmask8 __K, __m512d __A)
{
  return (__m512d)_mm512_mask_and_epi64((__v8di)__W, __K, _mm512_set1_epi64(0x7FFFFFFFFFFFFFFF),(__v8di)__A);
}
# 9335 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512))) _mm512_reduce_add_epi64(__m512i __W) {
  return __builtin_reduce_add((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512))) _mm512_reduce_mul_epi64(__m512i __W) {
  return __builtin_reduce_mul((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512))) _mm512_reduce_and_epi64(__m512i __W) {
  return __builtin_reduce_and((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512))) _mm512_reduce_or_epi64(__m512i __W) {
  return __builtin_reduce_or((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_add_epi64(__mmask8 __M, __m512i __W) {
  __W = _mm512_maskz_mov_epi64(__M, __W);
  return __builtin_reduce_add((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_mul_epi64(__mmask8 __M, __m512i __W) {
  __W = _mm512_mask_mov_epi64(_mm512_set1_epi64(1), __M, __W);
  return __builtin_reduce_mul((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_and_epi64(__mmask8 __M, __m512i __W) {
  __W = _mm512_mask_mov_epi64(_mm512_set1_epi64(-1LL), __M, __W);
  return __builtin_reduce_and((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_or_epi64(__mmask8 __M, __m512i __W) {
  __W = _mm512_maskz_mov_epi64(__M, __W);
  return __builtin_reduce_or((__v8di)__W);
}




static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512))) _mm512_reduce_add_pd(__m512d __W) {
  return __builtin_ia32_reduce_fadd_pd512(-0.0, __W);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512))) _mm512_reduce_mul_pd(__m512d __W) {
  return __builtin_ia32_reduce_fmul_pd512(1.0, __W);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_add_pd(__mmask8 __M, __m512d __W) {
  __W = _mm512_maskz_mov_pd(__M, __W);
  return __builtin_ia32_reduce_fadd_pd512(-0.0, __W);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_mul_pd(__mmask8 __M, __m512d __W) {
  __W = _mm512_mask_mov_pd(_mm512_set1_pd(1.0), __M, __W);
  return __builtin_ia32_reduce_fmul_pd512(1.0, __W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_add_epi32(__m512i __W) {
  return __builtin_reduce_add((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_mul_epi32(__m512i __W) {
  return __builtin_reduce_mul((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_and_epi32(__m512i __W) {
  return __builtin_reduce_and((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_or_epi32(__m512i __W) {
  return __builtin_reduce_or((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_add_epi32( __mmask16 __M, __m512i __W) {
  __W = _mm512_maskz_mov_epi32(__M, __W);
  return __builtin_reduce_add((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_mul_epi32( __mmask16 __M, __m512i __W) {
  __W = _mm512_mask_mov_epi32(_mm512_set1_epi32(1), __M, __W);
  return __builtin_reduce_mul((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_and_epi32( __mmask16 __M, __m512i __W) {
  __W = _mm512_mask_mov_epi32(_mm512_set1_epi32(-1), __M, __W);
  return __builtin_reduce_and((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_or_epi32(__mmask16 __M, __m512i __W) {
  __W = _mm512_maskz_mov_epi32(__M, __W);
  return __builtin_reduce_or((__v16si)__W);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_add_ps(__m512 __W) {
  return __builtin_ia32_reduce_fadd_ps512(-0.0f, __W);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_mul_ps(__m512 __W) {
  return __builtin_ia32_reduce_fmul_ps512(1.0f, __W);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_add_ps(__mmask16 __M, __m512 __W) {
  __W = _mm512_maskz_mov_ps(__M, __W);
  return __builtin_ia32_reduce_fadd_ps512(-0.0f, __W);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_mul_ps(__mmask16 __M, __m512 __W) {
  __W = _mm512_mask_mov_ps(_mm512_set1_ps(1.0f), __M, __W);
  return __builtin_ia32_reduce_fmul_ps512(1.0f, __W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_max_epi64(__m512i __V) {
  return __builtin_reduce_max((__v8di)__V);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_max_epu64(__m512i __V) {
  return __builtin_reduce_max((__v8du)__V);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_min_epi64(__m512i __V) {
  return __builtin_reduce_min((__v8di)__V);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_min_epu64(__m512i __V) {
  return __builtin_reduce_min((__v8du)__V);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_max_epi64(__mmask8 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi64(_mm512_set1_epi64(-9223372036854775807LL - 1LL), __M, __V);
  return __builtin_reduce_max((__v8di)__V);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_max_epu64(__mmask8 __M, __m512i __V) {
  __V = _mm512_maskz_mov_epi64(__M, __V);
  return __builtin_reduce_max((__v8du)__V);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_min_epi64(__mmask8 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi64(_mm512_set1_epi64(9223372036854775807LL), __M, __V);
  return __builtin_reduce_min((__v8di)__V);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_min_epu64(__mmask8 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi64(_mm512_set1_epi64(-1LL), __M, __V);
  return __builtin_reduce_min((__v8du)__V);
}
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_max_epi32(__m512i __V) {
  return __builtin_reduce_max((__v16si)__V);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_max_epu32(__m512i __V) {
  return __builtin_reduce_max((__v16su)__V);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_min_epi32(__m512i __V) {
  return __builtin_reduce_min((__v16si)__V);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_min_epu32(__m512i __V) {
  return __builtin_reduce_min((__v16su)__V);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_max_epi32(__mmask16 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi32(_mm512_set1_epi32(-2147483647 - 1), __M, __V);
  return __builtin_reduce_max((__v16si)__V);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_max_epu32(__mmask16 __M, __m512i __V) {
  __V = _mm512_maskz_mov_epi32(__M, __V);
  return __builtin_reduce_max((__v16su)__V);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_min_epi32(__mmask16 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi32(_mm512_set1_epi32(2147483647), __M, __V);
  return __builtin_reduce_min((__v16si)__V);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_min_epu32(__mmask16 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi32(_mm512_set1_epi32(-1), __M, __V);
  return __builtin_reduce_min((__v16su)__V);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_max_pd(__m512d __V) {
  return __builtin_ia32_reduce_fmax_pd512(__V);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_min_pd(__m512d __V) {
  return __builtin_ia32_reduce_fmin_pd512(__V);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_max_pd(__mmask8 __M, __m512d __V) {
  __V = _mm512_mask_mov_pd(_mm512_set1_pd(-__builtin_inf()), __M, __V);
  return __builtin_ia32_reduce_fmax_pd512(__V);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_min_pd(__mmask8 __M, __m512d __V) {
  __V = _mm512_mask_mov_pd(_mm512_set1_pd(__builtin_inf()), __M, __V);
  return __builtin_ia32_reduce_fmin_pd512(__V);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_max_ps(__m512 __V) {
  return __builtin_ia32_reduce_fmax_ps512(__V);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_reduce_min_ps(__m512 __V) {
  return __builtin_ia32_reduce_fmin_ps512(__V);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_max_ps(__mmask16 __M, __m512 __V) {
  __V = _mm512_mask_mov_ps(_mm512_set1_ps(-__builtin_inff()), __M, __V);
  return __builtin_ia32_reduce_fmax_ps512(__V);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_mask_reduce_min_ps(__mmask16 __M, __m512 __V) {
  __V = _mm512_mask_mov_ps(_mm512_set1_ps(__builtin_inff()), __M, __V);
  return __builtin_ia32_reduce_fmin_ps512(__V);
}
# 9606 "/usr/lib/clang/18/include/avx512fintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512"), __min_vector_width__(512)))
_mm512_cvtsi512_si32(__m512i __A) {
  __v16si __b = (__v16si)__A;
  return __b[0];
}
# 105 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vlintrin.h" 1 3
# 26 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
typedef short __v2hi __attribute__((__vector_size__(4)));
typedef char __v4qi __attribute__((__vector_size__(4)));
typedef char __v2qi __attribute__((__vector_size__(2)));
# 232 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_add_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_add_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_add_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_add_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_add_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_add_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_add_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_add_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sub_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sub_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sub_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sub_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sub_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sub_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sub_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sub_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_add_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_add_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_add_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_add_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_add_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_add_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_add_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_add_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_sub_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sub_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sub_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sub_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_sub_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sub_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sub_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sub_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mul_epi32(__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epi32(__X, __Y),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mul_epi32(__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epi32(__X, __Y),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_mul_epi32(__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epi32(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mul_epi32(__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epi32(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mul_epu32(__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epu32(__X, __Y),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mul_epu32(__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epu32(__X, __Y),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_mul_epu32(__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epu32(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mul_epu32(__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epu32(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mullo_epi32(__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_mullo_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mullo_epi32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_mullo_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mullo_epi32(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_mullo_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_mullo_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_mullo_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_and_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a & (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_and_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_and_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_and_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_and_epi32(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_and_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4su)__a & (__v4su)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_and_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_and_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_and_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_and_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_andnot_epi32(__m256i __A, __m256i __B)
{
  return (__m256i)(~(__v8su)__A & (__v8su)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_andnot_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                          (__v8si)_mm256_andnot_epi32(__A, __B),
                                          (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_andnot_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_andnot_epi32(_mm256_setzero_si256(),
                                           __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_andnot_epi32(__m128i __A, __m128i __B)
{
  return (__m128i)(~(__v4su)__A & (__v4su)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_andnot_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_andnot_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_andnot_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_andnot_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_or_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a | (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_or_epi32 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_or_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_or_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_or_epi32(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_or_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4su)__a | (__v4su)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_or_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_or_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_or_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_or_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_xor_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a ^ (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_xor_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_xor_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_xor_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_xor_epi32(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_xor_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4su)__a ^ (__v4su)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_xor_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_xor_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_xor_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_xor_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_and_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a & (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_and_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_and_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_and_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_and_epi64(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_and_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a & (__v2du)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_and_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_and_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_and_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_and_epi64(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_andnot_epi64(__m256i __A, __m256i __B)
{
  return (__m256i)(~(__v4du)__A & (__v4du)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_andnot_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                          (__v4di)_mm256_andnot_epi64(__A, __B),
                                          (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_andnot_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_andnot_epi64(_mm256_setzero_si256(),
                                           __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_andnot_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)(~(__v2du)__A & (__v2du)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_andnot_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_andnot_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_andnot_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_andnot_epi64(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_or_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a | (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_or_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_or_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_or_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_or_epi64(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_or_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a | (__v2du)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_or_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_or_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_or_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_or_epi64(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_xor_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a ^ (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_xor_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_xor_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_xor_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_xor_epi64(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_xor_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a ^ (__v2du)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_xor_epi64(__m128i __W, __mmask8 __U, __m128i __A,
        __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_xor_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_xor_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_xor_epi64(_mm_setzero_si128(), __U, __A, __B);
}
# 899 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmadd_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmadd_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmadd_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmsub_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmsub_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmadd_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd (-(__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmadd_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd (-(__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmsub_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd (-(__v2df) __A,
                                             (__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmadd_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmadd_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmadd_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmsub_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmsub_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fnmadd_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 (-(__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fnmadd_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 (-(__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fnmsub_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 (-(__v4df) __A,
                                                (__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmadd_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmadd_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmadd_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmsub_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmsub_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmadd_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps (-(__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmadd_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps (-(__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmsub_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps (-(__v4sf) __A,
                                             (__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmadd_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmadd_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmadd_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmsub_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmsub_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fnmadd_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 (-(__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fnmadd_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 (-(__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fnmsub_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 (-(__v8sf) __A,
                                                (__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmaddsub_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                (__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmaddsub_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                (__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmaddsub_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                (__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmsubadd_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                -(__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmsubadd_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                -(__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmaddsub_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   (__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmaddsub_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   (__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmaddsub_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   (__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmsubadd_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   -(__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmsubadd_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   -(__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmaddsub_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                (__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmaddsub_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                (__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmaddsub_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                (__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmsubadd_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                -(__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmsubadd_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                -(__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmaddsub_ps(__m256 __A, __mmask8 __U, __m256 __B,
                         __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   (__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmaddsub_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   (__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmaddsub_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   (__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmsubadd_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   -(__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmsubadd_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   -(__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmsub_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmsub_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmsub_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmsub_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmsubadd_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                -(__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmsubadd_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   -(__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmsubadd_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                -(__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmsubadd_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   -(__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmadd_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             -(__v2df) __B,
                                             (__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fnmadd_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                -(__v4df) __B,
                                                (__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmadd_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             -(__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fnmadd_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                -(__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmsub_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             -(__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmsub_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             -(__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fnmsub_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                -(__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fnmsub_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                -(__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmsub_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             -(__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmsub_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             -(__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fnmsub_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                -(__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fnmsub_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                -(__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_add_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_add_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_add_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_add_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_add_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_add_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_add_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_add_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_add_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_add_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_add_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_add_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_add_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_add_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_add_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_add_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_blend_epi32 (__mmask8 __U, __m128i __A, __m128i __W) {
  return (__m128i) __builtin_ia32_selectd_128 ((__mmask8) __U,
                (__v4si) __W,
                (__v4si) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_blend_epi32 (__mmask8 __U, __m256i __A, __m256i __W) {
  return (__m256i) __builtin_ia32_selectd_256 ((__mmask8) __U,
                (__v8si) __W,
                (__v8si) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_blend_pd (__mmask8 __U, __m128d __A, __m128d __W) {
  return (__m128d) __builtin_ia32_selectpd_128 ((__mmask8) __U,
                 (__v2df) __W,
                 (__v2df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_blend_pd (__mmask8 __U, __m256d __A, __m256d __W) {
  return (__m256d) __builtin_ia32_selectpd_256 ((__mmask8) __U,
                 (__v4df) __W,
                 (__v4df) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_blend_ps (__mmask8 __U, __m128 __A, __m128 __W) {
  return (__m128) __builtin_ia32_selectps_128 ((__mmask8) __U,
                (__v4sf) __W,
                (__v4sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_blend_ps (__mmask8 __U, __m256 __A, __m256 __W) {
  return (__m256) __builtin_ia32_selectps_256 ((__mmask8) __U,
                (__v8sf) __W,
                (__v8sf) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_blend_epi64 (__mmask8 __U, __m128i __A, __m128i __W) {
  return (__m128i) __builtin_ia32_selectq_128 ((__mmask8) __U,
                (__v2di) __W,
                (__v2di) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_blend_epi64 (__mmask8 __U, __m256i __A, __m256i __W) {
  return (__m256i) __builtin_ia32_selectq_256 ((__mmask8) __U,
                (__v4di) __W,
                (__v4di) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_compress_pd (__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_compressdf128_mask ((__v2df) __A,
                  (__v2df) __W,
                  (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_compress_pd (__mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_compressdf128_mask ((__v2df) __A,
                  (__v2df)
                  _mm_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compress_pd (__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_compressdf256_mask ((__v4df) __A,
                  (__v4df) __W,
                  (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_compress_pd (__mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_compressdf256_mask ((__v4df) __A,
                  (__v4df)
                  _mm256_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_compress_epi64 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compressdi128_mask ((__v2di) __A,
                  (__v2di) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_compress_epi64 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compressdi128_mask ((__v2di) __A,
                  (__v2di)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compress_epi64 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compressdi256_mask ((__v4di) __A,
                  (__v4di) __W,
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_compress_epi64 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compressdi256_mask ((__v4di) __A,
                  (__v4di)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_compress_ps (__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_compresssf128_mask ((__v4sf) __A,
                 (__v4sf) __W,
                 (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_compress_ps (__mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_compresssf128_mask ((__v4sf) __A,
                 (__v4sf)
                 _mm_setzero_ps (),
                 (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compress_ps (__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_compresssf256_mask ((__v8sf) __A,
                 (__v8sf) __W,
                 (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_compress_ps (__mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_compresssf256_mask ((__v8sf) __A,
                 (__v8sf)
                 _mm256_setzero_ps (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_compress_epi32 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compresssi128_mask ((__v4si) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_compress_epi32 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compresssi128_mask ((__v4si) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compress_epi32 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compresssi256_mask ((__v8si) __A,
                  (__v8si) __W,
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_compress_epi32 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compresssi256_mask ((__v8si) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m128d __A) {
  __builtin_ia32_compressstoredf128_mask ((__v2df *) __P,
            (__v2df) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m256d __A) {
  __builtin_ia32_compressstoredf256_mask ((__v4df *) __P,
            (__v4df) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m128i __A) {
  __builtin_ia32_compressstoredi128_mask ((__v2di *) __P,
            (__v2di) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m256i __A) {
  __builtin_ia32_compressstoredi256_mask ((__v4di *) __P,
            (__v4di) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_compressstoreu_ps (void *__P, __mmask8 __U, __m128 __A) {
  __builtin_ia32_compressstoresf128_mask ((__v4sf *) __P,
            (__v4sf) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_ps (void *__P, __mmask8 __U, __m256 __A) {
  __builtin_ia32_compressstoresf256_mask ((__v8sf *) __P,
            (__v8sf) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_compressstoreu_epi32 (void *__P, __mmask8 __U, __m128i __A) {
  __builtin_ia32_compressstoresi128_mask ((__v4si *) __P,
            (__v4si) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_epi32 (void *__P, __mmask8 __U, __m256i __A) {
  __builtin_ia32_compressstoresi256_mask ((__v8si *) __P,
            (__v8si) __A,
            (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi32_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepi32_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepi32_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_pd (__m256d __W, __mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepi32_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_pd (__mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepi32_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi32_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_cvtepi32_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_ps (__mmask8 __U, __m128i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_cvtepi32_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_ps (__m256 __W, __mmask8 __U, __m256i __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_cvtepi32_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_ps (__mmask8 __U, __m256i __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_cvtepi32_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtpd_epi32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2dq128_mask ((__v2df) __A,
                (__v4si) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtpd_epi32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2dq128_mask ((__v2df) __A,
                (__v4si)
                _mm_setzero_si128 (),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtpd_epi32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm256_cvtpd_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_epi32 (__mmask8 __U, __m256d __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm256_cvtpd_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtpd_ps (__m128 __W, __mmask8 __U, __m128d __A) {
  return (__m128) __builtin_ia32_cvtpd2ps_mask ((__v2df) __A,
            (__v4sf) __W,
            (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtpd_ps (__mmask8 __U, __m128d __A) {
  return (__m128) __builtin_ia32_cvtpd2ps_mask ((__v2df) __A,
            (__v4sf)
            _mm_setzero_ps (),
            (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtpd_ps (__m128 __W, __mmask8 __U, __m256d __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtpd_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_ps (__mmask8 __U, __m256d __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtpd_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtpd_epu32 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtpd_epu32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtpd_epu32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtpd_epu32 (__m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtpd_epu32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_epu32 (__mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtps_epi32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtps_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtps_epi32 (__mmask8 __U, __m128 __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtps_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtps_epi32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtps_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtps_epi32 (__mmask8 __U, __m256 __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtps_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtps_pd (__m128d __W, __mmask8 __U, __m128 __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtps_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtps_pd (__mmask8 __U, __m128 __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtps_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtps_pd (__m256d __W, __mmask8 __U, __m128 __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtps_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtps_pd (__mmask8 __U, __m128 __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtps_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtps_epu32 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtps_epu32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtps_epu32 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtps_epu32 (__m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtps_epu32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
                 (__v8si) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtps_epu32 (__mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttpd_epi32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2dq128_mask ((__v2df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttpd_epi32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2dq128_mask ((__v2df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttpd_epi32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm256_cvttpd_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttpd_epi32 (__mmask8 __U, __m256d __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm256_cvttpd_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvttpd_epu32 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttpd_epu32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttpd_epu32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvttpd_epu32 (__m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttpd_epu32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttpd_epu32 (__mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttps_epi32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvttps_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttps_epi32 (__mmask8 __U, __m128 __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvttps_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttps_epi32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvttps_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttps_epi32 (__mmask8 __U, __m256 __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvttps_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvttps_epu32 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttps_epu32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttps_epu32 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvttps_epu32 (__m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttps_epu32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
                  (__v8si) __W,
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttps_epu32 (__mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtepu32_pd (__m128i __A) {
  return (__m128d) __builtin_convertvector(
      __builtin_shufflevector((__v4su)__A, (__v4su)__A, 0, 1), __v2df);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepu32_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepu32_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepu32_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepu32_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu32_pd (__m128i __A) {
  return (__m256d)__builtin_convertvector((__v4su)__A, __v4df);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepu32_pd (__m256d __W, __mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepu32_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepu32_pd (__mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepu32_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtepu32_ps (__m128i __A) {
  return (__m128)__builtin_convertvector((__v4su)__A, __v4sf);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepu32_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_cvtepu32_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepu32_ps (__mmask8 __U, __m128i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_cvtepu32_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu32_ps (__m256i __A) {
  return (__m256)__builtin_convertvector((__v8su)__A, __v8sf);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepu32_ps (__m256 __W, __mmask8 __U, __m256i __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_cvtepu32_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepu32_ps (__mmask8 __U, __m256i __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_cvtepu32_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_div_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_div_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_div_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_div_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_div_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_div_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_div_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_div_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_div_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_div_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_div_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_div_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_div_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_div_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_div_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_div_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_expand_pd (__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_expanddf128_mask ((__v2df) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expand_pd (__mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_expanddf128_mask ((__v2df) __A,
                 (__v2df)
                 _mm_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expand_pd (__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_expanddf256_mask ((__v4df) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expand_pd (__mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_expanddf256_mask ((__v4df) __A,
                 (__v4df)
                 _mm256_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_expand_epi64 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expanddi128_mask ((__v2di) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expand_epi64 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expanddi128_mask ((__v2di) __A,
                 (__v2di)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expand_epi64 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expanddi256_mask ((__v4di) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expand_epi64 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expanddi256_mask ((__v4di) __A,
                 (__v4di)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_expandloadu_pd (__m128d __W, __mmask8 __U, void const *__P) {
  return (__m128d) __builtin_ia32_expandloaddf128_mask ((const __v2df *) __P,
              (__v2df) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expandloadu_pd (__mmask8 __U, void const *__P) {
  return (__m128d) __builtin_ia32_expandloaddf128_mask ((const __v2df *) __P,
               (__v2df)
               _mm_setzero_pd (),
               (__mmask8)
               __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expandloadu_pd (__m256d __W, __mmask8 __U, void const *__P) {
  return (__m256d) __builtin_ia32_expandloaddf256_mask ((const __v4df *) __P,
              (__v4df) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_pd (__mmask8 __U, void const *__P) {
  return (__m256d) __builtin_ia32_expandloaddf256_mask ((const __v4df *) __P,
               (__v4df)
               _mm256_setzero_pd (),
               (__mmask8)
               __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_expandloadu_epi64 (__m128i __W, __mmask8 __U, void const *__P) {
  return (__m128i) __builtin_ia32_expandloaddi128_mask ((const __v2di *) __P,
              (__v2di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expandloadu_epi64 (__mmask8 __U, void const *__P) {
  return (__m128i) __builtin_ia32_expandloaddi128_mask ((const __v2di *) __P,
               (__v2di)
               _mm_setzero_si128 (),
               (__mmask8)
               __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expandloadu_epi64 (__m256i __W, __mmask8 __U,
             void const *__P) {
  return (__m256i) __builtin_ia32_expandloaddi256_mask ((const __v4di *) __P,
              (__v4di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_epi64 (__mmask8 __U, void const *__P) {
  return (__m256i) __builtin_ia32_expandloaddi256_mask ((const __v4di *) __P,
               (__v4di)
               _mm256_setzero_si256 (),
               (__mmask8)
               __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_expandloadu_ps (__m128 __W, __mmask8 __U, void const *__P) {
  return (__m128) __builtin_ia32_expandloadsf128_mask ((const __v4sf *) __P,
                   (__v4sf) __W,
                   (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expandloadu_ps (__mmask8 __U, void const *__P) {
  return (__m128) __builtin_ia32_expandloadsf128_mask ((const __v4sf *) __P,
              (__v4sf)
              _mm_setzero_ps (),
              (__mmask8)
              __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expandloadu_ps (__m256 __W, __mmask8 __U, void const *__P) {
  return (__m256) __builtin_ia32_expandloadsf256_mask ((const __v8sf *) __P,
                   (__v8sf) __W,
                   (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_ps (__mmask8 __U, void const *__P) {
  return (__m256) __builtin_ia32_expandloadsf256_mask ((const __v8sf *) __P,
              (__v8sf)
              _mm256_setzero_ps (),
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_expandloadu_epi32 (__m128i __W, __mmask8 __U, void const *__P) {
  return (__m128i) __builtin_ia32_expandloadsi128_mask ((const __v4si *) __P,
              (__v4si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expandloadu_epi32 (__mmask8 __U, void const *__P) {
  return (__m128i) __builtin_ia32_expandloadsi128_mask ((const __v4si *) __P,
               (__v4si)
               _mm_setzero_si128 (),
               (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expandloadu_epi32 (__m256i __W, __mmask8 __U,
             void const *__P) {
  return (__m256i) __builtin_ia32_expandloadsi256_mask ((const __v8si *) __P,
              (__v8si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_epi32 (__mmask8 __U, void const *__P) {
  return (__m256i) __builtin_ia32_expandloadsi256_mask ((const __v8si *) __P,
               (__v8si)
               _mm256_setzero_si256 (),
               (__mmask8)
               __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_expand_ps (__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_expandsf128_mask ((__v4sf) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expand_ps (__mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_expandsf128_mask ((__v4sf) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expand_ps (__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_expandsf256_mask ((__v8sf) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expand_ps (__mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_expandsf256_mask ((__v8sf) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_expand_epi32 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expandsi128_mask ((__v4si) __A,
                (__v4si) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expand_epi32 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expandsi128_mask ((__v4si) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expand_epi32 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expandsi256_mask ((__v8si) __A,
                (__v8si) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expand_epi32 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expandsi256_mask ((__v8si) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_getexp_pd (__m128d __A) {
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_getexp_pd (__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_getexp_pd (__mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_getexp_pd (__m256d __A) {
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_getexp_pd (__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_getexp_pd (__mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_getexp_ps (__m128 __A) {
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_getexp_ps (__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_getexp_ps (__mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_getexp_ps (__m256 __A) {
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_getexp_ps (__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_getexp_ps (__mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_max_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_max_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_max_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_max_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_max_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_max_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_max_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_max_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_min_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_min_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_min_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_min_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_min_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_min_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_min_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_min_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_mul_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_mul_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mul_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_mul_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mul_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_mul_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mul_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_mul_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_mul_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_mul_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mul_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_mul_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mul_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_mul_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mul_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_mul_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_abs_epi32(__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_abs_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_abs_epi32(__mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_abs_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_abs_epi32(__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_abs_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_abs_epi32(__mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_abs_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_abs_epi64 (__m128i __A) {
  return (__m128i)__builtin_elementwise_abs((__v2di)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_abs_epi64 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_abs_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_abs_epi64 (__mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_abs_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_abs_epi64 (__m256i __A) {
  return (__m256i)__builtin_elementwise_abs((__v4di)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_abs_epi64 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_abs_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_abs_epi64 (__mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_abs_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_epi32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_epi32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_epi32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_max_epi64 (__m128i __A, __m128i __B) {
  return (__m128i)__builtin_elementwise_max((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_epi64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_max_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_epi64 (__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_max_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_max_epi64 (__m256i __A, __m256i __B) {
  return (__m256i)__builtin_elementwise_max((__v4di)__A, (__v4di)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_epi64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_max_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_epi64 (__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_max_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_epu32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epu32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_epu32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epu32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_epu32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epu32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_epu32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epu32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_max_epu64 (__m128i __A, __m128i __B) {
  return (__m128i)__builtin_elementwise_max((__v2du)__A, (__v2du)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_epu64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_max_epu64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_epu64 (__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_max_epu64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_max_epu64 (__m256i __A, __m256i __B) {
  return (__m256i)__builtin_elementwise_max((__v4du)__A, (__v4du)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_epu64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_max_epu64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_epu64 (__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_max_epu64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_epi32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_epi32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_epi32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_min_epi64 (__m128i __A, __m128i __B) {
  return (__m128i)__builtin_elementwise_min((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_epi64 (__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_min_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_epi64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_min_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_min_epi64 (__m256i __A, __m256i __B) {
  return (__m256i)__builtin_elementwise_min((__v4di)__A, (__v4di)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_epi64 (__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_min_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_epi64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_min_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_epu32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epu32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_epu32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epu32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_epu32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epu32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_epu32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epu32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_min_epu64 (__m128i __A, __m128i __B) {
  return (__m128i)__builtin_elementwise_min((__v2du)__A, (__v2du)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_epu64 (__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_min_epu64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_epu64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_min_epu64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_min_epu64 (__m256i __A, __m256i __B) {
  return (__m256i)__builtin_elementwise_min((__v4du)__A, (__v4du)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_epu64 (__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_min_epu64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_epu64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_min_epu64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}
# 3371 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_scalef_pd (__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_scalef_pd (__m128d __W, __mmask8 __U, __m128d __A,
        __m128d __B) {
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_scalef_pd (__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_scalef_pd (__m256d __A, __m256d __B) {
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
                (__v4df) __B,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_scalef_pd (__m256d __W, __mmask8 __U, __m256d __A,
           __m256d __B) {
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
                (__v4df) __B,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_scalef_pd (__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
                (__v4df) __B,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_scalef_ps (__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
               (__v4sf) __B,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_scalef_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
               (__v4sf) __B,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_scalef_ps (__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
               (__v4sf) __B,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_scalef_ps (__m256 __A, __m256 __B) {
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
               (__v8sf) __B,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_scalef_ps (__m256 __W, __mmask8 __U, __m256 __A,
           __m256 __B) {
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
               (__v8sf) __B,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_scalef_ps (__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
               (__v8sf) __B,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}
# 3638 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_sqrt_pd(__m128d __W, __mmask8 __U, __m128d __A) {
    return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                                (__v2df)_mm_sqrt_pd(__A),
                                                (__v2df)__W);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_sqrt_pd(__mmask8 __U, __m128d __A) {
    return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                                (__v2df)_mm_sqrt_pd(__A),
                                                (__v2df)_mm_setzero_pd());
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_sqrt_pd(__m256d __W, __mmask8 __U, __m256d __A) {
    return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                                (__v4df)_mm256_sqrt_pd(__A),
                                                (__v4df)__W);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_sqrt_pd(__mmask8 __U, __m256d __A) {
    return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                                (__v4df)_mm256_sqrt_pd(__A),
                                                (__v4df)_mm256_setzero_pd());
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_sqrt_ps(__m128 __W, __mmask8 __U, __m128 __A) {
    return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                               (__v4sf)_mm_sqrt_ps(__A),
                                               (__v4sf)__W);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_sqrt_ps(__mmask8 __U, __m128 __A) {
    return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                               (__v4sf)_mm_sqrt_ps(__A),
                                               (__v4sf)_mm_setzero_ps());
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_sqrt_ps(__m256 __W, __mmask8 __U, __m256 __A) {
    return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                               (__v8sf)_mm256_sqrt_ps(__A),
                                               (__v8sf)__W);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_sqrt_ps(__mmask8 __U, __m256 __A) {
    return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                               (__v8sf)_mm256_sqrt_ps(__A),
                                               (__v8sf)_mm256_setzero_ps());
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_sub_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                                (__v2df)_mm_sub_pd(__A, __B),
                                                (__v2df)__W);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_sub_pd(__mmask8 __U, __m128d __A, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                                (__v2df)_mm_sub_pd(__A, __B),
                                                (__v2df)_mm_setzero_pd());
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_sub_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                                (__v4df)_mm256_sub_pd(__A, __B),
                                                (__v4df)__W);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_sub_pd(__mmask8 __U, __m256d __A, __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                                (__v4df)_mm256_sub_pd(__A, __B),
                                                (__v4df)_mm256_setzero_pd());
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_sub_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                               (__v4sf)_mm_sub_ps(__A, __B),
                                               (__v4sf)__W);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_sub_ps(__mmask8 __U, __m128 __A, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                               (__v4sf)_mm_sub_ps(__A, __B),
                                               (__v4sf)_mm_setzero_ps());
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_sub_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                               (__v8sf)_mm256_sub_ps(__A, __B),
                                               (__v8sf)__W);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_sub_ps(__mmask8 __U, __m256 __A, __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                               (__v8sf)_mm256_sub_ps(__A, __B),
                                               (__v8sf)_mm256_setzero_ps());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_permutex2var_epi32(__m128i __A, __m128i __I, __m128i __B) {
    return (__m128i)__builtin_ia32_vpermi2vard128((__v4si) __A, (__v4si)__I,
                                                  (__v4si)__B);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_permutex2var_epi32(__m128i __A, __mmask8 __U, __m128i __I,
                              __m128i __B) {
    return (__m128i)__builtin_ia32_selectd_128(__U,
                                    (__v4si)_mm_permutex2var_epi32(__A, __I, __B),
                                    (__v4si)__A);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask2_permutex2var_epi32(__m128i __A, __m128i __I, __mmask8 __U,
                               __m128i __B) {
    return (__m128i)__builtin_ia32_selectd_128(__U,
                                    (__v4si)_mm_permutex2var_epi32(__A, __I, __B),
                                    (__v4si)__I);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_permutex2var_epi32(__mmask8 __U, __m128i __A, __m128i __I,
                               __m128i __B) {
    return (__m128i)__builtin_ia32_selectd_128(__U,
                                    (__v4si)_mm_permutex2var_epi32(__A, __I, __B),
                                    (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_permutex2var_epi32(__m256i __A, __m256i __I, __m256i __B) {
    return (__m256i)__builtin_ia32_vpermi2vard256((__v8si)__A, (__v8si) __I,
                                                  (__v8si) __B);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_permutex2var_epi32(__m256i __A, __mmask8 __U, __m256i __I,
                                 __m256i __B) {
    return (__m256i)__builtin_ia32_selectd_256(__U,
                                 (__v8si)_mm256_permutex2var_epi32(__A, __I, __B),
                                 (__v8si)__A);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask2_permutex2var_epi32(__m256i __A, __m256i __I, __mmask8 __U,
                                  __m256i __B) {
    return (__m256i)__builtin_ia32_selectd_256(__U,
                                 (__v8si)_mm256_permutex2var_epi32(__A, __I, __B),
                                 (__v8si)__I);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_permutex2var_epi32(__mmask8 __U, __m256i __A, __m256i __I,
                                  __m256i __B) {
    return (__m256i)__builtin_ia32_selectd_256(__U,
                                 (__v8si)_mm256_permutex2var_epi32(__A, __I, __B),
                                 (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_permutex2var_pd(__m128d __A, __m128i __I, __m128d __B) {
    return (__m128d)__builtin_ia32_vpermi2varpd128((__v2df)__A, (__v2di)__I,
                                                   (__v2df)__B);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_permutex2var_pd(__m128d __A, __mmask8 __U, __m128i __I, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128(__U,
                                       (__v2df)_mm_permutex2var_pd(__A, __I, __B),
                                       (__v2df)__A);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask2_permutex2var_pd(__m128d __A, __m128i __I, __mmask8 __U, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128(__U,
                                       (__v2df)_mm_permutex2var_pd(__A, __I, __B),
                                       (__v2df)(__m128d)__I);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_permutex2var_pd(__mmask8 __U, __m128d __A, __m128i __I, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128(__U,
                                       (__v2df)_mm_permutex2var_pd(__A, __I, __B),
                                       (__v2df)_mm_setzero_pd());
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_permutex2var_pd(__m256d __A, __m256i __I, __m256d __B) {
    return (__m256d)__builtin_ia32_vpermi2varpd256((__v4df)__A, (__v4di)__I,
                                                   (__v4df)__B);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_permutex2var_pd(__m256d __A, __mmask8 __U, __m256i __I,
                              __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256(__U,
                                    (__v4df)_mm256_permutex2var_pd(__A, __I, __B),
                                    (__v4df)__A);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask2_permutex2var_pd(__m256d __A, __m256i __I, __mmask8 __U,
                               __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256(__U,
                                    (__v4df)_mm256_permutex2var_pd(__A, __I, __B),
                                    (__v4df)(__m256d)__I);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_permutex2var_pd(__mmask8 __U, __m256d __A, __m256i __I,
                               __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256(__U,
                                    (__v4df)_mm256_permutex2var_pd(__A, __I, __B),
                                    (__v4df)_mm256_setzero_pd());
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_permutex2var_ps(__m128 __A, __m128i __I, __m128 __B) {
    return (__m128)__builtin_ia32_vpermi2varps128((__v4sf)__A, (__v4si)__I,
                                                  (__v4sf)__B);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_permutex2var_ps(__m128 __A, __mmask8 __U, __m128i __I, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128(__U,
                                       (__v4sf)_mm_permutex2var_ps(__A, __I, __B),
                                       (__v4sf)__A);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask2_permutex2var_ps(__m128 __A, __m128i __I, __mmask8 __U, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128(__U,
                                       (__v4sf)_mm_permutex2var_ps(__A, __I, __B),
                                       (__v4sf)(__m128)__I);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_permutex2var_ps(__mmask8 __U, __m128 __A, __m128i __I, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128(__U,
                                       (__v4sf)_mm_permutex2var_ps(__A, __I, __B),
                                       (__v4sf)_mm_setzero_ps());
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_permutex2var_ps(__m256 __A, __m256i __I, __m256 __B) {
    return (__m256)__builtin_ia32_vpermi2varps256((__v8sf)__A, (__v8si)__I,
                                                  (__v8sf) __B);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_permutex2var_ps(__m256 __A, __mmask8 __U, __m256i __I, __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256(__U,
                                    (__v8sf)_mm256_permutex2var_ps(__A, __I, __B),
                                    (__v8sf)__A);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask2_permutex2var_ps(__m256 __A, __m256i __I, __mmask8 __U,
                               __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256(__U,
                                    (__v8sf)_mm256_permutex2var_ps(__A, __I, __B),
                                    (__v8sf)(__m256)__I);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_permutex2var_ps(__mmask8 __U, __m256 __A, __m256i __I,
                               __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256(__U,
                                    (__v8sf)_mm256_permutex2var_ps(__A, __I, __B),
                                    (__v8sf)_mm256_setzero_ps());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_permutex2var_epi64(__m128i __A, __m128i __I, __m128i __B) {
    return (__m128i)__builtin_ia32_vpermi2varq128((__v2di)__A, (__v2di)__I,
                                                  (__v2di)__B);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_permutex2var_epi64(__m128i __A, __mmask8 __U, __m128i __I,
                              __m128i __B) {
    return (__m128i)__builtin_ia32_selectq_128(__U,
                                    (__v2di)_mm_permutex2var_epi64(__A, __I, __B),
                                    (__v2di)__A);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask2_permutex2var_epi64(__m128i __A, __m128i __I, __mmask8 __U,
                               __m128i __B) {
    return (__m128i)__builtin_ia32_selectq_128(__U,
                                    (__v2di)_mm_permutex2var_epi64(__A, __I, __B),
                                    (__v2di)__I);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_permutex2var_epi64(__mmask8 __U, __m128i __A, __m128i __I,
                               __m128i __B) {
    return (__m128i)__builtin_ia32_selectq_128(__U,
                                    (__v2di)_mm_permutex2var_epi64(__A, __I, __B),
                                    (__v2di)_mm_setzero_si128());
  }


  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_permutex2var_epi64(__m256i __A, __m256i __I, __m256i __B) {
    return (__m256i)__builtin_ia32_vpermi2varq256((__v4di)__A, (__v4di) __I,
                                                  (__v4di) __B);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_permutex2var_epi64(__m256i __A, __mmask8 __U, __m256i __I,
                                 __m256i __B) {
    return (__m256i)__builtin_ia32_selectq_256(__U,
                                 (__v4di)_mm256_permutex2var_epi64(__A, __I, __B),
                                 (__v4di)__A);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask2_permutex2var_epi64(__m256i __A, __m256i __I, __mmask8 __U,
                                  __m256i __B) {
    return (__m256i)__builtin_ia32_selectq_256(__U,
                                 (__v4di)_mm256_permutex2var_epi64(__A, __I, __B),
                                 (__v4di)__I);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_permutex2var_epi64(__mmask8 __U, __m256i __A, __m256i __I,
                                  __m256i __B) {
    return (__m256i)__builtin_ia32_selectq_256(__U,
                                 (__v4di)_mm256_permutex2var_epi64(__A, __I, __B),
                                 (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepi8_epi32(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepi8_epi32(__A),
                                               (__v4si)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepi8_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepi8_epi32(__A),
                                               (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepi8_epi32 (__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepi8_epi32(__A),
                                               (__v8si)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi8_epi32 (__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepi8_epi32(__A),
                                               (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepi8_epi64(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi8_epi64(__A),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepi8_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi8_epi64(__A),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepi8_epi64(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi8_epi64(__A),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi8_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi8_epi64(__A),
                                               (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepi32_epi64(__m128i __W, __mmask8 __U, __m128i __X)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi32_epi64(__X),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepi32_epi64(__mmask8 __U, __m128i __X)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi32_epi64(__X),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepi32_epi64(__m256i __W, __mmask8 __U, __m128i __X)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi32_epi64(__X),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi32_epi64(__mmask8 __U, __m128i __X)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi32_epi64(__X),
                                               (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepi16_epi32(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepi16_epi32(__A),
                                               (__v4si)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepi16_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepi16_epi32(__A),
                                               (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepi16_epi32(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepi16_epi32(__A),
                                               (__v8si)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi16_epi32 (__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepi16_epi32(__A),
                                               (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepi16_epi64(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi16_epi64(__A),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepi16_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi16_epi64(__A),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepi16_epi64(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi16_epi64(__A),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi16_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi16_epi64(__A),
                                               (__v4di)_mm256_setzero_si256());
  }


  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepu8_epi32(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepu8_epi32(__A),
                                               (__v4si)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepu8_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepu8_epi32(__A),
                                               (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepu8_epi32(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepu8_epi32(__A),
                                               (__v8si)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu8_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepu8_epi32(__A),
                                               (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepu8_epi64(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu8_epi64(__A),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepu8_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu8_epi64(__A),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepu8_epi64(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu8_epi64(__A),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu8_epi64 (__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu8_epi64(__A),
                                               (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepu32_epi64(__m128i __W, __mmask8 __U, __m128i __X)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu32_epi64(__X),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepu32_epi64(__mmask8 __U, __m128i __X)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu32_epi64(__X),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepu32_epi64(__m256i __W, __mmask8 __U, __m128i __X)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu32_epi64(__X),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu32_epi64(__mmask8 __U, __m128i __X)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu32_epi64(__X),
                                               (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepu16_epi32(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepu16_epi32(__A),
                                               (__v4si)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepu16_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepu16_epi32(__A),
                                               (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepu16_epi32(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepu16_epi32(__A),
                                               (__v8si)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu16_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepu16_epi32(__A),
                                               (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_mask_cvtepu16_epi64(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu16_epi64(__A),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
  _mm_maskz_cvtepu16_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu16_epi64(__A),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_mask_cvtepu16_epi64(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu16_epi64(__A),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu16_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu16_epi64(__A),
                                               (__v4di)_mm256_setzero_si256());
  }
# 4358 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_rolv_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_prolvd128((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_rolv_epi32 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                             (__v4si)_mm_rolv_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rolv_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                             (__v4si)_mm_rolv_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_rolv_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_prolvd256((__v8si)__A, (__v8si)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rolv_epi32 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                            (__v8si)_mm256_rolv_epi32(__A, __B),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rolv_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                            (__v8si)_mm256_rolv_epi32(__A, __B),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_rolv_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_prolvq128((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_rolv_epi64 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                             (__v2di)_mm_rolv_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rolv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                             (__v2di)_mm_rolv_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_rolv_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_prolvq256((__v4di)__A, (__v4di)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rolv_epi64 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                            (__v4di)_mm256_rolv_epi64(__A, __B),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rolv_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                            (__v4di)_mm256_rolv_epi64(__A, __B),
                                            (__v4di)_mm256_setzero_si256());
}
# 4498 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_sll_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sll_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sll_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sll_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sll_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sll_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sll_epi32(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sll_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_slli_epi32(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_slli_epi32(__A, (int)__B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_slli_epi32(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_slli_epi32(__A, (int)__B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_slli_epi32(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_slli_epi32(__A, (int)__B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_slli_epi32(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_slli_epi32(__A, (int)__B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_sll_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sll_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sll_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sll_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sll_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sll_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sll_epi64(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sll_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_slli_epi64(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_slli_epi64(__A, (int)__B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_slli_epi64(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_slli_epi64(__A, (int)__B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_slli_epi64(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_slli_epi64(__A, (int)__B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_slli_epi64(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_slli_epi64(__A, (int)__B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_rorv_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_prorvd128((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_rorv_epi32 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                             (__v4si)_mm_rorv_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rorv_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                             (__v4si)_mm_rorv_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_rorv_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_prorvd256((__v8si)__A, (__v8si)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rorv_epi32 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                            (__v8si)_mm256_rorv_epi32(__A, __B),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rorv_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                            (__v8si)_mm256_rorv_epi32(__A, __B),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_rorv_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_prorvq128((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_rorv_epi64 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                             (__v2di)_mm_rorv_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rorv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                             (__v2di)_mm_rorv_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_rorv_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_prorvq256((__v4di)__A, (__v4di)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rorv_epi64 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                            (__v4di)_mm256_rorv_epi64(__A, __B),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rorv_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                            (__v4di)_mm256_rorv_epi64(__A, __B),
                                            (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_sllv_epi64(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sllv_epi64(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sllv_epi64(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sllv_epi64(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sllv_epi64(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_sllv_epi64(__X, __Y),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sllv_epi64(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_sllv_epi64(__X, __Y),
                                            (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_sllv_epi32(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sllv_epi32(__X, __Y),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sllv_epi32(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sllv_epi32(__X, __Y),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sllv_epi32(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_sllv_epi32(__X, __Y),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sllv_epi32(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_sllv_epi32(__X, __Y),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srlv_epi64(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srlv_epi64(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srlv_epi64(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srlv_epi64(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srlv_epi64(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_srlv_epi64(__X, __Y),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srlv_epi64(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_srlv_epi64(__X, __Y),
                                            (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srlv_epi32(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srlv_epi32(__X, __Y),
                                            (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srlv_epi32(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srlv_epi32(__X, __Y),
                                            (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srlv_epi32(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srlv_epi32(__X, __Y),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srlv_epi32(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srlv_epi32(__X, __Y),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srl_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srl_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srl_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srl_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srl_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srl_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srl_epi32(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srl_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srli_epi32(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srli_epi32(__A, (int)__B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srli_epi32(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srli_epi32(__A, (int)__B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srli_epi32(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srli_epi32(__A, (int)__B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srli_epi32(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srli_epi32(__A, (int)__B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srl_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srl_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srl_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srl_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srl_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srl_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srl_epi64(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srl_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srli_epi64(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srli_epi64(__A, (int)__B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srli_epi64(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srli_epi64(__A, (int)__B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srli_epi64(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srli_epi64(__A, (int)__B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srli_epi64(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srli_epi64(__A, (int)__B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srav_epi32(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srav_epi32(__X, __Y),
                                            (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srav_epi32(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srav_epi32(__X, __Y),
                                            (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srav_epi32(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srav_epi32(__X, __Y),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srav_epi32(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srav_epi32(__X, __Y),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_srav_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psravq128((__v2di)__X, (__v2di)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srav_epi64(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srav_epi64(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srav_epi64(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srav_epi64(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_srav_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psravq256((__v4di)__X, (__v4di) __Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srav_epi64(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srav_epi64(__X, __Y),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srav_epi64 (__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srav_epi64(__X, __Y),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_mov_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectd_128 ((__mmask8) __U,
                 (__v4si) __A,
                 (__v4si) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mov_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectd_128 ((__mmask8) __U,
                 (__v4si) __A,
                 (__v4si) _mm_setzero_si128 ());
}


static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mov_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectd_256 ((__mmask8) __U,
                 (__v8si) __A,
                 (__v8si) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mov_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectd_256 ((__mmask8) __U,
                 (__v8si) __A,
                 (__v8si) _mm256_setzero_si256 ());
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_load_epi32 (void const *__P)
{
  return *(const __m128i *) __P;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_load_epi32 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa32load128_mask ((const __v4si *) __P,
              (__v4si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_load_epi32 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa32load128_mask ((const __v4si *) __P,
              (__v4si)
              _mm_setzero_si128 (),
              (__mmask8)
              __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_load_epi32 (void const *__P)
{
  return *(const __m256i *) __P;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_load_epi32 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa32load256_mask ((const __v8si *) __P,
              (__v8si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_load_epi32 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa32load256_mask ((const __v8si *) __P,
              (__v8si)
              _mm256_setzero_si256 (),
              (__mmask8)
              __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_store_epi32 (void *__P, __m128i __A)
{
  *(__m128i *) __P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_store_epi32 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_movdqa32store128_mask ((__v4si *) __P,
          (__v4si) __A,
          (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_store_epi32 (void *__P, __m256i __A)
{
  *(__m256i *) __P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_store_epi32 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_movdqa32store256_mask ((__v8si *) __P,
          (__v8si) __A,
          (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_mov_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectq_128 ((__mmask8) __U,
                 (__v2di) __A,
                 (__v2di) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mov_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectq_128 ((__mmask8) __U,
                 (__v2di) __A,
                 (__v2di) _mm_setzero_si128 ());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mov_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectq_256 ((__mmask8) __U,
                 (__v4di) __A,
                 (__v4di) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mov_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectq_256 ((__mmask8) __U,
                 (__v4di) __A,
                 (__v4di) _mm256_setzero_si256 ());
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_load_epi64 (void const *__P)
{
  return *(const __m128i *) __P;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_load_epi64 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa64load128_mask ((const __v2di *) __P,
              (__v2di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_load_epi64 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa64load128_mask ((const __v2di *) __P,
              (__v2di)
              _mm_setzero_si128 (),
              (__mmask8)
              __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_load_epi64 (void const *__P)
{
  return *(const __m256i *) __P;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_load_epi64 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa64load256_mask ((const __v4di *) __P,
              (__v4di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_load_epi64 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa64load256_mask ((const __v4di *) __P,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8)
              __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_store_epi64 (void *__P, __m128i __A)
{
  *(__m128i *) __P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_store_epi64 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_movdqa64store128_mask ((__v2di *) __P,
          (__v2di) __A,
          (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_store_epi64 (void *__P, __m256i __A)
{
  *(__m256i *) __P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_store_epi64 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_movdqa64store256_mask ((__v4di *) __P,
          (__v4di) __A,
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_movedup_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_movedup_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_movedup_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_movedup_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_movedup_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_movedup_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_movedup_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_movedup_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_set1_epi32(__m128i __O, __mmask8 __M, int __A)
{
   return (__m128i)__builtin_ia32_selectd_128(__M,
                                              (__v4si) _mm_set1_epi32(__A),
                                              (__v4si)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_set1_epi32( __mmask8 __M, int __A)
{
   return (__m128i)__builtin_ia32_selectd_128(__M,
                                              (__v4si) _mm_set1_epi32(__A),
                                              (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_set1_epi32(__m256i __O, __mmask8 __M, int __A)
{
   return (__m256i)__builtin_ia32_selectd_256(__M,
                                              (__v8si) _mm256_set1_epi32(__A),
                                              (__v8si)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_set1_epi32( __mmask8 __M, int __A)
{
   return (__m256i)__builtin_ia32_selectd_256(__M,
                                              (__v8si) _mm256_set1_epi32(__A),
                                              (__v8si)_mm256_setzero_si256());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_set1_epi64 (__m128i __O, __mmask8 __M, long long __A)
{
  return (__m128i) __builtin_ia32_selectq_128(__M,
                                              (__v2di) _mm_set1_epi64x(__A),
                                              (__v2di) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_set1_epi64 (__mmask8 __M, long long __A)
{
  return (__m128i) __builtin_ia32_selectq_128(__M,
                                              (__v2di) _mm_set1_epi64x(__A),
                                              (__v2di) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_set1_epi64 (__m256i __O, __mmask8 __M, long long __A)
{
  return (__m256i) __builtin_ia32_selectq_256(__M,
                                              (__v4di) _mm256_set1_epi64x(__A),
                                              (__v4di) __O) ;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_set1_epi64 (__mmask8 __M, long long __A)
{
   return (__m256i) __builtin_ia32_selectq_256(__M,
                                               (__v4di) _mm256_set1_epi64x(__A),
                                               (__v4di) _mm256_setzero_si256());
}
# 5436 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_load_pd (__m128d __W, __mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadapd128_mask ((const __v2df *) __P,
               (__v2df) __W,
               (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_load_pd (__mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadapd128_mask ((const __v2df *) __P,
               (__v2df)
               _mm_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_load_pd (__m256d __W, __mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadapd256_mask ((const __v4df *) __P,
               (__v4df) __W,
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_load_pd (__mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadapd256_mask ((const __v4df *) __P,
               (__v4df)
               _mm256_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_load_ps (__m128 __W, __mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadaps128_mask ((const __v4sf *) __P,
              (__v4sf) __W,
              (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_load_ps (__mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadaps128_mask ((const __v4sf *) __P,
              (__v4sf)
              _mm_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_load_ps (__m256 __W, __mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadaps256_mask ((const __v8sf *) __P,
              (__v8sf) __W,
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_load_ps (__mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadaps256_mask ((const __v8sf *) __P,
              (__v8sf)
              _mm256_setzero_ps (),
              (__mmask8) __U);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_loadu_epi64 (void const *__P)
{
  struct __loadu_epi64 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi64*)__P)->__v;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_loadu_epi64 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqudi128_mask ((const __v2di *) __P,
                 (__v2di) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_loadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqudi128_mask ((const __v2di *) __P,
                 (__v2di)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_loadu_epi64 (void const *__P)
{
  struct __loadu_epi64 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi64*)__P)->__v;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_loadu_epi64 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqudi256_mask ((const __v4di *) __P,
                 (__v4di) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_loadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqudi256_mask ((const __v4di *) __P,
                 (__v4di)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_loadu_epi32 (void const *__P)
{
  struct __loadu_epi32 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi32*)__P)->__v;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_loadu_epi32 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqusi128_mask ((const __v4si *) __P,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_loadu_epi32 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqusi128_mask ((const __v4si *) __P,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_loadu_epi32 (void const *__P)
{
  struct __loadu_epi32 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi32*)__P)->__v;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_loadu_epi32 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqusi256_mask ((const __v8si *) __P,
                 (__v8si) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_loadu_epi32 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqusi256_mask ((const __v8si *) __P,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_loadu_pd (__m128d __W, __mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadupd128_mask ((const __v2df *) __P,
               (__v2df) __W,
               (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_loadu_pd (__mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadupd128_mask ((const __v2df *) __P,
               (__v2df)
               _mm_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_loadu_pd (__m256d __W, __mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadupd256_mask ((const __v4df *) __P,
               (__v4df) __W,
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_loadu_pd (__mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadupd256_mask ((const __v4df *) __P,
               (__v4df)
               _mm256_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_loadu_ps (__m128 __W, __mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadups128_mask ((const __v4sf *) __P,
              (__v4sf) __W,
              (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_loadu_ps (__mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadups128_mask ((const __v4sf *) __P,
              (__v4sf)
              _mm_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_loadu_ps (__m256 __W, __mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadups256_mask ((const __v8sf *) __P,
              (__v8sf) __W,
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_loadu_ps (__mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadups256_mask ((const __v8sf *) __P,
              (__v8sf)
              _mm256_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_store_pd (void *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storeapd128_mask ((__v2df *) __P,
           (__v2df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_store_pd (void *__P, __mmask8 __U, __m256d __A)
{
  __builtin_ia32_storeapd256_mask ((__v4df *) __P,
           (__v4df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_store_ps (void *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storeaps128_mask ((__v4sf *) __P,
           (__v4sf) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_store_ps (void *__P, __mmask8 __U, __m256 __A)
{
  __builtin_ia32_storeaps256_mask ((__v8sf *) __P,
           (__v8sf) __A,
           (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_storeu_epi64 (void *__P, __m128i __A)
{
  struct __storeu_epi64 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi64*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_storeu_epi64 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedqudi128_mask ((__v2di *) __P,
             (__v2di) __A,
             (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_storeu_epi64 (void *__P, __m256i __A)
{
  struct __storeu_epi64 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi64*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_storeu_epi64 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_storedqudi256_mask ((__v4di *) __P,
             (__v4di) __A,
             (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_storeu_epi32 (void *__P, __m128i __A)
{
  struct __storeu_epi32 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi32*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_storeu_epi32 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedqusi128_mask ((__v4si *) __P,
             (__v4si) __A,
             (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_storeu_epi32 (void *__P, __m256i __A)
{
  struct __storeu_epi32 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi32*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_storeu_epi32 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_storedqusi256_mask ((__v8si *) __P,
             (__v8si) __A,
             (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_storeu_pd (void *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storeupd128_mask ((__v2df *) __P,
           (__v2df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_storeu_pd (void *__P, __mmask8 __U, __m256d __A)
{
  __builtin_ia32_storeupd256_mask ((__v4df *) __P,
           (__v4df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_storeu_ps (void *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storeups128_mask ((__v4sf *) __P,
           (__v4sf) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_storeu_ps (void *__P, __mmask8 __U, __m256 __A)
{
  __builtin_ia32_storeups256_mask ((__v8sf *) __P,
           (__v8sf) __A,
           (__mmask8) __U);
}


static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpackhi_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpackhi_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpackhi_pd(__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpackhi_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpackhi_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpackhi_pd(__A, __B),
                                           (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_pd(__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpackhi_pd(__A, __B),
                                           (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpackhi_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpackhi_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpackhi_ps(__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpackhi_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpackhi_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpackhi_ps(__A, __B),
                                           (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_ps(__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpackhi_ps(__A, __B),
                                           (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpacklo_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpacklo_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpacklo_pd(__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpacklo_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpacklo_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpacklo_pd(__A, __B),
                                           (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_pd(__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpacklo_pd(__A, __B),
                                           (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpacklo_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpacklo_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpacklo_ps(__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpacklo_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpacklo_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpacklo_ps(__A, __B),
                                           (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_ps(__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpacklo_ps(__A, __B),
                                           (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_rcp14_pd (__m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_rcp14_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rcp14_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_rcp14_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rcp14_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rcp14_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_rcp14_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_rcp14_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rcp14_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_rcp14_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rcp14_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rcp14_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}
# 6081 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_permutevar_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128i __C)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                            (__v2df)_mm_permutevar_pd(__A, __C),
                                            (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_permutevar_pd(__mmask8 __U, __m128d __A, __m128i __C)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                            (__v2df)_mm_permutevar_pd(__A, __C),
                                            (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutevar_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256i __C)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                         (__v4df)_mm256_permutevar_pd(__A, __C),
                                         (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutevar_pd(__mmask8 __U, __m256d __A, __m256i __C)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                         (__v4df)_mm256_permutevar_pd(__A, __C),
                                         (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_permutevar_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128i __C)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                            (__v4sf)_mm_permutevar_ps(__A, __C),
                                            (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_permutevar_ps(__mmask8 __U, __m128 __A, __m128i __C)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                            (__v4sf)_mm_permutevar_ps(__A, __C),
                                            (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutevar_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256i __C)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                          (__v8sf)_mm256_permutevar_ps(__A, __C),
                                          (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutevar_ps(__mmask8 __U, __m256 __A, __m256i __C)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                          (__v8sf)_mm256_permutevar_ps(__A, __C),
                                          (__v8sf)_mm256_setzero_ps());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_test_epi32_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd128_mask((__v4si)(__m128i)((_mm_and_si128 (__A, __B))), (__v4si)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_test_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd128_mask((__v4si)(__m128i)((_mm_and_si128 (__A, __B))), (__v4si)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_test_epi32_mask (__m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd256_mask((__v8si)(__m256i)((_mm256_and_si256 (__A, __B))), (__v8si)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_test_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd256_mask((__v8si)(__m256i)((_mm256_and_si256 (__A, __B))), (__v8si)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_test_epi64_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq128_mask((__v2di)(__m128i)((_mm_and_si128 (__A, __B))), (__v2di)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_test_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq128_mask((__v2di)(__m128i)((_mm_and_si128 (__A, __B))), (__v2di)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_test_epi64_mask (__m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq256_mask((__v4di)(__m256i)((_mm256_and_si256 (__A, __B))), (__v4di)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_test_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq256_mask((__v4di)(__m256i)((_mm256_and_si256 (__A, __B))), (__v4di)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_testn_epi32_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd128_mask((__v4si)(__m128i)((_mm_and_si128 (__A, __B))), (__v4si)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_testn_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd128_mask((__v4si)(__m128i)((_mm_and_si128 (__A, __B))), (__v4si)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_testn_epi32_mask (__m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd256_mask((__v8si)(__m256i)((_mm256_and_si256 (__A, __B))), (__v8si)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_testn_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd256_mask((__v8si)(__m256i)((_mm256_and_si256 (__A, __B))), (__v8si)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_testn_epi64_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq128_mask((__v2di)(__m128i)((_mm_and_si128 (__A, __B))), (__v2di)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_testn_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq128_mask((__v2di)(__m128i)((_mm_and_si128 (__A, __B))), (__v2di)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_testn_epi64_mask (__m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq256_mask((__v4di)(__m256i)((_mm256_and_si256 (__A, __B))), (__v4di)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_testn_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq256_mask((__v4di)(__m256i)((_mm256_and_si256 (__A, __B))), (__v4di)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpackhi_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpackhi_epi32(__A, __B),
                                           (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpackhi_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpackhi_epi32(__A, __B),
                                           (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpackhi_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpackhi_epi32(__A, __B),
                                        (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpackhi_epi32(__A, __B),
                                        (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpackhi_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpackhi_epi64(__A, __B),
                                           (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpackhi_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpackhi_epi64(__A, __B),
                                           (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpackhi_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpackhi_epi64(__A, __B),
                                        (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpackhi_epi64(__A, __B),
                                        (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpacklo_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpacklo_epi32(__A, __B),
                                           (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpacklo_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpacklo_epi32(__A, __B),
                                           (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpacklo_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpacklo_epi32(__A, __B),
                                        (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpacklo_epi32(__A, __B),
                                        (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpacklo_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpacklo_epi64(__A, __B),
                                           (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpacklo_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpacklo_epi64(__A, __B),
                                           (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpacklo_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpacklo_epi64(__A, __B),
                                        (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpacklo_epi64(__A, __B),
                                        (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_sra_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sra_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sra_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sra_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sra_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sra_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sra_epi32(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sra_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srai_epi32(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srai_epi32(__A, (int)__B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srai_epi32(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srai_epi32(__A, (int)__B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srai_epi32(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srai_epi32(__A, (int)__B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srai_epi32(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srai_epi32(__A, (int)__B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_sra_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psraq128((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_sra_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U, (__v2di)_mm_sra_epi64(__A, __B), (__v2di)__W);


}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sra_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U, (__v2di)_mm_sra_epi64(__A, __B), (__v2di)_mm_setzero_si128());


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_sra_epi64(__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psraq256((__v4di) __A, (__v2di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sra_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U, (__v4di)_mm256_sra_epi64(__A, __B), (__v4di)__W);


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sra_epi64(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U, (__v4di)_mm256_sra_epi64(__A, __B), (__v4di)_mm256_setzero_si256());


}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_srai_epi64(__m128i __A, unsigned int __imm)
{
  return (__m128i)__builtin_ia32_psraqi128((__v2di)__A, (int)__imm);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_srai_epi64(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __imm)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U, (__v2di)_mm_srai_epi64(__A, __imm), (__v2di)__W);


}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srai_epi64(__mmask8 __U, __m128i __A, unsigned int __imm)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U, (__v2di)_mm_srai_epi64(__A, __imm), (__v2di)_mm_setzero_si128());


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_srai_epi64(__m256i __A, unsigned int __imm)
{
  return (__m256i)__builtin_ia32_psraqi256((__v4di)__A, (int)__imm);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srai_epi64(__m256i __W, __mmask8 __U, __m256i __A,
                       unsigned int __imm)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U, (__v4di)_mm256_srai_epi64(__A, __imm), (__v4di)__W);


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srai_epi64(__mmask8 __U, __m256i __A, unsigned int __imm)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U, (__v4di)_mm256_srai_epi64(__A, __imm), (__v4di)_mm256_setzero_si256());


}
# 6691 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_rsqrt14_pd (__m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
                 (__v2df)
                 _mm_setzero_pd (),
                 (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_rsqrt14_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
                 (__v2df) __W,
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rsqrt14_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
                 (__v2df)
                 _mm_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_rsqrt14_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
                 (__v4df)
                 _mm256_setzero_pd (),
                 (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rsqrt14_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
                 (__v4df) __W,
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rsqrt14_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
                 (__v4df)
                 _mm256_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_rsqrt14_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_rsqrt14_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rsqrt14_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_rsqrt14_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rsqrt14_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
                (__v8sf) __W,
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rsqrt14_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_f32x4(__m128 __A)
{
  return (__m256)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcast_f32x4(__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                            (__v8sf)_mm256_broadcast_f32x4(__A),
                                            (__v8sf)__O);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcast_f32x4 (__mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                            (__v8sf)_mm256_broadcast_f32x4(__A),
                                            (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_i32x4(__m128i __A)
{
  return (__m256i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcast_i32x4(__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                            (__v8si)_mm256_broadcast_i32x4(__A),
                                            (__v8si)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcast_i32x4(__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                            (__v8si)_mm256_broadcast_i32x4(__A),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcastsd_pd (__m256d __O, __mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256(__M,
                                              (__v4df) _mm256_broadcastsd_pd(__A),
                                              (__v4df) __O);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcastsd_pd (__mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256(__M,
                                              (__v4df) _mm256_broadcastsd_pd(__A),
                                              (__v4df) _mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_broadcastss_ps (__m128 __O, __mmask8 __M, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128(__M,
                                             (__v4sf) _mm_broadcastss_ps(__A),
                                             (__v4sf) __O);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_broadcastss_ps (__mmask8 __M, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128(__M,
                                             (__v4sf) _mm_broadcastss_ps(__A),
                                             (__v4sf) _mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcastss_ps (__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256(__M,
                                             (__v8sf) _mm256_broadcastss_ps(__A),
                                             (__v8sf) __O);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcastss_ps (__mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256(__M,
                                             (__v8sf) _mm256_broadcastss_ps(__A),
                                             (__v8sf) _mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_broadcastd_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128(__M,
                                             (__v4si) _mm_broadcastd_epi32(__A),
                                             (__v4si) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_broadcastd_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128(__M,
                                             (__v4si) _mm_broadcastd_epi32(__A),
                                             (__v4si) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcastd_epi32 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256(__M,
                                             (__v8si) _mm256_broadcastd_epi32(__A),
                                             (__v8si) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcastd_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256(__M,
                                             (__v8si) _mm256_broadcastd_epi32(__A),
                                             (__v8si) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_broadcastq_epi64 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                             (__v2di) _mm_broadcastq_epi64(__A),
                                             (__v2di) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                             (__v2di) _mm_broadcastq_epi64(__A),
                                             (__v2di) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcastq_epi64 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                             (__v4di) _mm256_broadcastq_epi64(__A),
                                             (__v4di) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                             (__v4di) _mm256_broadcastq_epi64(__A),
                                             (__v4di) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtsepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtsepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtsepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsdb256mem_mask ((__v16qi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtsepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
               (__v8hi)_mm_setzero_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
               (__v8hi)__O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtsepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtsepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
               (__v8hi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtsepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtsepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtsepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtsepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
               (__v4si)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
               (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtsepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
               (__v4si) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtsepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
               (__v4si)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
               (__v4si)__O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
               (__v4si) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtsepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
               (__v8hi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtsepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtsepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
               (__v8hi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtusepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtusepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtusepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusdb256mem_mask ((__v16qi*) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtusepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
                (__v8hi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtusepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtusepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
                (__v8hi) _mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtusepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtusepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtusepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtusepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
                (__v4si)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
                (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtusepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
                (__v4si) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtusepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
                (__v4si)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
                (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
                (__v4si) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtusepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
                (__v8hi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtusepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtusepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
                (__v8hi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtepi32_epi8 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v4si)__A, __v4qi), (__v4qi){0, 0, 0, 0}, 0, 1,
      2, 3, 4, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
              (__v16qi)
              _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi32_epi8 (__m256i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v8si)__A, __v8qi),
      (__v8qi){0, 0, 0, 0, 0, 0, 0, 0}, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
      12, 13, 14, 15);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovdb256mem_mask ((__v16qi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtepi32_epi16 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v4si)__A, __v4hi), (__v4hi){0, 0, 0, 0}, 0, 1,
      2, 3, 4, 5, 6, 7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi32_epi16 (__m256i __A)
{
  return (__m128i)__builtin_convertvector((__v8si)__A, __v8hi);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtepi64_epi8 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v2di)__A, __v2qi), (__v2qi){0, 0}, 0, 1, 2, 3,
      3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi64_epi8 (__m256i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v4di)__A, __v4qi), (__v4qi){0, 0, 0, 0}, 0, 1,
      2, 3, 4, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtepi64_epi32 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v2di)__A, __v2si), (__v2si){0, 0}, 0, 1, 2, 3);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
              (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
              (__v4si) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi64_epi32 (__m256i __A)
{
  return (__m128i)__builtin_convertvector((__v4di)__A, __v4si);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm256_cvtepi64_epi32(__A),
                                             (__v4si)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm256_cvtepi64_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_cvtepi64_epi16 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v2di)__A, __v2hi), (__v2hi){0, 0}, 0, 1, 2, 3,
      3, 3, 3, 3);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi)__O,
              __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi64_epi16 (__m256i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v4di)__A, __v4hi), (__v4hi){0, 0, 0, 0}, 0, 1,
      2, 3, 4, 5, 6, 7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}
# 8086 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_permutexvar_pd (__m256i __X, __m256d __Y)
{
  return (__m256d)__builtin_ia32_permvardf256((__v4df)__Y, (__v4di)__X);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutexvar_pd (__m256d __W, __mmask8 __U, __m256i __X,
          __m256d __Y)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                        (__v4df)_mm256_permutexvar_pd(__X, __Y),
                                        (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_pd (__mmask8 __U, __m256i __X, __m256d __Y)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                        (__v4df)_mm256_permutexvar_pd(__X, __Y),
                                        (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_permutexvar_epi64 ( __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_permvardi256((__v4di) __Y, (__v4di) __X);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_epi64 (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                     (__v4di)_mm256_permutexvar_epi64(__X, __Y),
                                     (__v4di)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutexvar_epi64 (__m256i __W, __mmask8 __M, __m256i __X,
             __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                     (__v4di)_mm256_permutexvar_epi64(__X, __Y),
                                     (__v4di)__W);
}



static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutexvar_ps(__m256 __W, __mmask8 __U, __m256i __X, __m256 __Y)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                        (__v8sf)_mm256_permutevar8x32_ps((__Y), (__X)),
                                        (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_ps(__mmask8 __U, __m256i __X, __m256 __Y)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                        (__v8sf)_mm256_permutevar8x32_ps((__Y), (__X)),
                                        (__v8sf)_mm256_setzero_ps());
}



static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutexvar_epi32(__m256i __W, __mmask8 __M, __m256i __X,
                              __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                     (__v8si)_mm256_permutevar8x32_epi32((__Y), (__X)),
                                     (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_epi32(__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                     (__v8si)_mm256_permutevar8x32_epi32((__Y), (__X)),
                                     (__v8si)_mm256_setzero_si256());
}
# 8225 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_movehdup_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_movehdup_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_movehdup_ps (__mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_movehdup_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_movehdup_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_movehdup_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_movehdup_ps (__mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_movehdup_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_moveldup_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_moveldup_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_moveldup_ps (__mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_moveldup_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_moveldup_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_moveldup_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_moveldup_ps (__mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_moveldup_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}
# 8309 "/usr/lib/clang/18/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_mov_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_selectpd_128 ((__mmask8) __U,
              (__v2df) __A,
              (__v2df) __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mov_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_selectpd_128 ((__mmask8) __U,
              (__v2df) __A,
              (__v2df) _mm_setzero_pd ());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mov_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_selectpd_256 ((__mmask8) __U,
              (__v4df) __A,
              (__v4df) __W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mov_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_selectpd_256 ((__mmask8) __U,
              (__v4df) __A,
              (__v4df) _mm256_setzero_pd ());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_mov_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_selectps_128 ((__mmask8) __U,
             (__v4sf) __A,
             (__v4sf) __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mov_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_selectps_128 ((__mmask8) __U,
             (__v4sf) __A,
             (__v4sf) _mm_setzero_ps ());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mov_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_selectps_256 ((__mmask8) __U,
             (__v8sf) __A,
             (__v8sf) __W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mov_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_selectps_256 ((__mmask8) __U,
             (__v8sf) __A,
             (__v8sf) _mm256_setzero_ps ());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtph_ps (__m128 __W, __mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_vcvtph2ps_mask ((__v8hi) __A,
             (__v4sf) __W,
             (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtph_ps (__mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_vcvtph2ps_mask ((__v8hi) __A,
             (__v4sf)
             _mm_setzero_ps (),
             (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtph_ps (__m256 __W, __mmask8 __U, __m128i __A)
{
  return (__m256) __builtin_ia32_vcvtph2ps256_mask ((__v8hi) __A,
                (__v8sf) __W,
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtph_ps (__mmask8 __U, __m128i __A)
{
  return (__m256) __builtin_ia32_vcvtph2ps256_mask ((__v8hi) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}
# 110 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512bwintrin.h" 1 3
# 17 "/usr/lib/clang/18/include/avx512bwintrin.h" 3
typedef unsigned int __mmask32;
typedef unsigned long long __mmask64;
# 28 "/usr/lib/clang/18/include/avx512bwintrin.h" 3
static __inline __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_knot_mask32(__mmask32 __M)
{
  return __builtin_ia32_knotsi(__M);
}

static __inline __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _knot_mask64(__mmask64 __M) {
  return __builtin_ia32_knotdi(__M);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kand_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kandsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _kand_mask64(__mmask64 __A,
                                                            __mmask64 __B) {
  return (__mmask64)__builtin_ia32_kanddi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kandn_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kandnsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _kandn_mask64(__mmask64 __A,
                                                             __mmask64 __B) {
  return (__mmask64)__builtin_ia32_kandndi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kor_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_korsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _kor_mask64(__mmask64 __A,
                                                           __mmask64 __B) {
  return (__mmask64)__builtin_ia32_kordi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kxnor_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kxnorsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _kxnor_mask64(__mmask64 __A,
                                                             __mmask64 __B) {
  return (__mmask64)__builtin_ia32_kxnordi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kxor_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kxorsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _kxor_mask64(__mmask64 __A,
                                                            __mmask64 __B) {
  return (__mmask64)__builtin_ia32_kxordi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kortestc_mask32_u8(__mmask32 __A, __mmask32 __B)
{
  return (unsigned char)__builtin_ia32_kortestcsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kortestz_mask32_u8(__mmask32 __A, __mmask32 __B)
{
  return (unsigned char)__builtin_ia32_kortestzsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kortest_mask32_u8(__mmask32 __A, __mmask32 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_kortestcsi(__A, __B);
  return (unsigned char)__builtin_ia32_kortestzsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kortestc_mask64_u8(__mmask64 __A, __mmask64 __B) {
  return (unsigned char)__builtin_ia32_kortestcdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kortestz_mask64_u8(__mmask64 __A, __mmask64 __B) {
  return (unsigned char)__builtin_ia32_kortestzdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kortest_mask64_u8(__mmask64 __A, __mmask64 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_kortestcdi(__A, __B);
  return (unsigned char)__builtin_ia32_kortestzdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_ktestc_mask32_u8(__mmask32 __A, __mmask32 __B)
{
  return (unsigned char)__builtin_ia32_ktestcsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_ktestz_mask32_u8(__mmask32 __A, __mmask32 __B)
{
  return (unsigned char)__builtin_ia32_ktestzsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_ktest_mask32_u8(__mmask32 __A, __mmask32 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_ktestcsi(__A, __B);
  return (unsigned char)__builtin_ia32_ktestzsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_ktestc_mask64_u8(__mmask64 __A, __mmask64 __B) {
  return (unsigned char)__builtin_ia32_ktestcdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_ktestz_mask64_u8(__mmask64 __A, __mmask64 __B) {
  return (unsigned char)__builtin_ia32_ktestzdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_ktest_mask64_u8(__mmask64 __A, __mmask64 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_ktestcdi(__A, __B);
  return (unsigned char)__builtin_ia32_ktestzdi(__A, __B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_kadd_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kaddsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _kadd_mask64(__mmask64 __A,
                                                            __mmask64 __B) {
  return (__mmask64)__builtin_ia32_kadddi((__mmask64)__A, (__mmask64)__B);
}
# 184 "/usr/lib/clang/18/include/avx512bwintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_cvtmask32_u32(__mmask32 __A) {
  return (unsigned int)__builtin_ia32_kmovd((__mmask32)__A);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_cvtmask64_u64(__mmask64 __A) {
  return (unsigned long long)__builtin_ia32_kmovq((__mmask64)__A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_cvtu32_mask32(unsigned int __A) {
  return (__mmask32)__builtin_ia32_kmovd((__mmask32)__A);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_cvtu64_mask64(unsigned long long __A) {
  return (__mmask64)__builtin_ia32_kmovq((__mmask64)__A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_load_mask32(__mmask32 *__A) {
  return (__mmask32)__builtin_ia32_kmovd(*(__mmask32 *)__A);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _load_mask64(__mmask64 *__A) {
  return (__mmask64)__builtin_ia32_kmovq(*(__mmask64 *)__A);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_store_mask32(__mmask32 *__A, __mmask32 __B) {
  *(__mmask32 *)__A = __builtin_ia32_kmovd((__mmask32)__B);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _store_mask64(__mmask64 *__A,
                                                        __mmask64 __B) {
  *(__mmask64 *)__A = __builtin_ia32_kmovq((__mmask64)__B);
}
# 365 "/usr/lib/clang/18/include/avx512bwintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_add_epi8 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v64qu) __A + (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_add_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_add_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_add_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_add_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_sub_epi8 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v64qu) __A - (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_sub_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_sub_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_sub_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_sub_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_add_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v32hu) __A + (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_add_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_add_epi16(__A, __B),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_add_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_add_epi16(__A, __B),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_sub_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v32hu) __A - (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_sub_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_sub_epi16(__A, __B),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_sub_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_sub_epi16(__A, __B),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mullo_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v32hu) __A * (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_mullo_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_mullo_epi16(__A, __B),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_mullo_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_mullo_epi16(__A, __B),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_blend_epi8 (__mmask64 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectb_512 ((__mmask64) __U,
              (__v64qi) __W,
              (__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_blend_epi16 (__mmask32 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectw_512 ((__mmask32) __U,
              (__v32hi) __W,
              (__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_abs_epi8 (__m512i __A)
{
  return (__m512i)__builtin_elementwise_abs((__v64qs)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_abs_epi8 (__m512i __W, __mmask64 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_abs_epi8(__A),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_abs_epi8 (__mmask64 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_abs_epi8(__A),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_abs_epi16 (__m512i __A)
{
  return (__m512i)__builtin_elementwise_abs((__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_abs_epi16 (__m512i __W, __mmask32 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_abs_epi16(__A),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_abs_epi16 (__mmask32 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_abs_epi16(__A),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_packs_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packssdw512((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_packs_epi32(__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packs_epi32(__A, __B),
                                       (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_packs_epi32(__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packs_epi32(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_packs_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packsswb512((__v32hi)__A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_packs_epi16(__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packs_epi16(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_packs_epi16(__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packs_epi16(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_packus_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packusdw512((__v16si) __A, (__v16si) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_packus_epi32(__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packus_epi32(__A, __B),
                                       (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_packus_epi32(__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packus_epi32(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_packus_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packuswb512((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_packus_epi16(__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packus_epi16(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_packus_epi16(__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packus_epi16(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_adds_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_add_sat((__v64qs)__A, (__v64qs)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_adds_epi8 (__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_adds_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_adds_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_adds_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_adds_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_add_sat((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_adds_epi16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_adds_epi16(__A, __B),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_adds_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_adds_epi16(__A, __B),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_adds_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_add_sat((__v64qu) __A, (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_adds_epu8 (__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_adds_epu8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_adds_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_adds_epu8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_adds_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_add_sat((__v32hu) __A, (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_adds_epu16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_adds_epu16(__A, __B),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_adds_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_adds_epu16(__A, __B),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_avg_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pavgb512((__v64qi)__A, (__v64qi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_avg_epu8 (__m512i __W, __mmask64 __U, __m512i __A,
          __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
              (__v64qi)_mm512_avg_epu8(__A, __B),
              (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_avg_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
              (__v64qi)_mm512_avg_epu8(__A, __B),
              (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_avg_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pavgw512((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_avg_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
           __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
              (__v32hi)_mm512_avg_epu16(__A, __B),
              (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_avg_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
              (__v32hi)_mm512_avg_epu16(__A, __B),
              (__v32hi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_max_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v64qs) __A, (__v64qs) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_epi8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_max_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_max_epi8 (__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_max_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_max_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_epi16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_max_epi16(__A, __B),
                                            (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_max_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
           __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_max_epi16(__A, __B),
                                            (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_max_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v64qu)__A, (__v64qu)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_epu8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_max_epu8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_max_epu8 (__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_max_epu8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_max_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v32hu)__A, (__v32hu)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_epu16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_max_epu16(__A, __B),
                                            (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_max_epu16 (__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_max_epu16(__A, __B),
                                            (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_min_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v64qs) __A, (__v64qs) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_epi8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_min_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_min_epi8 (__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_min_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_min_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_epi16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_min_epi16(__A, __B),
                                            (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_min_epi16 (__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_min_epi16(__A, __B),
                                            (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_min_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v64qu)__A, (__v64qu)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_epu8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_min_epu8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_min_epu8 (__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_min_epu8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_min_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v32hu)__A, (__v32hu)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_epu16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_min_epu16(__A, __B),
                                            (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_min_epu16 (__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_min_epu16(__A, __B),
                                            (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_shuffle_epi8(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pshufb512((__v64qi)__A,(__v64qi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_shuffle_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                         (__v64qi)_mm512_shuffle_epi8(__A, __B),
                                         (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_shuffle_epi8(__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                         (__v64qi)_mm512_shuffle_epi8(__A, __B),
                                         (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_subs_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_sub_sat((__v64qs)__A, (__v64qs)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_subs_epi8 (__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_subs_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_subs_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_subs_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_subs_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_sub_sat((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_subs_epi16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_subs_epi16(__A, __B),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_subs_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_subs_epi16(__A, __B),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_subs_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_sub_sat((__v64qu) __A, (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_subs_epu8 (__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_subs_epu8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_subs_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_subs_epu8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_subs_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_sub_sat((__v32hu) __A, (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_subs_epu16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_subs_epu16(__A, __B),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_subs_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_subs_epu16(__A, __B),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_permutex2var_epi16(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpermi2varhi512((__v32hi)__A, (__v32hi)__I,
                                                 (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_permutex2var_epi16(__m512i __A, __mmask32 __U, __m512i __I,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                              (__v32hi)_mm512_permutex2var_epi16(__A, __I, __B),
                              (__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_epi16(__m512i __A, __m512i __I, __mmask32 __U,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                              (__v32hi)_mm512_permutex2var_epi16(__A, __I, __B),
                              (__v32hi)__I);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_epi16(__mmask32 __U, __m512i __A, __m512i __I,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                              (__v32hi)_mm512_permutex2var_epi16(__A, __I, __B),
                              (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mulhrs_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pmulhrsw512((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_mulhrs_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_mulhrs_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_mulhrs_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_mulhrs_epi16(__A, __B),
                                         (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mulhi_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pmulhw512((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_mulhi_epi16(__m512i __W, __mmask32 __U, __m512i __A,
       __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_mulhi_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_mulhi_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_mulhi_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mulhi_epu16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pmulhuw512((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_mulhi_epu16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_mulhi_epu16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_mulhi_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_mulhi_epu16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maddubs_epi16(__m512i __X, __m512i __Y) {
  return (__m512i)__builtin_ia32_pmaddubsw512((__v64qi)__X, (__v64qi)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_maddubs_epi16(__m512i __W, __mmask32 __U, __m512i __X,
                          __m512i __Y) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32) __U,
                                        (__v32hi)_mm512_maddubs_epi16(__X, __Y),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_maddubs_epi16(__mmask32 __U, __m512i __X, __m512i __Y) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32) __U,
                                        (__v32hi)_mm512_maddubs_epi16(__X, __Y),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_madd_epi16(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_pmaddwd512((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_madd_epi16(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_madd_epi16(__A, __B),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_madd_epi16(__mmask16 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_madd_epi16(__A, __B),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_cvtsepi16_epi8 (__m512i __A) {
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
               (__v32qi)_mm256_setzero_si256(),
               (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
               (__v32qi)__O,
               __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi16_epi8 (__mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
               (__v32qi) _mm256_setzero_si256(),
               __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_cvtusepi16_epi8 (__m512i __A) {
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
                (__v32qi) _mm256_setzero_si256(),
                (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
                (__v32qi) __O,
                __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi16_epi8 (__mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
                (__v32qi) _mm256_setzero_si256(),
                __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_cvtepi16_epi8 (__m512i __A) {
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
              (__v32qi) _mm256_undefined_si256(),
              (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
              (__v32qi) __O,
              __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi16_epi8 (__mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
              (__v32qi) _mm256_setzero_si256(),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovwb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovswb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovuswb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_unpackhi_epi8(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v64qi)__A, (__v64qi)__B,
                                          8, 64+8, 9, 64+9,
                                          10, 64+10, 11, 64+11,
                                          12, 64+12, 13, 64+13,
                                          14, 64+14, 15, 64+15,
                                          24, 64+24, 25, 64+25,
                                          26, 64+26, 27, 64+27,
                                          28, 64+28, 29, 64+29,
                                          30, 64+30, 31, 64+31,
                                          40, 64+40, 41, 64+41,
                                          42, 64+42, 43, 64+43,
                                          44, 64+44, 45, 64+45,
                                          46, 64+46, 47, 64+47,
                                          56, 64+56, 57, 64+57,
                                          58, 64+58, 59, 64+59,
                                          60, 64+60, 61, 64+61,
                                          62, 64+62, 63, 64+63);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_unpackhi_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpackhi_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpackhi_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_unpackhi_epi16(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v32hi)__A, (__v32hi)__B,
                                          4, 32+4, 5, 32+5,
                                          6, 32+6, 7, 32+7,
                                          12, 32+12, 13, 32+13,
                                          14, 32+14, 15, 32+15,
                                          20, 32+20, 21, 32+21,
                                          22, 32+22, 23, 32+23,
                                          28, 32+28, 29, 32+29,
                                          30, 32+30, 31, 32+31);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_unpackhi_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpackhi_epi16(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpackhi_epi16(__A, __B),
                                       (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_unpacklo_epi8(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v64qi)__A, (__v64qi)__B,
                                          0, 64+0, 1, 64+1,
                                          2, 64+2, 3, 64+3,
                                          4, 64+4, 5, 64+5,
                                          6, 64+6, 7, 64+7,
                                          16, 64+16, 17, 64+17,
                                          18, 64+18, 19, 64+19,
                                          20, 64+20, 21, 64+21,
                                          22, 64+22, 23, 64+23,
                                          32, 64+32, 33, 64+33,
                                          34, 64+34, 35, 64+35,
                                          36, 64+36, 37, 64+37,
                                          38, 64+38, 39, 64+39,
                                          48, 64+48, 49, 64+49,
                                          50, 64+50, 51, 64+51,
                                          52, 64+52, 53, 64+53,
                                          54, 64+54, 55, 64+55);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_unpacklo_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpacklo_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpacklo_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_unpacklo_epi16(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v32hi)__A, (__v32hi)__B,
                                          0, 32+0, 1, 32+1,
                                          2, 32+2, 3, 32+3,
                                          8, 32+8, 9, 32+9,
                                          10, 32+10, 11, 32+11,
                                          16, 32+16, 17, 32+17,
                                          18, 32+18, 19, 32+19,
                                          24, 32+24, 25, 32+25,
                                          26, 32+26, 27, 32+27);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_unpacklo_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpacklo_epi16(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpacklo_epi16(__A, __B),
                                       (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_cvtepi8_epi16(__m256i __A)
{


  return (__m512i)__builtin_convertvector((__v32qs)__A, __v32hi);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi8_epi16(__m512i __W, __mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepi8_epi16(__A),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi8_epi16(__mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepi8_epi16(__A),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_cvtepu8_epi16(__m256i __A)
{
  return (__m512i)__builtin_convertvector((__v32qu)__A, __v32hi);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu8_epi16(__m512i __W, __mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepu8_epi16(__A),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu8_epi16(__mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepu8_epi16(__A),
                                             (__v32hi)_mm512_setzero_si512());
}
# 1453 "/usr/lib/clang/18/include/avx512bwintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_sllv_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_psllv32hi((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_sllv_epi16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_sllv_epi16(__A, __B),
                                           (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_sllv_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_sllv_epi16(__A, __B),
                                           (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_sll_epi16(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psllw512((__v32hi) __A, (__v8hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_sll_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sll_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_sll_epi16(__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sll_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_slli_epi16(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psllwi512((__v32hi)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_slli_epi16(__m512i __W, __mmask32 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_slli_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_slli_epi16(__mmask32 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_slli_epi16(__A, __B),
                                         (__v32hi)_mm512_setzero_si512());
}




static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_srlv_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_psrlv32hi((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_srlv_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srlv_epi16(__A, __B),
                                           (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_srlv_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srlv_epi16(__A, __B),
                                           (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_srav_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_psrav32hi((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_srav_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srav_epi16(__A, __B),
                                           (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_srav_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srav_epi16(__A, __B),
                                           (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_sra_epi16(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psraw512((__v32hi) __A, (__v8hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_sra_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sra_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_sra_epi16(__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sra_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_srai_epi16(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psrawi512((__v32hi)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_srai_epi16(__m512i __W, __mmask32 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srai_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_srai_epi16(__mmask32 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srai_epi16(__A, __B),
                                         (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_srl_epi16(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrlw512((__v32hi) __A, (__v8hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_srl_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_srl_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_srl_epi16(__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_srl_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_srli_epi16(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psrlwi512((__v32hi)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_srli_epi16(__m512i __W, __mmask32 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srli_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_srli_epi16(__mmask32 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srli_epi16(__A, (unsigned int)__B),
                                         (__v32hi)_mm512_setzero_si512());
}




static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_mov_epi16 (__m512i __W, __mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectw_512 ((__mmask32) __U,
                (__v32hi) __A,
                (__v32hi) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_mov_epi16 (__mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectw_512 ((__mmask32) __U,
                (__v32hi) __A,
                (__v32hi) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_mov_epi8 (__m512i __W, __mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectb_512 ((__mmask64) __U,
                (__v64qi) __A,
                (__v64qi) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_mov_epi8 (__mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectb_512 ((__mmask64) __U,
                (__v64qi) __A,
                (__v64qi) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_set1_epi8 (__m512i __O, __mmask64 __M, char __A)
{
  return (__m512i) __builtin_ia32_selectb_512(__M,
                                              (__v64qi)_mm512_set1_epi8(__A),
                                              (__v64qi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_set1_epi8 (__mmask64 __M, char __A)
{
  return (__m512i) __builtin_ia32_selectb_512(__M,
                                              (__v64qi) _mm512_set1_epi8(__A),
                                              (__v64qi) _mm512_setzero_si512());
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512"))) _mm512_kunpackd(__mmask64 __A,
                                                               __mmask64 __B) {
  return (__mmask64) __builtin_ia32_kunpckdi ((__mmask64) __A,
                (__mmask64) __B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,no-evex512")))
_mm512_kunpackw (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_kunpcksi ((__mmask32) __A,
                (__mmask32) __B);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_loadu_epi16 (void const *__P)
{
  struct __loadu_epi16 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi16*)__P)->__v;
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_loadu_epi16 (__m512i __W, __mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquhi512_mask ((const __v32hi *) __P,
                 (__v32hi) __W,
                 (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_loadu_epi16 (__mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquhi512_mask ((const __v32hi *) __P,
                 (__v32hi)
                 _mm512_setzero_si512 (),
                 (__mmask32) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_loadu_epi8 (void const *__P)
{
  struct __loadu_epi8 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi8*)__P)->__v;
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquqi512_mask ((const __v64qi *) __P,
                 (__v64qi) __W,
                 (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquqi512_mask ((const __v64qi *) __P,
                 (__v64qi)
                 _mm512_setzero_si512 (),
                 (__mmask64) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_storeu_epi16 (void *__P, __m512i __A)
{
  struct __storeu_epi16 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi16*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_storeu_epi16 (void *__P, __mmask32 __U, __m512i __A)
{
  __builtin_ia32_storedquhi512_mask ((__v32hi *) __P,
             (__v32hi) __A,
             (__mmask32) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_storeu_epi8 (void *__P, __m512i __A)
{
  struct __storeu_epi8 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi8*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_storeu_epi8 (void *__P, __mmask64 __U, __m512i __A)
{
  __builtin_ia32_storedquqi512_mask ((__v64qi *) __P,
             (__v64qi) __A,
             (__mmask64) __U);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_test_epi8_mask (__m512i __A, __m512i __B)
{
  return ((__mmask64)__builtin_ia32_cmpb512_mask((__v64qi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v64qi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask64)-1));

}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_test_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return ((__mmask64)__builtin_ia32_cmpb512_mask((__v64qi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v64qi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask64)((__U))));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_test_epi16_mask (__m512i __A, __m512i __B)
{
  return ((__mmask32)__builtin_ia32_cmpw512_mask((__v32hi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v32hi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask32)-1));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_test_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return ((__mmask32)__builtin_ia32_cmpw512_mask((__v32hi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v32hi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask32)((__U))));

}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_testn_epi8_mask (__m512i __A, __m512i __B)
{
  return ((__mmask64)__builtin_ia32_cmpb512_mask((__v64qi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v64qi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask64)-1));
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_testn_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return ((__mmask64)__builtin_ia32_cmpb512_mask((__v64qi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v64qi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask64)((__U))));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_testn_epi16_mask (__m512i __A, __m512i __B)
{
  return ((__mmask32)__builtin_ia32_cmpw512_mask((__v32hi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v32hi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask32)-1));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_testn_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return ((__mmask32)__builtin_ia32_cmpw512_mask((__v32hi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v32hi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask32)((__U))));

}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_movepi8_mask (__m512i __A)
{
  return (__mmask64) __builtin_ia32_cvtb2mask512 ((__v64qi) __A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_movepi16_mask (__m512i __A)
{
  return (__mmask32) __builtin_ia32_cvtw2mask512 ((__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_movm_epi8 (__mmask64 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2b512 (__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_movm_epi16 (__mmask32 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2w512 (__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_broadcastb_epi8 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v16qi) __A, (__v16qi) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcastb_epi8 (__m512i __O, __mmask64 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectb_512(__M,
                                             (__v64qi) _mm512_broadcastb_epi8(__A),
                                             (__v64qi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcastb_epi8 (__mmask64 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectb_512(__M,
                                             (__v64qi) _mm512_broadcastb_epi8(__A),
                                             (__v64qi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_set1_epi16 (__m512i __O, __mmask32 __M, short __A)
{
  return (__m512i) __builtin_ia32_selectw_512(__M,
                                              (__v32hi) _mm512_set1_epi16(__A),
                                              (__v32hi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_set1_epi16 (__mmask32 __M, short __A)
{
  return (__m512i) __builtin_ia32_selectw_512(__M,
                                              (__v32hi) _mm512_set1_epi16(__A),
                                              (__v32hi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_broadcastw_epi16 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v8hi) __A, (__v8hi) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcastw_epi16 (__m512i __O, __mmask32 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectw_512(__M,
                                             (__v32hi) _mm512_broadcastw_epi16(__A),
                                             (__v32hi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcastw_epi16 (__mmask32 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectw_512(__M,
                                             (__v32hi) _mm512_broadcastw_epi16(__A),
                                             (__v32hi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_permutexvar_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_permvarhi512((__v32hi)__B, (__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_epi16 (__mmask32 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                    (__v32hi)_mm512_permutexvar_epi16(__A, __B),
                                    (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_mask_permutexvar_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
             __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                    (__v32hi)_mm512_permutexvar_epi16(__A, __B),
                                    (__v32hi)__W);
}
# 2004 "/usr/lib/clang/18/include/avx512bwintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512"), __min_vector_width__(512)))
_mm512_sad_epu8 (__m512i __A, __m512i __B)
{
 return (__m512i) __builtin_ia32_psadbw512 ((__v64qi) __A,
               (__v64qi) __B);
}
# 115 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512bitalgintrin.h" 1 3
# 23 "/usr/lib/clang/18/include/avx512bitalgintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg,evex512"), __min_vector_width__(512)))
_mm512_popcnt_epi16(__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcntw_512((__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg,evex512"), __min_vector_width__(512)))
_mm512_mask_popcnt_epi16(__m512i __A, __mmask32 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_selectw_512((__mmask32) __U,
              (__v32hi) _mm512_popcnt_epi16(__B),
              (__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg,evex512"), __min_vector_width__(512)))
_mm512_maskz_popcnt_epi16(__mmask32 __U, __m512i __B)
{
  return _mm512_mask_popcnt_epi16((__m512i) _mm512_setzero_si512(),
              __U,
              __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg,evex512"), __min_vector_width__(512)))
_mm512_popcnt_epi8(__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcntb_512((__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg,evex512"), __min_vector_width__(512)))
_mm512_mask_popcnt_epi8(__m512i __A, __mmask64 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_selectb_512((__mmask64) __U,
              (__v64qi) _mm512_popcnt_epi8(__B),
              (__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg,evex512"), __min_vector_width__(512)))
_mm512_maskz_popcnt_epi8(__mmask64 __U, __m512i __B)
{
  return _mm512_mask_popcnt_epi8((__m512i) _mm512_setzero_si512(),
              __U,
              __B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg,evex512"), __min_vector_width__(512)))
_mm512_mask_bitshuffle_epi64_mask(__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_vpshufbitqmb512_mask((__v64qi) __A,
              (__v64qi) __B,
              __U);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg,evex512"), __min_vector_width__(512)))
_mm512_bitshuffle_epi64_mask(__m512i __A, __m512i __B)
{
  return _mm512_mask_bitshuffle_epi64_mask((__mmask64) -1,
              __A,
              __B);
}
# 120 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512cdintrin.h" 1 3
# 22 "/usr/lib/clang/18/include/avx512cdintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_conflict_epi64 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictdi_512 ((__v8di) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_mask_conflict_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_conflict_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_maskz_conflict_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_conflict_epi64(__A),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_conflict_epi32 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictsi_512 ((__v16si) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_mask_conflict_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_conflict_epi32(__A),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_maskz_conflict_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_conflict_epi32(__A),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_lzcnt_epi32 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntd_512 ((__v16si) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_mask_lzcnt_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_lzcnt_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_maskz_lzcnt_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_lzcnt_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_lzcnt_epi64 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntq_512 ((__v8di) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_mask_lzcnt_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_lzcnt_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_maskz_lzcnt_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_lzcnt_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m512i) _mm512_set1_epi64((long long) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd,evex512"), __min_vector_width__(512)))
_mm512_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m512i) _mm512_set1_epi32((int) __A);

}
# 125 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vpopcntdqintrin.h" 1 3
# 24 "/usr/lib/clang/18/include/avx512vpopcntdqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,evex512"), __min_vector_width__(512))) _mm512_popcnt_epi64(__m512i __A) {
  return (__m512i)__builtin_ia32_vpopcntq_512((__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,evex512"), __min_vector_width__(512)))
_mm512_mask_popcnt_epi64(__m512i __W, __mmask8 __U, __m512i __A) {
  return (__m512i)__builtin_ia32_selectq_512(
      (__mmask8)__U, (__v8di)_mm512_popcnt_epi64(__A), (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,evex512"), __min_vector_width__(512)))
_mm512_maskz_popcnt_epi64(__mmask8 __U, __m512i __A) {
  return _mm512_mask_popcnt_epi64((__m512i)_mm512_setzero_si512(), __U, __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,evex512"), __min_vector_width__(512))) _mm512_popcnt_epi32(__m512i __A) {
  return (__m512i)__builtin_ia32_vpopcntd_512((__v16si)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,evex512"), __min_vector_width__(512)))
_mm512_mask_popcnt_epi32(__m512i __W, __mmask16 __U, __m512i __A) {
  return (__m512i)__builtin_ia32_selectd_512(
      (__mmask16)__U, (__v16si)_mm512_popcnt_epi32(__A), (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,evex512"), __min_vector_width__(512)))
_mm512_maskz_popcnt_epi32(__mmask16 __U, __m512i __A) {
  return _mm512_mask_popcnt_epi32((__m512i)_mm512_setzero_si512(), __U, __A);
}
# 130 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vpopcntdqvlintrin.h" 1 3
# 28 "/usr/lib/clang/18/include/avx512vpopcntdqvlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_popcnt_epi64(__m128i __A) {
  return (__m128i)__builtin_ia32_vpopcntq_128((__v2di)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_popcnt_epi64(__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectq_128(
      (__mmask8)__U, (__v2di)_mm_popcnt_epi64(__A), (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_popcnt_epi64(__mmask8 __U, __m128i __A) {
  return _mm_mask_popcnt_epi64((__m128i)_mm_setzero_si128(), __U, __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_popcnt_epi32(__m128i __A) {
  return (__m128i)__builtin_ia32_vpopcntd_128((__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_popcnt_epi32(__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectd_128(
      (__mmask8)__U, (__v4si)_mm_popcnt_epi32(__A), (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_popcnt_epi32(__mmask8 __U, __m128i __A) {
  return _mm_mask_popcnt_epi32((__m128i)_mm_setzero_si128(), __U, __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_popcnt_epi64(__m256i __A) {
  return (__m256i)__builtin_ia32_vpopcntq_256((__v4di)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_popcnt_epi64(__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectq_256(
      (__mmask8)__U, (__v4di)_mm256_popcnt_epi64(__A), (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_popcnt_epi64(__mmask8 __U, __m256i __A) {
  return _mm256_mask_popcnt_epi64((__m256i)_mm256_setzero_si256(), __U, __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_popcnt_epi32(__m256i __A) {
  return (__m256i)__builtin_ia32_vpopcntd_256((__v8si)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_popcnt_epi32(__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectd_256(
      (__mmask8)__U, (__v8si)_mm256_popcnt_epi32(__A), (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_popcnt_epi32(__mmask8 __U, __m256i __A) {
  return _mm256_mask_popcnt_epi32((__m256i)_mm256_setzero_si256(), __U, __A);
}
# 135 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vnniintrin.h" 1 3
# 22 "/usr/lib/clang/18/include/avx512vnniintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_dpbusd_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpdpbusd512((__v16si)__S, (__v16si)__A,
                                             (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_mask_dpbusd_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                    (__v16si)_mm512_dpbusd_epi32(__S, __A, __B),
                                    (__v16si)__S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_maskz_dpbusd_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                    (__v16si)_mm512_dpbusd_epi32(__S, __A, __B),
                                    (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_dpbusds_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpdpbusds512((__v16si)__S, (__v16si)__A,
                                              (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_mask_dpbusds_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                   (__v16si)_mm512_dpbusds_epi32(__S, __A, __B),
                                   (__v16si)__S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_maskz_dpbusds_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                   (__v16si)_mm512_dpbusds_epi32(__S, __A, __B),
                                   (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_dpwssd_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpdpwssd512((__v16si)__S, (__v16si)__A,
                                             (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_mask_dpwssd_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                    (__v16si)_mm512_dpwssd_epi32(__S, __A, __B),
                                    (__v16si)__S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_maskz_dpwssd_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                    (__v16si)_mm512_dpwssd_epi32(__S, __A, __B),
                                    (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_dpwssds_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpdpwssds512((__v16si)__S, (__v16si)__A,
                                              (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_mask_dpwssds_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                   (__v16si)_mm512_dpwssds_epi32(__S, __A, __B),
                                   (__v16si)__S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni,evex512"), __min_vector_width__(512)))
_mm512_maskz_dpwssds_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                   (__v16si)_mm512_dpwssds_epi32(__S, __A, __B),
                                   (__v16si)_mm512_setzero_si512());
}
# 140 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vlvnniintrin.h" 1 3
# 179 "/usr/lib/clang/18/include/avx512vlvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(256)))
_mm256_mask_dpbusd_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpbusd256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)__S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_dpbusd_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpbusd256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(256)))
_mm256_mask_dpbusds_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                    (__v8si)((__m256i)__builtin_ia32_vpdpbusds256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                    (__v8si)__S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_dpbusds_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpbusds256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(256)))
_mm256_mask_dpwssd_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpwssd256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)__S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_dpwssd_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpwssd256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(256)))
_mm256_mask_dpwssds_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                    (__v8si)((__m256i)__builtin_ia32_vpdpwssds256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                    (__v8si)__S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_dpwssds_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                    (__v8si)((__m256i)__builtin_ia32_vpdpwssds256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                    (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(128)))
_mm_mask_dpbusd_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                        (__v4si)((__m128i)__builtin_ia32_vpdpbusd128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                        (__v4si)__S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(128)))
_mm_maskz_dpbusd_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                        (__v4si)((__m128i)__builtin_ia32_vpdpbusd128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                        (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(128)))
_mm_mask_dpbusds_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                       (__v4si)((__m128i)__builtin_ia32_vpdpbusds128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                       (__v4si)__S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(128)))
_mm_maskz_dpbusds_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                       (__v4si)((__m128i)__builtin_ia32_vpdpbusds128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                       (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(128)))
_mm_mask_dpwssd_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                        (__v4si)((__m128i)__builtin_ia32_vpdpwssd128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                        (__v4si)__S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(128)))
_mm_maskz_dpwssd_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                        (__v4si)((__m128i)__builtin_ia32_vpdpwssd128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                        (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(128)))
_mm_mask_dpwssds_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                       (__v4si)((__m128i)__builtin_ia32_vpdpwssds128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                       (__v4si)__S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni,no-evex512"), __min_vector_width__(128)))
_mm_maskz_dpwssds_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                       (__v4si)((__m128i)__builtin_ia32_vpdpwssds128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                       (__v4si)_mm_setzero_si128());
}
# 145 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avxvnniintrin.h" 1 3
# 63 "/usr/lib/clang/18/include/avxvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(256)))
_mm256_dpbusd_avx_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpdpbusd256((__v8si)__S, (__v8si)__A, (__v8si)__B);
}
# 86 "/usr/lib/clang/18/include/avxvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(256)))
_mm256_dpbusds_avx_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpdpbusds256((__v8si)__S, (__v8si)__A, (__v8si)__B);
}
# 107 "/usr/lib/clang/18/include/avxvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(256)))
_mm256_dpwssd_avx_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpdpwssd256((__v8si)__S, (__v8si)__A, (__v8si)__B);
}
# 128 "/usr/lib/clang/18/include/avxvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(256)))
_mm256_dpwssds_avx_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpdpwssds256((__v8si)__S, (__v8si)__A, (__v8si)__B);
}
# 151 "/usr/lib/clang/18/include/avxvnniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(128)))
_mm_dpbusd_avx_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpdpbusd128((__v4si)__S, (__v4si)__A, (__v4si)__B);
}
# 174 "/usr/lib/clang/18/include/avxvnniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(128)))
_mm_dpbusds_avx_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpdpbusds128((__v4si)__S, (__v4si)__A, (__v4si)__B);
}
# 195 "/usr/lib/clang/18/include/avxvnniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(128)))
_mm_dpwssd_avx_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpdpwssd128((__v4si)__S, (__v4si)__A, (__v4si)__B);
}
# 216 "/usr/lib/clang/18/include/avxvnniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(128)))
_mm_dpwssds_avx_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpdpwssds128((__v4si)__S, (__v4si)__A, (__v4si)__B);
}
# 150 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512dqintrin.h" 1 3
# 23 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_knot_mask8(__mmask8 __M)
{
  return __builtin_ia32_knotqi(__M);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kand_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kandqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kandn_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kandnqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kor_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_korqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kxnor_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kxnorqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kxor_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kxorqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kortestc_mask8_u8(__mmask8 __A, __mmask8 __B)
{
  return (unsigned char)__builtin_ia32_kortestcqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kortestz_mask8_u8(__mmask8 __A, __mmask8 __B)
{
  return (unsigned char)__builtin_ia32_kortestzqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kortest_mask8_u8(__mmask8 __A, __mmask8 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_kortestcqi(__A, __B);
  return (unsigned char)__builtin_ia32_kortestzqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_ktestc_mask8_u8(__mmask8 __A, __mmask8 __B)
{
  return (unsigned char)__builtin_ia32_ktestcqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_ktestz_mask8_u8(__mmask8 __A, __mmask8 __B)
{
  return (unsigned char)__builtin_ia32_ktestzqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_ktest_mask8_u8(__mmask8 __A, __mmask8 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_ktestcqi(__A, __B);
  return (unsigned char)__builtin_ia32_ktestzqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_ktestc_mask16_u8(__mmask16 __A, __mmask16 __B)
{
  return (unsigned char)__builtin_ia32_ktestchi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_ktestz_mask16_u8(__mmask16 __A, __mmask16 __B)
{
  return (unsigned char)__builtin_ia32_ktestzhi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_ktest_mask16_u8(__mmask16 __A, __mmask16 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_ktestchi(__A, __B);
  return (unsigned char)__builtin_ia32_ktestzhi(__A, __B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kadd_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kaddqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_kadd_mask16(__mmask16 __A, __mmask16 __B)
{
  return (__mmask16)__builtin_ia32_kaddhi((__mmask16)__A, (__mmask16)__B);
}







static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_cvtmask8_u32(__mmask8 __A) {
  return (unsigned int)__builtin_ia32_kmovb((__mmask8)__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_cvtu32_mask8(unsigned int __A) {
  return (__mmask8)__builtin_ia32_kmovb((__mmask8)__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_load_mask8(__mmask8 *__A) {
  return (__mmask8)__builtin_ia32_kmovb(*(__mmask8 *)__A);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,no-evex512")))
_store_mask8(__mmask8 *__A, __mmask8 __B) {
  *(__mmask8 *)__A = __builtin_ia32_kmovb((__mmask8)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mullo_epi64 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v8du) __A * (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_mullo_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_mullo_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_mullo_epi64(__mmask8 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_mullo_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_xor_pd(__m512d __A, __m512d __B) {
  return (__m512d)((__v8du)__A ^ (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_xor_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_xor_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_xor_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_xor_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_xor_ps (__m512 __A, __m512 __B) {
  return (__m512)((__v16su)__A ^ (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_xor_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_xor_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_xor_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_xor_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_or_pd(__m512d __A, __m512d __B) {
  return (__m512d)((__v8du)__A | (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_or_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_or_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_or_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_or_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_or_ps(__m512 __A, __m512 __B) {
  return (__m512)((__v16su)__A | (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_or_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_or_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_or_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_or_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_and_pd(__m512d __A, __m512d __B) {
  return (__m512d)((__v8du)__A & (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_and_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_and_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_and_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_and_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_and_ps(__m512 __A, __m512 __B) {
  return (__m512)((__v16su)__A & (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_and_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_and_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_and_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_and_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_andnot_pd(__m512d __A, __m512d __B) {
  return (__m512d)(~(__v8du)__A & (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_andnot_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_andnot_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_andnot_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_andnot_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_andnot_ps(__m512 __A, __m512 __B) {
  return (__m512)(~(__v16su)__A & (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_andnot_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_andnot_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_andnot_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_andnot_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvtpd_epi64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
                (__v8di) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_epi64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) __U,
                0x04);
}
# 361 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvtpd_epu64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_epu64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
# 400 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvtps_epi64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtps_epi64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
                (__v8di) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtps_epi64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) __U,
                0x04);
}
# 439 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvtps_epu64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtps_epu64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtps_epu64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
# 479 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvtepi64_pd (__m512i __A) {
  return (__m512d)__builtin_convertvector((__v8di)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_pd (__m512d __W, __mmask8 __U, __m512i __A) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtepi64_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_pd (__mmask8 __U, __m512i __A) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtepi64_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}
# 513 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvtepi64_ps (__m512i __A) {
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
               (__v8sf) _mm256_setzero_ps(),
               (__mmask8) -1,
               0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_ps (__m256 __W, __mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
               (__v8sf) __W,
               (__mmask8) __U,
               0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_ps (__mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
               (__v8sf) _mm256_setzero_ps(),
               (__mmask8) __U,
               0x04);
}
# 553 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvttpd_epi64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttpd_epi64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
# 592 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvttpd_epu64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
                  (__v8di) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttpd_epu64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) __U,
                  0x04);
}
# 631 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvttps_epi64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttps_epi64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttps_epi64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
# 670 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvttps_epu64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttps_epu64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
                  (__v8di) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttps_epu64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) __U,
                  0x04);
}
# 709 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvtepu64_pd (__m512i __A) {
  return (__m512d)__builtin_convertvector((__v8du)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu64_pd (__m512d __W, __mmask8 __U, __m512i __A) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtepu64_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu64_pd (__mmask8 __U, __m512i __A) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtepu64_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}
# 745 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_cvtepu64_ps (__m512i __A) {
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
                (__v8sf) _mm256_setzero_ps(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu64_ps (__m256 __W, __mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
                (__v8sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu64_ps (__mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
                (__v8sf) _mm256_setzero_ps(),
                (__mmask8) __U,
                0x04);
}
# 1055 "/usr/lib/clang/18/include/avx512dqintrin.h" 3
static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_movepi32_mask (__m512i __A)
{
  return (__mmask16) __builtin_ia32_cvtd2mask512 ((__v16si) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_movm_epi32 (__mmask16 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2d512 (__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_movm_epi64 (__mmask8 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2q512 (__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_movepi64_mask (__m512i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask512 ((__v8di) __A);
}


static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_broadcast_f32x2 (__m128 __A)
{
  return (__m512)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 0, 1, 0, 1, 0, 1,
                                         0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_f32x2 (__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                             (__v16sf)_mm512_broadcast_f32x2(__A),
                                             (__v16sf)__O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f32x2 (__mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                             (__v16sf)_mm512_broadcast_f32x2(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_broadcast_f32x8(__m256 __A)
{
  return (__m512)__builtin_shufflevector((__v8sf)__A, (__v8sf)__A,
                                         0, 1, 2, 3, 4, 5, 6, 7,
                                         0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_f32x8(__m512 __O, __mmask16 __M, __m256 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x8(__A),
                                           (__v16sf)__O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f32x8(__mmask16 __M, __m256 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x8(__A),
                                           (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_broadcast_f64x2(__m128d __A)
{
  return (__m512d)__builtin_shufflevector((__v2df)__A, (__v2df)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_f64x2(__m512d __O, __mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x2(__A),
                                            (__v8df)__O);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f64x2(__mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x2(__A),
                                            (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_broadcast_i32x2 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_i32x2 (__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_broadcast_i32x2(__A),
                                             (__v16si)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i32x2 (__mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_broadcast_i32x2(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_broadcast_i32x8(__m256i __A)
{
  return (__m512i)__builtin_shufflevector((__v8si)__A, (__v8si)__A,
                                          0, 1, 2, 3, 4, 5, 6, 7,
                                          0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_i32x8(__m512i __O, __mmask16 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x8(__A),
                                           (__v16si)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i32x8(__mmask16 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x8(__A),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_broadcast_i64x2(__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v2di)__A, (__v2di)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_mask_broadcast_i64x2(__m512i __O, __mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x2(__A),
                                            (__v8di)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq,evex512"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i64x2(__mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x2(__A),
                                            (__v8di)_mm512_setzero_si512());
}
# 155 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vlbitalgintrin.h" 1 3
# 27 "/usr/lib/clang/18/include/avx512vlbitalgintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(256)))
_mm256_popcnt_epi16(__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcntw_256((__v16hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(256)))
_mm256_mask_popcnt_epi16(__m256i __A, __mmask16 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_selectw_256((__mmask16) __U,
              (__v16hi) _mm256_popcnt_epi16(__B),
              (__v16hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_popcnt_epi16(__mmask16 __U, __m256i __B)
{
  return _mm256_mask_popcnt_epi16((__m256i) _mm256_setzero_si256(),
              __U,
              __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(128)))
_mm_popcnt_epi16(__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcntw_128((__v8hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(128)))
_mm_mask_popcnt_epi16(__m128i __A, __mmask8 __U, __m128i __B)
{
  return (__m128i) __builtin_ia32_selectw_128((__mmask8) __U,
              (__v8hi) _mm_popcnt_epi16(__B),
              (__v8hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(128)))
_mm_maskz_popcnt_epi16(__mmask8 __U, __m128i __B)
{
  return _mm_mask_popcnt_epi16((__m128i) _mm_setzero_si128(),
              __U,
              __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(256)))
_mm256_popcnt_epi8(__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcntb_256((__v32qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(256)))
_mm256_mask_popcnt_epi8(__m256i __A, __mmask32 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_selectb_256((__mmask32) __U,
              (__v32qi) _mm256_popcnt_epi8(__B),
              (__v32qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_popcnt_epi8(__mmask32 __U, __m256i __B)
{
  return _mm256_mask_popcnt_epi8((__m256i) _mm256_setzero_si256(),
              __U,
              __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(128)))
_mm_popcnt_epi8(__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcntb_128((__v16qi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(128)))
_mm_mask_popcnt_epi8(__m128i __A, __mmask16 __U, __m128i __B)
{
  return (__m128i) __builtin_ia32_selectb_128((__mmask16) __U,
              (__v16qi) _mm_popcnt_epi8(__B),
              (__v16qi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(128)))
_mm_maskz_popcnt_epi8(__mmask16 __U, __m128i __B)
{
  return _mm_mask_popcnt_epi8((__m128i) _mm_setzero_si128(),
              __U,
              __B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(256)))
_mm256_mask_bitshuffle_epi64_mask(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_vpshufbitqmb256_mask((__v32qi) __A,
              (__v32qi) __B,
              __U);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(256)))
_mm256_bitshuffle_epi64_mask(__m256i __A, __m256i __B)
{
  return _mm256_mask_bitshuffle_epi64_mask((__mmask32) -1,
              __A,
              __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(128)))
_mm_mask_bitshuffle_epi64_mask(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_vpshufbitqmb128_mask((__v16qi) __A,
              (__v16qi) __B,
              __U);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg,no-evex512"), __min_vector_width__(128)))
_mm_bitshuffle_epi64_mask(__m128i __A, __m128i __B)
{
  return _mm_mask_bitshuffle_epi64_mask((__mmask16) -1,
              __A,
              __B);
}
# 160 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vlbwintrin.h" 1 3
# 309 "/usr/lib/clang/18/include/avx512vlbwintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_add_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B){
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_add_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_add_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_add_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_add_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_add_epi16(__A, __B),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_add_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_add_epi16(__A, __B),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sub_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_sub_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sub_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_sub_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sub_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_sub_epi16(__A, __B),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sub_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_sub_epi16(__A, __B),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_add_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_add_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_add_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_add_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_add_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_add_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_add_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_add_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_sub_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_sub_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sub_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_sub_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_sub_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sub_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sub_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sub_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mullo_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_mullo_epi16(__A, __B),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mullo_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_mullo_epi16(__A, __B),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_mullo_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mullo_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mullo_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mullo_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_blend_epi8 (__mmask16 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_selectb_128 ((__mmask16) __U,
              (__v16qi) __W,
              (__v16qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_blend_epi8 (__mmask32 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_selectb_256 ((__mmask32) __U,
               (__v32qi) __W,
               (__v32qi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_blend_epi16 (__mmask8 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_selectw_128 ((__mmask8) __U,
               (__v8hi) __W,
               (__v8hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_blend_epi16 (__mmask16 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_selectw_256 ((__mmask16) __U,
               (__v16hi) __W,
               (__v16hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_abs_epi8(__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_abs_epi8(__A),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_abs_epi8(__mmask16 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_abs_epi8(__A),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_abs_epi8(__m256i __W, __mmask32 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_abs_epi8(__A),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_abs_epi8 (__mmask32 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_abs_epi8(__A),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_abs_epi16(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_abs_epi16(__A),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_abs_epi16(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_abs_epi16(__A),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_abs_epi16(__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_abs_epi16(__A),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_abs_epi16(__mmask16 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_abs_epi16(__A),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_packs_epi32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packs_epi32(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_packs_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packs_epi32(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_packs_epi32(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                          (__v16hi)_mm256_packs_epi32(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_packs_epi32(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                          (__v16hi)_mm256_packs_epi32(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_packs_epi16(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_packs_epi16(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_packs_epi16(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_packs_epi16(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_packs_epi16(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                          (__v32qi)_mm256_packs_epi16(__A, __B),
                                          (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_packs_epi16(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                          (__v32qi)_mm256_packs_epi16(__A, __B),
                                          (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_packus_epi32(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packus_epi32(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_packus_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packus_epi32(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_packus_epi32(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                         (__v16hi)_mm256_packus_epi32(__A, __B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_packus_epi32(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                         (__v16hi)_mm256_packus_epi32(__A, __B),
                                         (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_packus_epi16(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                            (__v16qi)_mm_packus_epi16(__A, __B),
                                            (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_packus_epi16(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                            (__v16qi)_mm_packus_epi16(__A, __B),
                                            (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_packus_epi16(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                         (__v32qi)_mm256_packus_epi16(__A, __B),
                                         (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_packus_epi16(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                         (__v32qi)_mm256_packus_epi16(__A, __B),
                                         (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_adds_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_adds_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_adds_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epi8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_adds_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epi8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_adds_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_adds_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_adds_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_adds_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_adds_epu8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_adds_epu8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_adds_epu8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epu8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_adds_epu8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epu8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_adds_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_adds_epu16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_adds_epu16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epu16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_adds_epu16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epu16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_avg_epu8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_avg_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_avg_epu8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_avg_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_avg_epu8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_avg_epu8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_avg_epu8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_avg_epu8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_avg_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_avg_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_avg_epu16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_avg_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_avg_epu16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                            (__v16hi)_mm256_avg_epu16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_avg_epu16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                            (__v16hi)_mm256_avg_epu16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_epi8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_epi8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_epi8(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_epi8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_epi16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_epi16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_epi16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epi16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_epi16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epi16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_epu8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_epu8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_epu8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epu8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_epu8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epu8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_max_epu16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_max_epu16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_epu16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epu16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_epu16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epu16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_epi8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_epi8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_epi8(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_epi8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_epi16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_epi16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_epi16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epi16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_epi16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epi16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_epu8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_epu8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_epu8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epu8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_epu8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epu8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_min_epu16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_min_epu16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_epu16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epu16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_epu16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epu16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_shuffle_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                            (__v16qi)_mm_shuffle_epi8(__A, __B),
                                            (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_shuffle_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                            (__v16qi)_mm_shuffle_epi8(__A, __B),
                                            (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_shuffle_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                         (__v32qi)_mm256_shuffle_epi8(__A, __B),
                                         (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_shuffle_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                         (__v32qi)_mm256_shuffle_epi8(__A, __B),
                                         (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_subs_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_subs_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_subs_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epi8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_subs_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epi8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_subs_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_subs_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_subs_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_subs_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_subs_epu8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_subs_epu8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_subs_epu8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epu8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_subs_epu8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epu8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_subs_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_subs_epu16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_subs_epu16(__m256i __W, __mmask16 __U, __m256i __A,
      __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epu16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_subs_epu16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epu16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_permutex2var_epi16(__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpermi2varhi128((__v8hi)__A, (__v8hi)__I,
                                                 (__v8hi) __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_permutex2var_epi16(__m128i __A, __mmask8 __U, __m128i __I,
                            __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                  (__v8hi)_mm_permutex2var_epi16(__A, __I, __B),
                                  (__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask2_permutex2var_epi16(__m128i __A, __m128i __I, __mmask8 __U,
                             __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                  (__v8hi)_mm_permutex2var_epi16(__A, __I, __B),
                                  (__v8hi)__I);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_permutex2var_epi16 (__mmask8 __U, __m128i __A, __m128i __I,
            __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                  (__v8hi)_mm_permutex2var_epi16(__A, __I, __B),
                                  (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_permutex2var_epi16(__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpermi2varhi256((__v16hi)__A, (__v16hi)__I,
                                                 (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutex2var_epi16(__m256i __A, __mmask16 __U, __m256i __I,
                               __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                              (__v16hi)_mm256_permutex2var_epi16(__A, __I, __B),
                              (__v16hi)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask2_permutex2var_epi16(__m256i __A, __m256i __I, __mmask16 __U,
                                __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                              (__v16hi)_mm256_permutex2var_epi16(__A, __I, __B),
                              (__v16hi)__I);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutex2var_epi16 (__mmask16 __U, __m256i __A, __m256i __I,
                                 __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                              (__v16hi)_mm256_permutex2var_epi16(__A, __I, __B),
                              (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_maddubs_epi16(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                            (__v8hi)_mm_maddubs_epi16(__X, __Y),
                                            (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_maddubs_epi16(__mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                            (__v8hi)_mm_maddubs_epi16(__X, __Y),
                                            (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_maddubs_epi16(__m256i __W, __mmask16 __U, __m256i __X,
                          __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                        (__v16hi)_mm256_maddubs_epi16(__X, __Y),
                                        (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_maddubs_epi16(__mmask16 __U, __m256i __X, __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                        (__v16hi)_mm256_maddubs_epi16(__X, __Y),
                                        (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_madd_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_madd_epi16(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_madd_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_madd_epi16(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_madd_epi16(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_madd_epi16(__A, __B),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_madd_epi16(__mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_madd_epi16(__A, __B),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_cvtsepi16_epi8 (__m128i __A) {
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
               (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtsepi16_epi8 (__mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_cvtsepi16_epi8 (__m256i __A) {
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
               (__v16qi) _mm_setzero_si128(),
               (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
               (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi16_epi8 (__mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_cvtusepi16_epi8 (__m128i __A) {
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
                (__v16qi) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtusepi16_epi8 (__mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
                (__v16qi) _mm_setzero_si128(),
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_cvtusepi16_epi8 (__m256i __A) {
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
                (__v16qi) _mm_setzero_si128(),
                (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi16_epi8 (__mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
                (__v16qi) _mm_setzero_si128(),
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_cvtepi16_epi8 (__m128i __A) {
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v8hi)__A, __v8qi),
      (__v8qi){0, 0, 0, 0, 0, 0, 0, 0}, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
      12, 13, 14, 15);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
               (__v16qi) __O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi16_epi8 (__mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi16_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovwb128mem_mask ((__v16qi *) __P, (__v8hi) __A, __M);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovswb128mem_mask ((__v16qi *) __P, (__v8hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovuswb128mem_mask ((__v16qi *) __P, (__v8hi) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi16_epi8 (__m256i __A) {
  return (__m128i)__builtin_convertvector((__v16hi) __A, __v16qi);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm256_cvtepi16_epi8(__A),
                                             (__v16qi)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi16_epi8 (__mmask16 __M, __m256i __A) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm256_cvtepi16_epi8(__A),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi16_storeu_epi8 (void * __P, __mmask16 __M, __m256i __A)
{
  __builtin_ia32_pmovwb256mem_mask ((__v16qi *) __P, (__v16hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask16 __M, __m256i __A)
{
  __builtin_ia32_pmovswb256mem_mask ((__v16qi *) __P, (__v16hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask16 __M, __m256i __A)
{
  __builtin_ia32_pmovuswb256mem_mask ((__v16qi*) __P, (__v16hi) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_mulhrs_epi16(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhrs_epi16(__X, __Y),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mulhrs_epi16(__mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhrs_epi16(__X, __Y),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mulhrs_epi16(__m256i __W, __mmask16 __U, __m256i __X, __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_mulhrs_epi16(__X, __Y),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mulhrs_epi16(__mmask16 __U, __m256i __X, __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_mulhrs_epi16(__X, __Y),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_mulhi_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mulhi_epu16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mulhi_epu16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epu16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mulhi_epu16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epu16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_mulhi_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mulhi_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mulhi_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mulhi_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpackhi_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpackhi_epi8(__A, __B),
                                           (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpackhi_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpackhi_epi8(__A, __B),
                                           (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpackhi_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpackhi_epi8(__A, __B),
                                        (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpackhi_epi8(__A, __B),
                                        (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpackhi_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpackhi_epi16(__A, __B),
                                           (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpackhi_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpackhi_epi16(__A, __B),
                                           (__v8hi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpackhi_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpackhi_epi16(__A, __B),
                                       (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpackhi_epi16(__A, __B),
                                       (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpacklo_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpacklo_epi8(__A, __B),
                                           (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpacklo_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpacklo_epi8(__A, __B),
                                           (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpacklo_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpacklo_epi8(__A, __B),
                                        (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpacklo_epi8(__A, __B),
                                        (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_unpacklo_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpacklo_epi16(__A, __B),
                                           (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_unpacklo_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpacklo_epi16(__A, __B),
                                           (__v8hi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_unpacklo_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpacklo_epi16(__A, __B),
                                       (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpacklo_epi16(__A, __B),
                                       (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi8_epi16(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepi8_epi16(__A),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi8_epi16(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepi8_epi16(__A),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi8_epi16(__m256i __W, __mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepi8_epi16(__A),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi8_epi16(__mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepi8_epi16(__A),
                                             (__v16hi)_mm256_setzero_si256());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepu8_epi16(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepu8_epi16(__A),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepu8_epi16(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepu8_epi16(__A),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepu8_epi16(__m256i __W, __mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepu8_epi16(__A),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepu8_epi16 (__mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepu8_epi16(__A),
                                             (__v16hi)_mm256_setzero_si256());
}
# 1871 "/usr/lib/clang/18/include/avx512vlbwintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_sllv_epi16(__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psllv16hi((__v16hi)__A, (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sllv_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_sllv_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sllv_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_sllv_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_sllv_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psllv8hi((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_sllv_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sllv_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sllv_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sllv_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_sll_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sll_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sll_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sll_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sll_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sll_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sll_epi16(__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sll_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_slli_epi16(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_slli_epi16(__A, (int)__B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_slli_epi16 (__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_slli_epi16(__A, (int)__B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_slli_epi16(__m256i __W, __mmask16 __U, __m256i __A,
                       unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_slli_epi16(__A, (int)__B),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_slli_epi16(__mmask16 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_slli_epi16(__A, (int)__B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_srlv_epi16(__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psrlv16hi((__v16hi)__A, (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srlv_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srlv_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srlv_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srlv_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_srlv_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrlv8hi((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_srlv_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srlv_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srlv_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srlv_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_srav_epi16(__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psrav16hi((__v16hi)__A, (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srav_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srav_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srav_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srav_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_srav_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrav8hi((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_srav_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srav_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srav_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srav_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_sra_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sra_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_sra_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sra_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sra_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sra_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sra_epi16(__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sra_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_srai_epi16(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srai_epi16(__A, (int)__B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srai_epi16(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srai_epi16(__A, (int)__B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srai_epi16(__m256i __W, __mmask16 __U, __m256i __A,
                       unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srai_epi16(__A, (int)__B),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srai_epi16(__mmask16 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srai_epi16(__A, (int)__B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_srl_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srl_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srl_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srl_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srl_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_srl_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srl_epi16(__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_srl_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_srli_epi16(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srli_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_srli_epi16 (__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srli_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_srli_epi16(__m256i __W, __mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srli_epi16(__A, __B),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_srli_epi16(__mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srli_epi16(__A, __B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_mov_epi16 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectw_128 ((__mmask8) __U,
                (__v8hi) __A,
                (__v8hi) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mov_epi16 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectw_128 ((__mmask8) __U,
                (__v8hi) __A,
                (__v8hi) _mm_setzero_si128 ());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mov_epi16 (__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectw_256 ((__mmask16) __U,
                (__v16hi) __A,
                (__v16hi) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mov_epi16 (__mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectw_256 ((__mmask16) __U,
                (__v16hi) __A,
                (__v16hi) _mm256_setzero_si256 ());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_mov_epi8 (__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectb_128 ((__mmask16) __U,
                (__v16qi) __A,
                (__v16qi) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mov_epi8 (__mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectb_128 ((__mmask16) __U,
                (__v16qi) __A,
                (__v16qi) _mm_setzero_si128 ());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mov_epi8 (__m256i __W, __mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectb_256 ((__mmask32) __U,
                (__v32qi) __A,
                (__v32qi) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mov_epi8 (__mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectb_256 ((__mmask32) __U,
                (__v32qi) __A,
                (__v32qi) _mm256_setzero_si256 ());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_set1_epi8 (__m128i __O, __mmask16 __M, char __A)
{
  return (__m128i) __builtin_ia32_selectb_128(__M,
                                              (__v16qi) _mm_set1_epi8(__A),
                                              (__v16qi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_set1_epi8 (__mmask16 __M, char __A)
{
 return (__m128i) __builtin_ia32_selectb_128(__M,
                                             (__v16qi) _mm_set1_epi8(__A),
                                             (__v16qi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_set1_epi8 (__m256i __O, __mmask32 __M, char __A)
{
  return (__m256i) __builtin_ia32_selectb_256(__M,
                                              (__v32qi) _mm256_set1_epi8(__A),
                                              (__v32qi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_set1_epi8 (__mmask32 __M, char __A)
{
  return (__m256i) __builtin_ia32_selectb_256(__M,
                                              (__v32qi) _mm256_set1_epi8(__A),
                                              (__v32qi) _mm256_setzero_si256());
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_loadu_epi16 (void const *__P)
{
  struct __loadu_epi16 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi16*)__P)->__v;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_loadu_epi16 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquhi128_mask ((const __v8hi *) __P,
                 (__v8hi) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_loadu_epi16 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquhi128_mask ((const __v8hi *) __P,
                 (__v8hi)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_loadu_epi16 (void const *__P)
{
  struct __loadu_epi16 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi16*)__P)->__v;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_loadu_epi16 (__m256i __W, __mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquhi256_mask ((const __v16hi *) __P,
                 (__v16hi) __W,
                 (__mmask16) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_loadu_epi16 (__mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquhi256_mask ((const __v16hi *) __P,
                 (__v16hi)
                 _mm256_setzero_si256 (),
                 (__mmask16) __U);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_loadu_epi8 (void const *__P)
{
  struct __loadu_epi8 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi8*)__P)->__v;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquqi128_mask ((const __v16qi *) __P,
                 (__v16qi) __W,
                 (__mmask16) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquqi128_mask ((const __v16qi *) __P,
                 (__v16qi)
                 _mm_setzero_si128 (),
                 (__mmask16) __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_loadu_epi8 (void const *__P)
{
  struct __loadu_epi8 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi8*)__P)->__v;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquqi256_mask ((const __v32qi *) __P,
                 (__v32qi) __W,
                 (__mmask32) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquqi256_mask ((const __v32qi *) __P,
                 (__v32qi)
                 _mm256_setzero_si256 (),
                 (__mmask32) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_storeu_epi16 (void *__P, __m128i __A)
{
  struct __storeu_epi16 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi16*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_storeu_epi16 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedquhi128_mask ((__v8hi *) __P,
             (__v8hi) __A,
             (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_storeu_epi16 (void *__P, __m256i __A)
{
  struct __storeu_epi16 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi16*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_storeu_epi16 (void *__P, __mmask16 __U, __m256i __A)
{
  __builtin_ia32_storedquhi256_mask ((__v16hi *) __P,
             (__v16hi) __A,
             (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_storeu_epi8 (void *__P, __m128i __A)
{
  struct __storeu_epi8 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi8*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_storeu_epi8 (void *__P, __mmask16 __U, __m128i __A)
{
  __builtin_ia32_storedquqi128_mask ((__v16qi *) __P,
             (__v16qi) __A,
             (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_storeu_epi8 (void *__P, __m256i __A)
{
  struct __storeu_epi8 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi8*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_storeu_epi8 (void *__P, __mmask32 __U, __m256i __A)
{
  __builtin_ia32_storedquqi256_mask ((__v32qi *) __P,
             (__v32qi) __A,
             (__mmask32) __U);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_test_epi8_mask (__m128i __A, __m128i __B)
{
  return ((__mmask16)__builtin_ia32_cmpb128_mask((__v16qi)(__m128i)((_mm_and_si128(__A, __B))), (__v16qi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask16)-1));
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_test_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return ((__mmask16)__builtin_ia32_cmpb128_mask((__v16qi)(__m128i)((_mm_and_si128 (__A, __B))), (__v16qi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask16)((__U))));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_test_epi8_mask (__m256i __A, __m256i __B)
{
  return ((__mmask32)__builtin_ia32_cmpb256_mask((__v32qi)(__m256i)((_mm256_and_si256(__A, __B))), (__v32qi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask32)-1));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_test_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return ((__mmask32)__builtin_ia32_cmpb256_mask((__v32qi)(__m256i)((_mm256_and_si256(__A, __B))), (__v32qi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask32)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_test_epi16_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpw128_mask((__v8hi)(__m128i)((_mm_and_si128 (__A, __B))), (__v8hi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_test_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpw128_mask((__v8hi)(__m128i)((_mm_and_si128 (__A, __B))), (__v8hi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_test_epi16_mask (__m256i __A, __m256i __B)
{
  return ((__mmask16)__builtin_ia32_cmpw256_mask((__v16hi)(__m256i)((_mm256_and_si256 (__A, __B))), (__v16hi)(__m256i)((_mm256_setzero_si256 ())), (int)(_MM_CMPINT_NE), (__mmask16)-1));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_test_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return ((__mmask16)__builtin_ia32_cmpw256_mask((__v16hi)(__m256i)((_mm256_and_si256(__A, __B))), (__v16hi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask16)((__U))));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_testn_epi8_mask (__m128i __A, __m128i __B)
{
  return ((__mmask16)__builtin_ia32_cmpb128_mask((__v16qi)(__m128i)((_mm_and_si128 (__A, __B))), (__v16qi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask16)-1));
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_testn_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return ((__mmask16)__builtin_ia32_cmpb128_mask((__v16qi)(__m128i)((_mm_and_si128 (__A, __B))), (__v16qi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask16)((__U))));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_testn_epi8_mask (__m256i __A, __m256i __B)
{
  return ((__mmask32)__builtin_ia32_cmpb256_mask((__v32qi)(__m256i)((_mm256_and_si256 (__A, __B))), (__v32qi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask32)-1));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_testn_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return ((__mmask32)__builtin_ia32_cmpb256_mask((__v32qi)(__m256i)((_mm256_and_si256 (__A, __B))), (__v32qi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask32)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_testn_epi16_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpw128_mask((__v8hi)(__m128i)((_mm_and_si128 (__A, __B))), (__v8hi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_testn_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpw128_mask((__v8hi)(__m128i)((_mm_and_si128(__A, __B))), (__v8hi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_testn_epi16_mask (__m256i __A, __m256i __B)
{
  return ((__mmask16)__builtin_ia32_cmpw256_mask((__v16hi)(__m256i)((_mm256_and_si256(__A, __B))), (__v16hi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask16)-1));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_testn_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return ((__mmask16)__builtin_ia32_cmpw256_mask((__v16hi)(__m256i)((_mm256_and_si256 (__A, __B))), (__v16hi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask16)((__U))));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_movepi8_mask (__m128i __A)
{
  return (__mmask16) __builtin_ia32_cvtb2mask128 ((__v16qi) __A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_movepi8_mask (__m256i __A)
{
  return (__mmask32) __builtin_ia32_cvtb2mask256 ((__v32qi) __A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_movepi16_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtw2mask128 ((__v8hi) __A);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_movepi16_mask (__m256i __A)
{
  return (__mmask16) __builtin_ia32_cvtw2mask256 ((__v16hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_movm_epi8 (__mmask16 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2b128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_movm_epi8 (__mmask32 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2b256 (__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_movm_epi16 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2w128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_movm_epi16 (__mmask16 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2w256 (__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_broadcastb_epi8 (__m128i __O, __mmask16 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128(__M,
                                             (__v16qi) _mm_broadcastb_epi8(__A),
                                             (__v16qi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_broadcastb_epi8 (__mmask16 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128(__M,
                                             (__v16qi) _mm_broadcastb_epi8(__A),
                                             (__v16qi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcastb_epi8 (__m256i __O, __mmask32 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectb_256(__M,
                                             (__v32qi) _mm256_broadcastb_epi8(__A),
                                             (__v32qi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcastb_epi8 (__mmask32 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectb_256(__M,
                                             (__v32qi) _mm256_broadcastb_epi8(__A),
                                             (__v32qi) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_broadcastw_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128(__M,
                                             (__v8hi) _mm_broadcastw_epi16(__A),
                                             (__v8hi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_broadcastw_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128(__M,
                                             (__v8hi) _mm_broadcastw_epi16(__A),
                                             (__v8hi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcastw_epi16 (__m256i __O, __mmask16 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256(__M,
                                             (__v16hi) _mm256_broadcastw_epi16(__A),
                                             (__v16hi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcastw_epi16 (__mmask16 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256(__M,
                                             (__v16hi) _mm256_broadcastw_epi16(__A),
                                             (__v16hi) _mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_set1_epi16 (__m256i __O, __mmask16 __M, short __A)
{
  return (__m256i) __builtin_ia32_selectw_256 (__M,
                                               (__v16hi) _mm256_set1_epi16(__A),
                                               (__v16hi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_set1_epi16 (__mmask16 __M, short __A)
{
  return (__m256i) __builtin_ia32_selectw_256(__M,
                                              (__v16hi)_mm256_set1_epi16(__A),
                                              (__v16hi) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_set1_epi16 (__m128i __O, __mmask8 __M, short __A)
{
  return (__m128i) __builtin_ia32_selectw_128(__M,
                                              (__v8hi) _mm_set1_epi16(__A),
                                              (__v8hi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_set1_epi16 (__mmask8 __M, short __A)
{
  return (__m128i) __builtin_ia32_selectw_128(__M,
                                              (__v8hi) _mm_set1_epi16(__A),
                                              (__v8hi) _mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_permutexvar_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_permvarhi128((__v8hi) __B, (__v8hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_maskz_permutexvar_epi16 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                        (__v8hi)_mm_permutexvar_epi16(__A, __B),
                                        (__v8hi) _mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_permutexvar_epi16 (__m128i __W, __mmask8 __M, __m128i __A,
          __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                        (__v8hi)_mm_permutexvar_epi16(__A, __B),
                                        (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_permutexvar_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_permvarhi256((__v16hi) __B, (__v16hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_epi16 (__mmask16 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                    (__v16hi)_mm256_permutexvar_epi16(__A, __B),
                                    (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutexvar_epi16 (__m256i __W, __mmask16 __M, __m256i __A,
             __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                    (__v16hi)_mm256_permutexvar_epi16(__A, __B),
                                    (__v16hi)__W);
}
# 2812 "/usr/lib/clang/18/include/avx512vlbwintrin.h" 3
static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_add_epi16(__m128i __W) {
  return __builtin_reduce_add((__v8hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_mul_epi16(__m128i __W) {
  return __builtin_reduce_mul((__v8hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_and_epi16(__m128i __W) {
  return __builtin_reduce_and((__v8hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_or_epi16(__m128i __W) {
  return __builtin_reduce_or((__v8hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_add_epi16( __mmask8 __M, __m128i __W) {
  __W = _mm_maskz_mov_epi16(__M, __W);
  return __builtin_reduce_add((__v8hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_mul_epi16( __mmask8 __M, __m128i __W) {
  __W = _mm_mask_mov_epi16(_mm_set1_epi16(1), __M, __W);
  return __builtin_reduce_mul((__v8hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_and_epi16( __mmask8 __M, __m128i __W) {
  __W = _mm_mask_mov_epi16(_mm_set1_epi16(-1), __M, __W);
  return __builtin_reduce_and((__v8hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_or_epi16(__mmask8 __M, __m128i __W) {
  __W = _mm_maskz_mov_epi16(__M, __W);
  return __builtin_reduce_or((__v8hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_max_epi16(__m128i __V) {
  return __builtin_reduce_max((__v8hi)__V);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_max_epu16(__m128i __V) {
  return __builtin_reduce_max((__v8hu)__V);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_min_epi16(__m128i __V) {
  return __builtin_reduce_min((__v8hi)__V);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_min_epu16(__m128i __V) {
  return __builtin_reduce_min((__v8hu)__V);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_max_epi16(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi16(_mm_set1_epi16(-32767-1), __M, __V);
  return __builtin_reduce_max((__v8hi)__V);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_max_epu16(__mmask16 __M, __m128i __V) {
  __V = _mm_maskz_mov_epi16(__M, __V);
  return __builtin_reduce_max((__v8hu)__V);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_min_epi16(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi16(_mm_set1_epi16(32767), __M, __V);
  return __builtin_reduce_min((__v8hi)__V);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_min_epu16(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi16(_mm_set1_epi16(-1), __M, __V);
  return __builtin_reduce_min((__v8hu)__V);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_add_epi16(__m256i __W) {
  return __builtin_reduce_add((__v16hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_mul_epi16(__m256i __W) {
  return __builtin_reduce_mul((__v16hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_and_epi16(__m256i __W) {
  return __builtin_reduce_and((__v16hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_or_epi16(__m256i __W) {
  return __builtin_reduce_or((__v16hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_add_epi16( __mmask16 __M, __m256i __W) {
  __W = _mm256_maskz_mov_epi16(__M, __W);
  return __builtin_reduce_add((__v16hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_mul_epi16( __mmask16 __M, __m256i __W) {
  __W = _mm256_mask_mov_epi16(_mm256_set1_epi16(1), __M, __W);
  return __builtin_reduce_mul((__v16hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_and_epi16( __mmask16 __M, __m256i __W) {
  __W = _mm256_mask_mov_epi16(_mm256_set1_epi16(-1), __M, __W);
  return __builtin_reduce_and((__v16hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_or_epi16(__mmask16 __M, __m256i __W) {
  __W = _mm256_maskz_mov_epi16(__M, __W);
  return __builtin_reduce_or((__v16hi)__W);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_max_epi16(__m256i __V) {
  return __builtin_reduce_max((__v16hi)__V);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_max_epu16(__m256i __V) {
  return __builtin_reduce_max((__v16hu)__V);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_min_epi16(__m256i __V) {
  return __builtin_reduce_min((__v16hi)__V);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_min_epu16(__m256i __V) {
  return __builtin_reduce_min((__v16hu)__V);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_max_epi16(__mmask16 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi16(_mm256_set1_epi16(-32767-1), __M, __V);
  return __builtin_reduce_max((__v16hi)__V);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_max_epu16(__mmask16 __M, __m256i __V) {
  __V = _mm256_maskz_mov_epi16(__M, __V);
  return __builtin_reduce_max((__v16hu)__V);
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_min_epi16(__mmask16 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi16(_mm256_set1_epi16(32767), __M, __V);
  return __builtin_reduce_min((__v16hi)__V);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_min_epu16(__mmask16 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi16(_mm256_set1_epi16(-1), __M, __V);
  return __builtin_reduce_min((__v16hu)__V);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_add_epi8(__m128i __W) {
  return __builtin_reduce_add((__v16qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_mul_epi8(__m128i __W) {
  return __builtin_reduce_mul((__v16qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_and_epi8(__m128i __W) {
  return __builtin_reduce_and((__v16qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_or_epi8(__m128i __W) {
  return __builtin_reduce_or((__v16qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_add_epi8(__mmask16 __M, __m128i __W) {
  __W = _mm_maskz_mov_epi8(__M, __W);
  return __builtin_reduce_add((__v16qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_mul_epi8(__mmask16 __M, __m128i __W) {
  __W = _mm_mask_mov_epi8(_mm_set1_epi8(1), __M, __W);
  return __builtin_reduce_mul((__v16qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_and_epi8(__mmask16 __M, __m128i __W) {
  __W = _mm_mask_mov_epi8(_mm_set1_epi8(-1), __M, __W);
  return __builtin_reduce_and((__v16qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_or_epi8(__mmask16 __M, __m128i __W) {
  __W = _mm_maskz_mov_epi8(__M, __W);
  return __builtin_reduce_or((__v16qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_max_epi8(__m128i __V) {
  return __builtin_reduce_max((__v16qs)__V);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_max_epu8(__m128i __V) {
  return __builtin_reduce_max((__v16qu)__V);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_min_epi8(__m128i __V) {
  return __builtin_reduce_min((__v16qs)__V);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_reduce_min_epu8(__m128i __V) {
  return __builtin_reduce_min((__v16qu)__V);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_max_epi8(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi8(_mm_set1_epi8(-127-1), __M, __V);
  return __builtin_reduce_max((__v16qs)__V);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_max_epu8(__mmask16 __M, __m128i __V) {
  __V = _mm_maskz_mov_epi8(__M, __V);
  return __builtin_reduce_max((__v16qu)__V);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_min_epi8(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi8(_mm_set1_epi8(127), __M, __V);
  return __builtin_reduce_min((__v16qs)__V);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(128)))
_mm_mask_reduce_min_epu8(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi8(_mm_set1_epi8(-1), __M, __V);
  return __builtin_reduce_min((__v16qu)__V);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_add_epi8(__m256i __W) {
  return __builtin_reduce_add((__v32qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_mul_epi8(__m256i __W) {
  return __builtin_reduce_mul((__v32qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_and_epi8(__m256i __W) {
  return __builtin_reduce_and((__v32qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_or_epi8(__m256i __W) {
  return __builtin_reduce_or((__v32qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_add_epi8(__mmask32 __M, __m256i __W) {
  __W = _mm256_maskz_mov_epi8(__M, __W);
  return __builtin_reduce_add((__v32qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_mul_epi8(__mmask32 __M, __m256i __W) {
  __W = _mm256_mask_mov_epi8(_mm256_set1_epi8(1), __M, __W);
  return __builtin_reduce_mul((__v32qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_and_epi8(__mmask32 __M, __m256i __W) {
  __W = _mm256_mask_mov_epi8(_mm256_set1_epi8(-1), __M, __W);
  return __builtin_reduce_and((__v32qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_or_epi8(__mmask32 __M, __m256i __W) {
  __W = _mm256_maskz_mov_epi8(__M, __W);
  return __builtin_reduce_or((__v32qs)__W);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_max_epi8(__m256i __V) {
  return __builtin_reduce_max((__v32qs)__V);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_max_epu8(__m256i __V) {
  return __builtin_reduce_max((__v32qu)__V);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_min_epi8(__m256i __V) {
  return __builtin_reduce_min((__v32qs)__V);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_min_epu8(__m256i __V) {
  return __builtin_reduce_min((__v32qu)__V);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_max_epi8(__mmask32 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi8(_mm256_set1_epi8(-127-1), __M, __V);
  return __builtin_reduce_max((__v32qs)__V);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_max_epu8(__mmask32 __M, __m256i __V) {
  __V = _mm256_maskz_mov_epi8(__M, __V);
  return __builtin_reduce_max((__v32qu)__V);
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_min_epi8(__mmask32 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi8(_mm256_set1_epi8(127), __M, __V);
  return __builtin_reduce_min((__v32qs)__V);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw,no-evex512"), __min_vector_width__(256)))
_mm256_mask_reduce_min_epu8(__mmask32 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi8(_mm256_set1_epi8(-1), __M, __V);
  return __builtin_reduce_min((__v32qu)__V);
}
# 165 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vlcdintrin.h" 1 3
# 26 "/usr/lib/clang/18/include/avx512vlcdintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m128i) _mm_set1_epi64x((long long) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m256i) _mm256_set1_epi64x((long long)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m128i) _mm_set1_epi32((int)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m256i) _mm256_set1_epi32((int)__A);
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_conflict_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictdi_128 ((__v2di) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_mask_conflict_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_conflict_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_maskz_conflict_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_conflict_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_conflict_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictdi_256 ((__v4di) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_mask_conflict_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_conflict_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_conflict_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_conflict_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_conflict_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictsi_128 ((__v4si) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_mask_conflict_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_conflict_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_maskz_conflict_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_conflict_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_conflict_epi32 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictsi_256 ((__v8si) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_mask_conflict_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_conflict_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_conflict_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_conflict_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_lzcnt_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntd_128 ((__v4si) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_mask_lzcnt_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_lzcnt_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_maskz_lzcnt_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_lzcnt_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_lzcnt_epi32 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntd_256 ((__v8si) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_mask_lzcnt_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_lzcnt_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_lzcnt_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_lzcnt_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_lzcnt_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntq_128 ((__v2di) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_mask_lzcnt_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_lzcnt_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(128)))
_mm_maskz_lzcnt_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_lzcnt_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_lzcnt_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntq_256 ((__v4di) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_mask_lzcnt_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_lzcnt_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_lzcnt_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_lzcnt_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}
# 170 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vldqintrin.h" 1 3
# 27 "/usr/lib/clang/18/include/avx512vldqintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mullo_epi64 (__m256i __A, __m256i __B) {
  return (__m256i) ((__v4du) __A * (__v4du) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mullo_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_mullo_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mullo_epi64(__mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_mullo_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mullo_epi64 (__m128i __A, __m128i __B) {
  return (__m128i) ((__v2du) __A * (__v2du) __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_mullo_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_mullo_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_mullo_epi64(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_mullo_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_andnot_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_andnot_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_andnot_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_andnot_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_andnot_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_andnot_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_andnot_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_andnot_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_andnot_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_andnot_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_andnot_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_andnot_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_andnot_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_andnot_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_andnot_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_andnot_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_and_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_and_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_and_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_and_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_and_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_and_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_and_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_and_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_and_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_and_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_and_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_and_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_and_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_and_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_and_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_and_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_xor_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_xor_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_xor_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_xor_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_xor_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_xor_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_xor_pd (__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_xor_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_xor_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_xor_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_xor_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_xor_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_xor_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_xor_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_xor_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_xor_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_or_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_or_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_or_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_or_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_or_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_or_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_or_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_or_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_or_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_or_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_or_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_or_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_or_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_or_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_or_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_or_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvtpd_epi64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtpd_epi64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtpd_epi64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvtpd_epi64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtpd_epi64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_epi64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvtpd_epu64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtpd_epu64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtpd_epu64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvtpd_epu64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtpd_epu64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_epu64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvtps_epi64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtps_epi64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvtps_epi64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtps_epi64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvtps_epu64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtps_epu64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvtps_epu64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtps_epu64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvtepi64_pd (__m128i __A) {
  return (__m128d)__builtin_convertvector((__v2di)__A, __v2df);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi64_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtepi64_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtepi64_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi64_pd (__m256i __A) {
  return (__m256d)__builtin_convertvector((__v4di)__A, __v4df);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_pd (__m256d __W, __mmask8 __U, __m256i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtepi64_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_pd (__mmask8 __U, __m256i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtepi64_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvtepi64_ps (__m128i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi64_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_ps (__mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi64_ps (__m256i __A) {
  return (__m128)__builtin_convertvector((__v4di)__A, __v4sf);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_ps (__m128 __W, __mmask8 __U, __m256i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtepi64_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_ps (__mmask8 __U, __m256i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtepi64_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvttpd_epi64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttpd_epi64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttpd_epi64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvttpd_epi64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttpd_epi64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttpd_epi64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvttpd_epu64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttpd_epu64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttpd_epu64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvttpd_epu64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttpd_epu64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttpd_epu64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvttps_epi64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttps_epi64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvttps_epi64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttps_epi64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvttps_epu64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttps_epu64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvttps_epu64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttps_epu64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvtepu64_pd (__m128i __A) {
  return (__m128d)__builtin_convertvector((__v2du)__A, __v2df);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepu64_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtepu64_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepu64_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtepu64_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu64_pd (__m256i __A) {
  return (__m256d)__builtin_convertvector((__v4du)__A, __v4df);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepu64_pd (__m256d __W, __mmask8 __U, __m256i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtepu64_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepu64_pd (__mmask8 __U, __m256i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtepu64_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_cvtepu64_ps (__m128i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepu64_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepu64_ps (__mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu64_ps (__m256i __A) {
  return (__m128)__builtin_convertvector((__v4du)__A, __v4sf);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepu64_ps (__m128 __W, __mmask8 __U, __m256i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtepu64_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepu64_ps (__mmask8 __U, __m256i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtepu64_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}
# 911 "/usr/lib/clang/18/include/avx512vldqintrin.h" 3
static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_movepi32_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtd2mask128 ((__v4si) __A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_movepi32_mask (__m256i __A)
{
  return (__mmask8) __builtin_ia32_cvtd2mask256 ((__v8si) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_movm_epi32 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2d128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_movm_epi32 (__mmask8 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2d256 (__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_movm_epi64 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2q128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_movm_epi64 (__mmask8 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2q256 (__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_movepi64_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask128 ((__v2di) __A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_movepi64_mask (__m256i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask256 ((__v4di) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_f32x2 (__m128 __A)
{
  return (__m256)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcast_f32x2 (__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                             (__v8sf)_mm256_broadcast_f32x2(__A),
                                             (__v8sf)__O);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcast_f32x2 (__mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                             (__v8sf)_mm256_broadcast_f32x2(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_f64x2(__m128d __A)
{
  return (__m256d)__builtin_shufflevector((__v2df)__A, (__v2df)__A,
                                          0, 1, 0, 1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcast_f64x2(__m256d __O, __mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__M,
                                            (__v4df)_mm256_broadcast_f64x2(__A),
                                            (__v4df)__O);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcast_f64x2 (__mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__M,
                                            (__v4df)_mm256_broadcast_f64x2(__A),
                                            (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_broadcast_i32x2 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 0, 1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_mask_broadcast_i32x2 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_broadcast_i32x2(__A),
                                             (__v4si)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(128)))
_mm_maskz_broadcast_i32x2 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_broadcast_i32x2(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_i32x2 (__m128i __A)
{
  return (__m256i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcast_i32x2 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_broadcast_i32x2(__A),
                                             (__v8si)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcast_i32x2 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_broadcast_i32x2(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_broadcast_i64x2(__m128i __A)
{
  return (__m256i)__builtin_shufflevector((__v2di)__A, (__v2di)__A,
                                          0, 1, 0, 1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_mask_broadcast_i64x2(__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                            (__v4di)_mm256_broadcast_i64x2(__A),
                                            (__v4di)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_broadcast_i64x2 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                            (__v4di)_mm256_broadcast_i64x2(__A),
                                            (__v4di)_mm256_setzero_si256());
}
# 175 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512erintrin.h" 1 3
# 180 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512ifmaintrin.h" 1 3
# 22 "/usr/lib/clang/18/include/avx512ifmaintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,evex512"), __min_vector_width__(512)))
_mm512_madd52hi_epu64 (__m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i)__builtin_ia32_vpmadd52huq512((__v8di) __X, (__v8di) __Y,
                                                (__v8di) __Z);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,evex512"), __min_vector_width__(512)))
_mm512_mask_madd52hi_epu64 (__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                   (__v8di)_mm512_madd52hi_epu64(__W, __X, __Y),
                                   (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,evex512"), __min_vector_width__(512)))
_mm512_maskz_madd52hi_epu64 (__mmask8 __M, __m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                   (__v8di)_mm512_madd52hi_epu64(__X, __Y, __Z),
                                   (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,evex512"), __min_vector_width__(512)))
_mm512_madd52lo_epu64 (__m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i)__builtin_ia32_vpmadd52luq512((__v8di) __X, (__v8di) __Y,
                                                (__v8di) __Z);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,evex512"), __min_vector_width__(512)))
_mm512_mask_madd52lo_epu64 (__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                   (__v8di)_mm512_madd52lo_epu64(__W, __X, __Y),
                                   (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,evex512"), __min_vector_width__(512)))
_mm512_maskz_madd52lo_epu64 (__mmask8 __M, __m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                   (__v8di)_mm512_madd52lo_epu64(__X, __Y, __Z),
                                   (__v8di)_mm512_setzero_si512());
}
# 185 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512ifmavlintrin.h" 1 3
# 43 "/usr/lib/clang/18/include/avx512ifmavlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_madd52hi_epu64 (__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                      (__v2di)((__m128i)__builtin_ia32_vpmadd52huq128((__v2di)(__W), (__v2di)(__X), (__v2di)(__Y))),
                                      (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_madd52hi_epu64 (__mmask8 __M, __m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                      (__v2di)((__m128i)__builtin_ia32_vpmadd52huq128((__v2di)(__X), (__v2di)(__Y), (__v2di)(__Z))),
                                      (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_madd52hi_epu64 (__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                   (__v4di)((__m256i)__builtin_ia32_vpmadd52huq256((__v4di)(__W), (__v4di)(__X), (__v4di)(__Y))),
                                   (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_madd52hi_epu64 (__mmask8 __M, __m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                   (__v4di)((__m256i)__builtin_ia32_vpmadd52huq256((__v4di)(__X), (__v4di)(__Y), (__v4di)(__Z))),
                                   (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_madd52lo_epu64 (__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                      (__v2di)((__m128i)__builtin_ia32_vpmadd52luq128((__v2di)(__W), (__v2di)(__X), (__v2di)(__Y))),
                                      (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_madd52lo_epu64 (__mmask8 __M, __m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                      (__v2di)((__m128i)__builtin_ia32_vpmadd52luq128((__v2di)(__X), (__v2di)(__Y), (__v2di)(__Z))),
                                      (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_madd52lo_epu64 (__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                   (__v4di)((__m256i)__builtin_ia32_vpmadd52luq256((__v4di)(__W), (__v4di)(__X), (__v4di)(__Y))),
                                   (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_madd52lo_epu64 (__mmask8 __M, __m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                   (__v4di)((__m256i)__builtin_ia32_vpmadd52luq256((__v4di)(__X), (__v4di)(__Y), (__v4di)(__Z))),
                                   (__v4di)_mm256_setzero_si256());
}
# 190 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avxifmaintrin.h" 1 3
# 58 "/usr/lib/clang/18/include/avxifmaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxifma"), __min_vector_width__(128)))
_mm_madd52hi_avx_epu64(__m128i __X, __m128i __Y, __m128i __Z) {
  return (__m128i)__builtin_ia32_vpmadd52huq128((__v2di)__X, (__v2di)__Y,
                                                (__v2di)__Z);
}
# 95 "/usr/lib/clang/18/include/avxifmaintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxifma"), __min_vector_width__(256)))
_mm256_madd52hi_avx_epu64(__m256i __X, __m256i __Y, __m256i __Z) {
  return (__m256i)__builtin_ia32_vpmadd52huq256((__v4di)__X, (__v4di)__Y,
                                                (__v4di)__Z);
}
# 132 "/usr/lib/clang/18/include/avxifmaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxifma"), __min_vector_width__(128)))
_mm_madd52lo_avx_epu64(__m128i __X, __m128i __Y, __m128i __Z) {
  return (__m128i)__builtin_ia32_vpmadd52luq128((__v2di)__X, (__v2di)__Y,
                                                (__v2di)__Z);
}
# 169 "/usr/lib/clang/18/include/avxifmaintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxifma"), __min_vector_width__(256)))
_mm256_madd52lo_avx_epu64(__m256i __X, __m256i __Y, __m256i __Z) {
  return (__m256i)__builtin_ia32_vpmadd52luq256((__v4di)__X, (__v4di)__Y,
                                                (__v4di)__Z);
}
# 195 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vbmiintrin.h" 1 3
# 22 "/usr/lib/clang/18/include/avx512vbmiintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_permutex2var_epi8(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpermi2varqi512((__v64qi)__A, (__v64qi)__I,
                                                 (__v64qi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_mask_permutex2var_epi8(__m512i __A, __mmask64 __U, __m512i __I,
                              __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512(__U,
                               (__v64qi)_mm512_permutex2var_epi8(__A, __I, __B),
                               (__v64qi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_epi8(__m512i __A, __m512i __I, __mmask64 __U,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512(__U,
                               (__v64qi)_mm512_permutex2var_epi8(__A, __I, __B),
                               (__v64qi)__I);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_epi8(__mmask64 __U, __m512i __A, __m512i __I,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512(__U,
                               (__v64qi)_mm512_permutex2var_epi8(__A, __I, __B),
                               (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_permutexvar_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_permvarqi512((__v64qi) __B, (__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_epi8 (__mmask64 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                     (__v64qi)_mm512_permutexvar_epi8(__A, __B),
                                     (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_mask_permutexvar_epi8 (__m512i __W, __mmask64 __M, __m512i __A,
             __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                     (__v64qi)_mm512_permutexvar_epi8(__A, __B),
                                     (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_multishift_epi64_epi8(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_vpmultishiftqb512((__v64qi)__X, (__v64qi) __Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_mask_multishift_epi64_epi8(__m512i __W, __mmask64 __M, __m512i __X,
                                  __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                (__v64qi)_mm512_multishift_epi64_epi8(__X, __Y),
                                (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,evex512"), __min_vector_width__(512)))
_mm512_maskz_multishift_epi64_epi8(__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                (__v64qi)_mm512_multishift_epi64_epi8(__X, __Y),
                                (__v64qi)_mm512_setzero_si512());
}
# 200 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vbmivlintrin.h" 1 3
# 27 "/usr/lib/clang/18/include/avx512vbmivlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_permutex2var_epi8(__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpermi2varqi128((__v16qi)__A,
                                                 (__v16qi)__I,
                                                 (__v16qi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_permutex2var_epi8(__m128i __A, __mmask16 __U, __m128i __I,
                           __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128(__U,
                                  (__v16qi)_mm_permutex2var_epi8(__A, __I, __B),
                                  (__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask2_permutex2var_epi8(__m128i __A, __m128i __I, __mmask16 __U,
                            __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128(__U,
                                  (__v16qi)_mm_permutex2var_epi8(__A, __I, __B),
                                  (__v16qi)__I);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_permutex2var_epi8(__mmask16 __U, __m128i __A, __m128i __I,
                            __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128(__U,
                                  (__v16qi)_mm_permutex2var_epi8(__A, __I, __B),
                                  (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_permutex2var_epi8(__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpermi2varqi256((__v32qi)__A, (__v32qi)__I,
                                                 (__v32qi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutex2var_epi8(__m256i __A, __mmask32 __U, __m256i __I,
                              __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256(__U,
                               (__v32qi)_mm256_permutex2var_epi8(__A, __I, __B),
                               (__v32qi)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask2_permutex2var_epi8(__m256i __A, __m256i __I, __mmask32 __U,
                               __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256(__U,
                               (__v32qi)_mm256_permutex2var_epi8(__A, __I, __B),
                               (__v32qi)__I);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutex2var_epi8(__mmask32 __U, __m256i __A, __m256i __I,
                               __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256(__U,
                               (__v32qi)_mm256_permutex2var_epi8(__A, __I, __B),
                               (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_permutexvar_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_permvarqi128((__v16qi)__B, (__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_permutexvar_epi8 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                        (__v16qi)_mm_permutexvar_epi8(__A, __B),
                                        (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_permutexvar_epi8 (__m128i __W, __mmask16 __M, __m128i __A,
          __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                        (__v16qi)_mm_permutexvar_epi8(__A, __B),
                                        (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_permutexvar_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_permvarqi256((__v32qi) __B, (__v32qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_epi8 (__mmask32 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                     (__v32qi)_mm256_permutexvar_epi8(__A, __B),
                                     (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_permutexvar_epi8 (__m256i __W, __mmask32 __M, __m256i __A,
             __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                     (__v32qi)_mm256_permutexvar_epi8(__A, __B),
                                     (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_multishift_epi64_epi8(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_vpmultishiftqb128((__v16qi)__X, (__v16qi)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_multishift_epi64_epi8(__m128i __W, __mmask16 __M, __m128i __X,
                               __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                   (__v16qi)_mm_multishift_epi64_epi8(__X, __Y),
                                   (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_multishift_epi64_epi8(__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                   (__v16qi)_mm_multishift_epi64_epi8(__X, __Y),
                                   (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_multishift_epi64_epi8(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_vpmultishiftqb256((__v32qi)__X, (__v32qi)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_multishift_epi64_epi8(__m256i __W, __mmask32 __M, __m256i __X,
                                  __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                (__v32qi)_mm256_multishift_epi64_epi8(__X, __Y),
                                (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_multishift_epi64_epi8(__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                (__v32qi)_mm256_multishift_epi64_epi8(__X, __Y),
                                (__v32qi)_mm256_setzero_si256());
}
# 205 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vbmi2intrin.h" 1 3
# 21 "/usr/lib/clang/18/include/avx512vbmi2intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_compress_epi16(__m512i __S, __mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compresshi512_mask ((__v32hi) __D,
              (__v32hi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_compress_epi16(__mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compresshi512_mask ((__v32hi) __D,
              (__v32hi) _mm512_setzero_si512(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_compress_epi8(__m512i __S, __mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compressqi512_mask ((__v64qi) __D,
              (__v64qi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_compress_epi8(__mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compressqi512_mask ((__v64qi) __D,
              (__v64qi) _mm512_setzero_si512(),
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_epi16(void *__P, __mmask32 __U, __m512i __D)
{
  __builtin_ia32_compressstorehi512_mask ((__v32hi *) __P, (__v32hi) __D,
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_epi8(void *__P, __mmask64 __U, __m512i __D)
{
  __builtin_ia32_compressstoreqi512_mask ((__v64qi *) __P, (__v64qi) __D,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_expand_epi16(__m512i __S, __mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandhi512_mask ((__v32hi) __D,
              (__v32hi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_expand_epi16(__mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandhi512_mask ((__v32hi) __D,
              (__v32hi) _mm512_setzero_si512(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_expand_epi8(__m512i __S, __mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandqi512_mask ((__v64qi) __D,
              (__v64qi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_expand_epi8(__mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandqi512_mask ((__v64qi) __D,
              (__v64qi) _mm512_setzero_si512(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_expandloadu_epi16(__m512i __S, __mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadhi512_mask ((const __v32hi *)__P,
              (__v32hi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_epi16(__mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadhi512_mask ((const __v32hi *)__P,
              (__v32hi) _mm512_setzero_si512(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_expandloadu_epi8(__m512i __S, __mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadqi512_mask ((const __v64qi *)__P,
              (__v64qi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_epi8(__mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadqi512_mask ((const __v64qi *)__P,
              (__v64qi) _mm512_setzero_si512(),
              __U);
}
# 215 "/usr/lib/clang/18/include/avx512vbmi2intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_shldv_epi64(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshldvq512((__v8di)__A, (__v8di)__B,
                                             (__v8di)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_shldv_epi64(__m512i __A, __mmask8 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                      (__v8di)_mm512_shldv_epi64(__A, __B, __C),
                                      (__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_shldv_epi64(__mmask8 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                      (__v8di)_mm512_shldv_epi64(__A, __B, __C),
                                      (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_shldv_epi32(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshldvd512((__v16si)__A, (__v16si)__B,
                                             (__v16si)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_shldv_epi32(__m512i __A, __mmask16 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                     (__v16si)_mm512_shldv_epi32(__A, __B, __C),
                                     (__v16si)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_shldv_epi32(__mmask16 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                     (__v16si)_mm512_shldv_epi32(__A, __B, __C),
                                     (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_shldv_epi16(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshldvw512((__v32hi)__A, (__v32hi)__B,
                                             (__v32hi)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_shldv_epi16(__m512i __A, __mmask32 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                                     (__v32hi)_mm512_shldv_epi16(__A, __B, __C),
                                     (__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_shldv_epi16(__mmask32 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                                     (__v32hi)_mm512_shldv_epi16(__A, __B, __C),
                                     (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_shrdv_epi64(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshrdvq512((__v8di)__A, (__v8di)__B,
                                             (__v8di)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_shrdv_epi64(__m512i __A, __mmask8 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                      (__v8di)_mm512_shrdv_epi64(__A, __B, __C),
                                      (__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_shrdv_epi64(__mmask8 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                      (__v8di)_mm512_shrdv_epi64(__A, __B, __C),
                                      (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_shrdv_epi32(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshrdvd512((__v16si)__A, (__v16si)__B,
                                             (__v16si)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_shrdv_epi32(__m512i __A, __mmask16 __U, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_selectd_512(__U,
                                     (__v16si)_mm512_shrdv_epi32(__A, __B, __C),
                                     (__v16si)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_shrdv_epi32(__mmask16 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_selectd_512(__U,
                                     (__v16si)_mm512_shrdv_epi32(__A, __B, __C),
                                     (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_shrdv_epi16(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshrdvw512((__v32hi)__A, (__v32hi)__B,
                                             (__v32hi)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_mask_shrdv_epi16(__m512i __A, __mmask32 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                                     (__v32hi)_mm512_shrdv_epi16(__A, __B, __C),
                                     (__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2,evex512"), __min_vector_width__(512)))
_mm512_maskz_shrdv_epi16(__mmask32 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                                     (__v32hi)_mm512_shrdv_epi16(__A, __B, __C),
                                     (__v32hi)_mm512_setzero_si512());
}
# 210 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vlvbmi2intrin.h" 1 3
# 27 "/usr/lib/clang/18/include/avx512vlvbmi2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_compress_epi16(__m128i __S, __mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compresshi128_mask ((__v8hi) __D,
              (__v8hi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_compress_epi16(__mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compresshi128_mask ((__v8hi) __D,
              (__v8hi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_compress_epi8(__m128i __S, __mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compressqi128_mask ((__v16qi) __D,
              (__v16qi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_compress_epi8(__mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compressqi128_mask ((__v16qi) __D,
              (__v16qi) _mm_setzero_si128(),
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_compressstoreu_epi16(void *__P, __mmask8 __U, __m128i __D)
{
  __builtin_ia32_compressstorehi128_mask ((__v8hi *) __P, (__v8hi) __D,
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_compressstoreu_epi8(void *__P, __mmask16 __U, __m128i __D)
{
  __builtin_ia32_compressstoreqi128_mask ((__v16qi *) __P, (__v16qi) __D,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_expand_epi16(__m128i __S, __mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandhi128_mask ((__v8hi) __D,
              (__v8hi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expand_epi16(__mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandhi128_mask ((__v8hi) __D,
              (__v8hi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_expand_epi8(__m128i __S, __mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandqi128_mask ((__v16qi) __D,
              (__v16qi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expand_epi8(__mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandqi128_mask ((__v16qi) __D,
              (__v16qi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_expandloadu_epi16(__m128i __S, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadhi128_mask ((const __v8hi *)__P,
              (__v8hi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expandloadu_epi16(__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadhi128_mask ((const __v8hi *)__P,
              (__v8hi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_expandloadu_epi8(__m128i __S, __mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadqi128_mask ((const __v16qi *)__P,
              (__v16qi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_expandloadu_epi8(__mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadqi128_mask ((const __v16qi *)__P,
              (__v16qi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compress_epi16(__m256i __S, __mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compresshi256_mask ((__v16hi) __D,
              (__v16hi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_compress_epi16(__mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compresshi256_mask ((__v16hi) __D,
              (__v16hi) _mm256_setzero_si256(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compress_epi8(__m256i __S, __mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compressqi256_mask ((__v32qi) __D,
              (__v32qi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_compress_epi8(__mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compressqi256_mask ((__v32qi) __D,
              (__v32qi) _mm256_setzero_si256(),
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_epi16(void *__P, __mmask16 __U, __m256i __D)
{
  __builtin_ia32_compressstorehi256_mask ((__v16hi *) __P, (__v16hi) __D,
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_epi8(void *__P, __mmask32 __U, __m256i __D)
{
  __builtin_ia32_compressstoreqi256_mask ((__v32qi *) __P, (__v32qi) __D,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expand_epi16(__m256i __S, __mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandhi256_mask ((__v16hi) __D,
              (__v16hi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expand_epi16(__mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandhi256_mask ((__v16hi) __D,
              (__v16hi) _mm256_setzero_si256(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expand_epi8(__m256i __S, __mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandqi256_mask ((__v32qi) __D,
              (__v32qi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expand_epi8(__mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandqi256_mask ((__v32qi) __D,
              (__v32qi) _mm256_setzero_si256(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expandloadu_epi16(__m256i __S, __mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadhi256_mask ((const __v16hi *)__P,
              (__v16hi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_epi16(__mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadhi256_mask ((const __v16hi *)__P,
              (__v16hi) _mm256_setzero_si256(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_expandloadu_epi8(__m256i __S, __mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadqi256_mask ((const __v32qi *)__P,
              (__v32qi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_epi8(__mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadqi256_mask ((const __v32qi *)__P,
              (__v32qi) _mm256_setzero_si256(),
              __U);
}
# 415 "/usr/lib/clang/18/include/avx512vlvbmi2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_shldv_epi64(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshldvq256((__v4di)__A, (__v4di)__B,
                                             (__v4di)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_shldv_epi64(__m256i __A, __mmask8 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                      (__v4di)_mm256_shldv_epi64(__A, __B, __C),
                                      (__v4di)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_shldv_epi64(__mmask8 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                      (__v4di)_mm256_shldv_epi64(__A, __B, __C),
                                      (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_shldv_epi64(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshldvq128((__v2di)__A, (__v2di)__B,
                                             (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_shldv_epi64(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                         (__v2di)_mm_shldv_epi64(__A, __B, __C),
                                         (__v2di)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_shldv_epi64(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                         (__v2di)_mm_shldv_epi64(__A, __B, __C),
                                         (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_shldv_epi32(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshldvd256((__v8si)__A, (__v8si)__B,
                                             (__v8si)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_shldv_epi32(__m256i __A, __mmask8 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                      (__v8si)_mm256_shldv_epi32(__A, __B, __C),
                                      (__v8si)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_shldv_epi32(__mmask8 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                      (__v8si)_mm256_shldv_epi32(__A, __B, __C),
                                      (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_shldv_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshldvd128((__v4si)__A, (__v4si)__B,
                                             (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_shldv_epi32(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                         (__v4si)_mm_shldv_epi32(__A, __B, __C),
                                         (__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_shldv_epi32(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                         (__v4si)_mm_shldv_epi32(__A, __B, __C),
                                         (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_shldv_epi16(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshldvw256((__v16hi)__A, (__v16hi)__B,
                                             (__v16hi)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_shldv_epi16(__m256i __A, __mmask16 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                                      (__v16hi)_mm256_shldv_epi16(__A, __B, __C),
                                      (__v16hi)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_shldv_epi16(__mmask16 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                                      (__v16hi)_mm256_shldv_epi16(__A, __B, __C),
                                      (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_shldv_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshldvw128((__v8hi)__A, (__v8hi)__B,
                                             (__v8hi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_shldv_epi16(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                         (__v8hi)_mm_shldv_epi16(__A, __B, __C),
                                         (__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_shldv_epi16(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                         (__v8hi)_mm_shldv_epi16(__A, __B, __C),
                                         (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_shrdv_epi64(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshrdvq256((__v4di)__A, (__v4di)__B,
                                             (__v4di)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_shrdv_epi64(__m256i __A, __mmask8 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                      (__v4di)_mm256_shrdv_epi64(__A, __B, __C),
                                      (__v4di)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_shrdv_epi64(__mmask8 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                      (__v4di)_mm256_shrdv_epi64(__A, __B, __C),
                                      (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_shrdv_epi64(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshrdvq128((__v2di)__A, (__v2di)__B,
                                             (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_shrdv_epi64(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                         (__v2di)_mm_shrdv_epi64(__A, __B, __C),
                                         (__v2di)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_shrdv_epi64(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                         (__v2di)_mm_shrdv_epi64(__A, __B, __C),
                                         (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_shrdv_epi32(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshrdvd256((__v8si)__A, (__v8si)__B,
                                             (__v8si)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_shrdv_epi32(__m256i __A, __mmask8 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                      (__v8si)_mm256_shrdv_epi32(__A, __B, __C),
                                      (__v8si)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_shrdv_epi32(__mmask8 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                      (__v8si)_mm256_shrdv_epi32(__A, __B, __C),
                                      (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_shrdv_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshrdvd128((__v4si)__A, (__v4si)__B,
                                             (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_shrdv_epi32(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                         (__v4si)_mm_shrdv_epi32(__A, __B, __C),
                                         (__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_shrdv_epi32(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                         (__v4si)_mm_shrdv_epi32(__A, __B, __C),
                                         (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_shrdv_epi16(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshrdvw256((__v16hi)__A, (__v16hi)__B,
                                             (__v16hi)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_mask_shrdv_epi16(__m256i __A, __mmask16 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                                     (__v16hi)_mm256_shrdv_epi16(__A, __B, __C),
                                     (__v16hi)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_shrdv_epi16(__mmask16 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                                     (__v16hi)_mm256_shrdv_epi16(__A, __B, __C),
                                     (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_shrdv_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshrdvw128((__v8hi)__A, (__v8hi)__B,
                                             (__v8hi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_mask_shrdv_epi16(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                         (__v8hi)_mm_shrdv_epi16(__A, __B, __C),
                                         (__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2,no-evex512"), __min_vector_width__(128)))
_mm_maskz_shrdv_epi16(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                         (__v8hi)_mm_shrdv_epi16(__A, __B, __C),
                                         (__v8hi)_mm_setzero_si128());
}
# 215 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512pfintrin.h" 1 3
# 220 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512fp16intrin.h" 1 3
# 19 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
typedef _Float16 __v32hf __attribute__((__vector_size__(64), __aligned__(64)));
typedef _Float16 __m512h __attribute__((__vector_size__(64), __aligned__(64)));
typedef _Float16 __m512h_u __attribute__((__vector_size__(64), __aligned__(1)));
# 36 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_cvtsh_h(__m512h __a) {
  return __a[0];
}

static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_setzero_ph(void) {
  return (__m128h){0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256))) _mm256_setzero_ph(void) {
  return (__m256h){0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256))) _mm256_undefined_ph(void) {
  return (__m256h)__builtin_ia32_undef256();
}

static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_setzero_ph(void) {
  return (__m512h){0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_undefined_ph(void) {
  return (__m128h)__builtin_ia32_undef128();
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_undefined_ph(void) {
  return (__m512h)__builtin_ia32_undef512();
}

static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_set1_ph(_Float16 __h) {
  return (__m512h)(__v32hf){__h, __h, __h, __h, __h, __h, __h, __h,
                            __h, __h, __h, __h, __h, __h, __h, __h,
                            __h, __h, __h, __h, __h, __h, __h, __h,
                            __h, __h, __h, __h, __h, __h, __h, __h};
}

static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_set_ph(_Float16 __h1, _Float16 __h2, _Float16 __h3, _Float16 __h4,
              _Float16 __h5, _Float16 __h6, _Float16 __h7, _Float16 __h8,
              _Float16 __h9, _Float16 __h10, _Float16 __h11, _Float16 __h12,
              _Float16 __h13, _Float16 __h14, _Float16 __h15, _Float16 __h16,
              _Float16 __h17, _Float16 __h18, _Float16 __h19, _Float16 __h20,
              _Float16 __h21, _Float16 __h22, _Float16 __h23, _Float16 __h24,
              _Float16 __h25, _Float16 __h26, _Float16 __h27, _Float16 __h28,
              _Float16 __h29, _Float16 __h30, _Float16 __h31, _Float16 __h32) {
  return (__m512h)(__v32hf){__h32, __h31, __h30, __h29, __h28, __h27, __h26,
                            __h25, __h24, __h23, __h22, __h21, __h20, __h19,
                            __h18, __h17, __h16, __h15, __h14, __h13, __h12,
                            __h11, __h10, __h9, __h8, __h7, __h6, __h5,
                            __h4, __h3, __h2, __h1};
}
# 98 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_set1_pch(_Float16 _Complex h) {
  return (__m512h)_mm512_set1_ps(__builtin_bit_cast(float, h));
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_castph_ps(__m128h __a) {
  return (__m128)__a;
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256))) _mm256_castph_ps(__m256h __a) {
  return (__m256)__a;
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_castph_ps(__m512h __a) {
  return (__m512)__a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_castph_pd(__m128h __a) {
  return (__m128d)__a;
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256))) _mm256_castph_pd(__m256h __a) {
  return (__m256d)__a;
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_castph_pd(__m512h __a) {
  return (__m512d)__a;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_castph_si128(__m128h __a) {
  return (__m128i)__a;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256)))
_mm256_castph_si256(__m256h __a) {
  return (__m256i)__a;
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_castph_si512(__m512h __a) {
  return (__m512i)__a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_castps_ph(__m128 __a) {
  return (__m128h)__a;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256))) _mm256_castps_ph(__m256 __a) {
  return (__m256h)__a;
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_castps_ph(__m512 __a) {
  return (__m512h)__a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_castpd_ph(__m128d __a) {
  return (__m128h)__a;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256))) _mm256_castpd_ph(__m256d __a) {
  return (__m256h)__a;
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_castpd_ph(__m512d __a) {
  return (__m512h)__a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_castsi128_ph(__m128i __a) {
  return (__m128h)__a;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256)))
_mm256_castsi256_ph(__m256i __a) {
  return (__m256h)__a;
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_castsi512_ph(__m512i __a) {
  return (__m512h)__a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256)))
_mm256_castph256_ph128(__m256h __a) {
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_castph512_ph128(__m512h __a) {
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_castph512_ph256(__m512h __a) {
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                 12, 13, 14, 15);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256)))
_mm256_castph128_ph256(__m128h __a) {
  return __builtin_shufflevector(__a, __builtin_nondeterministic_value(__a),
                                  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_castph128_ph512(__m128h __a) {
  __m256h __b = __builtin_nondeterministic_value(__b);
  return __builtin_shufflevector(
      __builtin_shufflevector(__a, __builtin_nondeterministic_value(__a),
                              0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15),
      __b, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
      20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_castph256_ph512(__m256h __a) {
  return __builtin_shufflevector(__a, __builtin_nondeterministic_value(__a), 0,
                                 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,
                                 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,
                                 27, 28, 29, 30, 31);
}
# 232 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256)))
_mm256_zextph128_ph256(__m128h __a) {
  return __builtin_shufflevector(__a, (__v8hf)_mm_setzero_ph(), 0, 1, 2, 3, 4,
                                 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}
# 251 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_zextph128_ph512(__m128h __a) {
  return __builtin_shufflevector(
      __a, (__v8hf)_mm_setzero_ph(), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
      13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15);
}
# 271 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_zextph256_ph512(__m256h __a) {
  return __builtin_shufflevector(__a, (__v16hf)_mm256_setzero_ph(), 0, 1, 2, 3,
                                 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
                                 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,
                                 29, 30, 31);
}







static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_comieq_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x10,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_comilt_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x01,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_comile_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x02,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_comigt_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x0e,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_comige_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x0d,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_comineq_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x14,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_ucomieq_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x00,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_ucomilt_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x11,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_ucomile_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x12,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_ucomigt_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x1e,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_ucomige_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x1d,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_ucomineq_sh(__m128h A,
                                                            __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x04,
                                0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_add_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)((__v32hf)__A + (__v32hf)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_add_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_add_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_add_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_add_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 389 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_sub_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)((__v32hf)__A - (__v32hf)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_sub_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_sub_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_sub_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_sub_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 421 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_mul_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)((__v32hf)__A * (__v32hf)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_mul_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_mul_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_mul_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_mul_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 453 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_div_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)((__v32hf)__A / (__v32hf)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_div_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_div_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_div_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_div_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 485 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_min_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)__builtin_ia32_minph512((__v32hf)__A, (__v32hf)__B,
                                          0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_min_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_min_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_min_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_min_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 518 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_max_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)__builtin_ia32_maxph512((__v32hf)__A, (__v32hf)__B,
                                          0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_max_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_max_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_max_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_max_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 551 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_abs_ph(__m512h __A) {
  return (__m512h)_mm512_and_epi32(_mm512_set1_epi32(0x7FFF7FFF), (__m512i)__A);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_conj_pch(__m512h __A) {
  return (__m512h)_mm512_xor_ps((__m512)__A, _mm512_set1_ps(-0.0f));
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_conj_pch(__m512h __W, __mmask16 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_selectps_512(
      (__mmask16)__U, (__v16sf)_mm512_conj_pch(__A), (__v16sf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_conj_pch(__mmask16 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_selectps_512((__mmask16)__U,
                                              (__v16sf)_mm512_conj_pch(__A),
                                              (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_add_sh(__m128h __A,
                                                           __m128h __B) {
  __A[0] += __B[0];
  return __A;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_add_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  __A = _mm_add_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_add_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  __A = _mm_add_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, _mm_setzero_ph());
}
# 608 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_sub_sh(__m128h __A,
                                                           __m128h __B) {
  __A[0] -= __B[0];
  return __A;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_sub_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  __A = _mm_sub_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_sub_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  __A = _mm_sub_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, _mm_setzero_ph());
}
# 644 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mul_sh(__m128h __A,
                                                           __m128h __B) {
  __A[0] *= __B[0];
  return __A;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_mul_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  __A = _mm_mul_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_mul_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  __A = _mm_mul_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, _mm_setzero_ph());
}
# 680 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_div_sh(__m128h __A,
                                                           __m128h __B) {
  __A[0] /= __B[0];
  return __A;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_div_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  __A = _mm_div_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_div_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  __A = _mm_div_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, _mm_setzero_ph());
}
# 716 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_min_sh(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_minsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_min_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_minsh_round_mask((__v8hf)__A, (__v8hf)__B,
                                                  (__v8hf)__W, (__mmask8)__U,
                                                  0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_min_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_minsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 755 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_max_sh(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_maxsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_max_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_maxsh_round_mask((__v8hf)__A, (__v8hf)__B,
                                                  (__v8hf)__W, (__mmask8)__U,
                                                  0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_max_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_maxsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 830 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_load_sh(void const *__dp) {
  struct __mm_load_sh_struct {
    _Float16 __u;
  } __attribute__((__packed__, __may_alias__));
  _Float16 __u = ((const struct __mm_load_sh_struct *)__dp)->__u;
  return (__m128h){__u, 0, 0, 0, 0, 0, 0, 0};
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask_load_sh(__m128h __W, __mmask8 __U, const void *__A) {
  __m128h src = (__v8hf)__builtin_shufflevector(
      (__v8hf)__W, (__v8hf)_mm_setzero_ph(), 0, 8, 8, 8, 8, 8, 8, 8);

  return (__m128h)__builtin_ia32_loadsh128_mask((const __v8hf *)__A, src, __U & 1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_load_sh(__mmask8 __U, const void *__A) {
  return (__m128h)__builtin_ia32_loadsh128_mask(
      (const __v8hf *)__A, (__v8hf)_mm_setzero_ph(), __U & 1);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_load_ph(void const *__p) {
  return *(const __m512h *)__p;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256)))
_mm256_load_ph(void const *__p) {
  return *(const __m256h *)__p;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_load_ph(void const *__p) {
  return *(const __m128h *)__p;
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_loadu_ph(void const *__p) {
  struct __loadu_ph {
    __m512h_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ph *)__p)->__v;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256)))
_mm256_loadu_ph(void const *__p) {
  struct __loadu_ph {
    __m256h_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ph *)__p)->__v;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_loadu_ph(void const *__p) {
  struct __loadu_ph {
    __m128h_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ph *)__p)->__v;
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_store_sh(void *__dp,
                                                          __m128h __a) {
  struct __mm_store_sh_struct {
    _Float16 __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store_sh_struct *)__dp)->__u = __a[0];
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_store_sh(void *__W,
                                                               __mmask8 __U,
                                                               __m128h __A) {
  __builtin_ia32_storesh128_mask((__v8hf *)__W, __A, __U & 1);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_store_ph(void *__P,
                                                             __m512h __A) {
  *(__m512h *)__P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256))) _mm256_store_ph(void *__P,
                                                             __m256h __A) {
  *(__m256h *)__P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_store_ph(void *__P,
                                                          __m128h __A) {
  *(__m128h *)__P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_storeu_ph(void *__P,
                                                              __m512h __A) {
  struct __storeu_ph {
    __m512h_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ph *)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(256))) _mm256_storeu_ph(void *__P,
                                                              __m256h __A) {
  struct __storeu_ph {
    __m256h_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ph *)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_storeu_ph(void *__P,
                                                           __m128h __A) {
  struct __storeu_ph {
    __m128h_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ph *)__P)->__v = __A;
}


static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_move_sh(__m128h __a,
                                                            __m128h __b) {
  __a[0] = __b[0];
  return __a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_move_sh(__m128h __W,
                                                                 __mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return __builtin_ia32_selectsh_128(__U, _mm_move_sh(__A, __B), __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_move_sh(__mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return __builtin_ia32_selectsh_128(__U, _mm_move_sh(__A, __B),
                                     _mm_setzero_ph());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvtsi16_si128(short __a) {
  return (__m128i)(__v8hi){__a, 0, 0, 0, 0, 0, 0, 0};
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvtsi128_si16(__m128i __a) {
  __v8hi __b = (__v8hi)__a;
  return __b[0];
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_rcp_ph(__m512h __A) {
  return (__m512h)__builtin_ia32_rcpph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_undefined_ph(), (__mmask32)-1);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_rcp_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_rcpph512_mask((__v32hf)__A, (__v32hf)__W,
                                               (__mmask32)__U);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_rcp_ph(__mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_rcpph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_rsqrt_ph(__m512h __A) {
  return (__m512h)__builtin_ia32_rsqrtph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_undefined_ph(), (__mmask32)-1);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_rsqrt_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_rsqrtph512_mask((__v32hf)__A, (__v32hf)__W,
                                                 (__mmask32)__U);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_rsqrt_ph(__mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_rsqrtph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U);
}
# 1039 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_getexp_ph(__m512h __A) {
  return (__m512h)__builtin_ia32_getexpph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_undefined_ph(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_getexp_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_getexpph512_mask(
      (__v32hf)__A, (__v32hf)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_getexp_ph(__mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_getexpph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U,
      0x04);
}
# 1072 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_scalef_ph(__m512h __A,
                                                                 __m512h __B) {
  return (__m512h)__builtin_ia32_scalefph512_mask(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)_mm512_undefined_ph(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_scalef_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_scalefph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  (__v32hf)__W, (__mmask32)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_scalef_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_scalefph512_mask(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U,
      0x04);
}
# 1168 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_rcp_sh(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_rcpsh_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_rcp_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_rcpsh_mask((__v8hf)__A, (__v8hf)__B,
                                            (__v8hf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_rcp_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_rcpsh_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_rsqrt_sh(__m128h __A,
                                                             __m128h __B) {
  return (__m128h)__builtin_ia32_rsqrtsh_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_rsqrt_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_rsqrtsh_mask((__v8hf)__A, (__v8hf)__B,
                                              (__v8hf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rsqrt_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_rsqrtsh_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}
# 1244 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_getexp_sh(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_getexpsh128_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask_getexp_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_getexpsh128_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)__W, (__mmask8)__U,
      0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_getexp_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_getexpsh128_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1280 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_scalef_sh(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_scalefsh_round_mask(
      (__v8hf)__A, (__v8hf)(__B), (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask_scalef_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_scalefsh_round_mask((__v8hf)__A, (__v8hf)__B,
                                                     (__v8hf)__W, (__mmask8)__U,
                                                     0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_scalef_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_scalefsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1384 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_sqrt_ph(__m512h __A) {
  return (__m512h)__builtin_ia32_sqrtph512((__v32hf)__A,
                                           0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_sqrt_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)(__U),
      (__v32hf)__builtin_ia32_sqrtph512((__A), (0x04)),
      (__v32hf)(__m512h)(__W));
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_sqrt_ph(__mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)(__U),
      (__v32hf)__builtin_ia32_sqrtph512((__A), (0x04)),
      (__v32hf)_mm512_setzero_ph());
}
# 1420 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_sqrt_sh(__m128h __A,
                                                            __m128h __B) {
  return (__m128h)__builtin_ia32_sqrtsh_round_mask(
      (__v8hf)(__m128h)(__A), (__v8hf)(__m128h)(__B), (__v8hf)_mm_setzero_ph(),
      (__mmask8)-1, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_sqrt_sh(__m128h __W,
                                                                 __mmask32 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_sqrtsh_round_mask(
      (__v8hf)(__m128h)(__A), (__v8hf)(__m128h)(__B), (__v8hf)(__m128h)(__W),
      (__mmask8)(__U), 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_sqrt_sh(__mmask32 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_sqrtsh_round_mask(
      (__v8hf)(__m128h)(__A), (__v8hf)(__m128h)(__B), (__v8hf)_mm_setzero_ph(),
      (__mmask8)(__U), 0x04);
}
# 1472 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_cvtpd_ph(__m512d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph512_mask(
      (__v8df)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtpd_ph(__m128h __W, __mmask8 __U, __m512d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph512_mask(
      (__v8df)__A, (__v8hf)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_ph(__mmask8 __U, __m512d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph512_mask(
      (__v8df)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1503 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_cvtph_pd(__m128h __A) {
  return (__m512d)__builtin_ia32_vcvtph2pd512_mask(
      (__v8hf)__A, (__v8df)_mm512_setzero_pd(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtph_pd(__m512d __W, __mmask8 __U, __m128h __A) {
  return (__m512d)__builtin_ia32_vcvtph2pd512_mask(
      (__v8hf)__A, (__v8df)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtph_pd(__mmask8 __U, __m128h __A) {
  return (__m512d)__builtin_ia32_vcvtph2pd512_mask(
      (__v8hf)__A, (__v8df)_mm512_setzero_pd(), (__mmask8)__U,
      0x04);
}
# 1536 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvtsh_ss(__m128 __A,
                                                            __m128h __B) {
  return (__m128)__builtin_ia32_vcvtsh2ss_round_mask(
      (__v4sf)__A, (__v8hf)__B, (__v4sf)_mm_undefined_ps(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_cvtsh_ss(__m128 __W,
                                                                 __mmask8 __U,
                                                                 __m128 __A,
                                                                 __m128h __B) {
  return (__m128)__builtin_ia32_vcvtsh2ss_round_mask((__v4sf)__A, (__v8hf)__B,
                                                     (__v4sf)__W, (__mmask8)__U,
                                                     0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_cvtsh_ss(__mmask8 __U,
                                                                  __m128 __A,
                                                                  __m128h __B) {
  return (__m128)__builtin_ia32_vcvtsh2ss_round_mask(
      (__v4sf)__A, (__v8hf)__B, (__v4sf)_mm_setzero_ps(), (__mmask8)__U,
      0x04);
}
# 1574 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvtss_sh(__m128h __A,
                                                             __m128 __B) {
  return (__m128h)__builtin_ia32_vcvtss2sh_round_mask(
      (__v8hf)__A, (__v4sf)__B, (__v8hf)_mm_undefined_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_cvtss_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128 __B) {
  return (__m128h)__builtin_ia32_vcvtss2sh_round_mask(
      (__v8hf)__A, (__v4sf)__B, (__v8hf)__W, (__mmask8)__U,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_maskz_cvtss_sh(__mmask8 __U,
                                                                   __m128h __A,
                                                                   __m128 __B) {
  return (__m128h)__builtin_ia32_vcvtss2sh_round_mask(
      (__v8hf)__A, (__v4sf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1612 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvtsd_sh(__m128h __A,
                                                             __m128d __B) {
  return (__m128h)__builtin_ia32_vcvtsd2sh_round_mask(
      (__v8hf)__A, (__v2df)__B, (__v8hf)_mm_undefined_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_cvtsd_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128d __B) {
  return (__m128h)__builtin_ia32_vcvtsd2sh_round_mask(
      (__v8hf)__A, (__v2df)__B, (__v8hf)__W, (__mmask8)__U,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtsd_sh(__mmask8 __U, __m128h __A, __m128d __B) {
  return (__m128h)__builtin_ia32_vcvtsd2sh_round_mask(
      (__v8hf)__A, (__v2df)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1649 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvtsh_sd(__m128d __A,
                                                             __m128h __B) {
  return (__m128d)__builtin_ia32_vcvtsh2sd_round_mask(
      (__v2df)__A, (__v8hf)__B, (__v2df)_mm_undefined_pd(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_cvtsh_sd(__m128d __W,
                                                                  __mmask8 __U,
                                                                  __m128d __A,
                                                                  __m128h __B) {
  return (__m128d)__builtin_ia32_vcvtsh2sd_round_mask(
      (__v2df)__A, (__v8hf)__B, (__v2df)__W, (__mmask8)__U,
      0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtsh_sd(__mmask8 __U, __m128d __A, __m128h __B) {
  return (__m128d)__builtin_ia32_vcvtsh2sd_round_mask(
      (__v2df)__A, (__v8hf)__B, (__v2df)_mm_setzero_pd(), (__mmask8)__U,
      0x04);
}
# 1686 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtph_epi16(__m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2w512_mask(
      (__v32hf)__A, (__v32hi)_mm512_setzero_si512(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtph_epi16(__m512i __W, __mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2w512_mask(
      (__v32hf)__A, (__v32hi)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epi16(__mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2w512_mask(
      (__v32hf)__A, (__v32hi)_mm512_setzero_si512(), (__mmask32)__U,
      0x04);
}
# 1720 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvttph_epi16(__m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2w512_mask(
      (__v32hf)__A, (__v32hi)_mm512_setzero_si512(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttph_epi16(__m512i __W, __mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2w512_mask(
      (__v32hf)__A, (__v32hi)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epi16(__mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2w512_mask(
      (__v32hf)__A, (__v32hi)_mm512_setzero_si512(), (__mmask32)__U,
      0x04);
}
# 1753 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtepi16_ph(__m512i __A) {
  return (__m512h)__builtin_ia32_vcvtw2ph512_mask(
      (__v32hi)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_ph(__m512h __W, __mmask32 __U, __m512i __A) {
  return (__m512h)__builtin_ia32_vcvtw2ph512_mask(
      (__v32hi)__A, (__v32hf)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi16_ph(__mmask32 __U, __m512i __A) {
  return (__m512h)__builtin_ia32_vcvtw2ph512_mask(
      (__v32hi)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U,
      0x04);
}
# 1787 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtph_epu16(__m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uw512_mask(
      (__v32hf)__A, (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtph_epu16(__m512i __W, __mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uw512_mask(
      (__v32hf)__A, (__v32hu)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epu16(__mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uw512_mask(
      (__v32hf)__A, (__v32hu)_mm512_setzero_si512(), (__mmask32)__U,
      0x04);
}
# 1821 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvttph_epu16(__m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uw512_mask(
      (__v32hf)__A, (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttph_epu16(__m512i __W, __mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uw512_mask(
      (__v32hf)__A, (__v32hu)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epu16(__mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uw512_mask(
      (__v32hf)__A, (__v32hu)_mm512_setzero_si512(), (__mmask32)__U,
      0x04);
}
# 1854 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtepu16_ph(__m512i __A) {
  return (__m512h)__builtin_ia32_vcvtuw2ph512_mask(
      (__v32hu)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu16_ph(__m512h __W, __mmask32 __U, __m512i __A) {
  return (__m512h)__builtin_ia32_vcvtuw2ph512_mask(
      (__v32hu)__A, (__v32hf)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu16_ph(__mmask32 __U, __m512i __A) {
  return (__m512h)__builtin_ia32_vcvtuw2ph512_mask(
      (__v32hu)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U,
      0x04);
}
# 1888 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtph_epi32(__m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2dq512_mask(
      (__v16hf)__A, (__v16si)_mm512_setzero_si512(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtph_epi32(__m512i __W, __mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2dq512_mask(
      (__v16hf)__A, (__v16si)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epi32(__mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2dq512_mask(
      (__v16hf)__A, (__v16si)_mm512_setzero_si512(), (__mmask16)__U,
      0x04);
}
# 1922 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtph_epu32(__m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2udq512_mask(
      (__v16hf)__A, (__v16su)_mm512_setzero_si512(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtph_epu32(__m512i __W, __mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2udq512_mask(
      (__v16hf)__A, (__v16su)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epu32(__mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2udq512_mask(
      (__v16hf)__A, (__v16su)_mm512_setzero_si512(), (__mmask16)__U,
      0x04);
}
# 1955 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtepi32_ph(__m512i __A) {
  return (__m256h)__builtin_ia32_vcvtdq2ph512_mask(
      (__v16si)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_ph(__m256h __W, __mmask16 __U, __m512i __A) {
  return (__m256h)__builtin_ia32_vcvtdq2ph512_mask(
      (__v16si)__A, (__v16hf)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_ph(__mmask16 __U, __m512i __A) {
  return (__m256h)__builtin_ia32_vcvtdq2ph512_mask(
      (__v16si)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 1988 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtepu32_ph(__m512i __A) {
  return (__m256h)__builtin_ia32_vcvtudq2ph512_mask(
      (__v16su)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu32_ph(__m256h __W, __mmask16 __U, __m512i __A) {
  return (__m256h)__builtin_ia32_vcvtudq2ph512_mask(
      (__v16su)__A, (__v16hf)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu32_ph(__mmask16 __U, __m512i __A) {
  return (__m256h)__builtin_ia32_vcvtudq2ph512_mask(
      (__v16su)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 2022 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvttph_epi32(__m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2dq512_mask(
      (__v16hf)__A, (__v16si)_mm512_setzero_si512(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttph_epi32(__m512i __W, __mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2dq512_mask(
      (__v16hf)__A, (__v16si)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epi32(__mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2dq512_mask(
      (__v16hf)__A, (__v16si)_mm512_setzero_si512(), (__mmask16)__U,
      0x04);
}
# 2056 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvttph_epu32(__m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2udq512_mask(
      (__v16hf)__A, (__v16su)_mm512_setzero_si512(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttph_epu32(__m512i __W, __mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2udq512_mask(
      (__v16hf)__A, (__v16su)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epu32(__mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2udq512_mask(
      (__v16hf)__A, (__v16su)_mm512_setzero_si512(), (__mmask16)__U,
      0x04);
}
# 2088 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtepi64_ph(__m512i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph512_mask(
      (__v8di)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_ph(__m128h __W, __mmask8 __U, __m512i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph512_mask(
      (__v8di)__A, (__v8hf)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_ph(__mmask8 __U, __m512i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph512_mask(
      (__v8di)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 2121 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtph_epi64(__m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2qq512_mask(
      (__v8hf)__A, (__v8di)_mm512_setzero_si512(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtph_epi64(__m512i __W, __mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2qq512_mask(
      (__v8hf)__A, (__v8di)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epi64(__mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2qq512_mask(
      (__v8hf)__A, (__v8di)_mm512_setzero_si512(), (__mmask8)__U,
      0x04);
}
# 2153 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtepu64_ph(__m512i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph512_mask(
      (__v8du)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtepu64_ph(__m128h __W, __mmask8 __U, __m512i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph512_mask(
      (__v8du)__A, (__v8hf)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtepu64_ph(__mmask8 __U, __m512i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph512_mask(
      (__v8du)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 2186 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvtph_epu64(__m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uqq512_mask(
      (__v8hf)__A, (__v8du)_mm512_setzero_si512(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtph_epu64(__m512i __W, __mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uqq512_mask(
      (__v8hf)__A, (__v8du)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epu64(__mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uqq512_mask(
      (__v8hf)__A, (__v8du)_mm512_setzero_si512(), (__mmask8)__U,
      0x04);
}
# 2219 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvttph_epi64(__m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2qq512_mask(
      (__v8hf)__A, (__v8di)_mm512_setzero_si512(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttph_epi64(__m512i __W, __mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2qq512_mask(
      (__v8hf)__A, (__v8di)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epi64(__mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2qq512_mask(
      (__v8hf)__A, (__v8di)_mm512_setzero_si512(), (__mmask8)__U,
      0x04);
}
# 2252 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_cvttph_epu64(__m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uqq512_mask(
      (__v8hf)__A, (__v8du)_mm512_setzero_si512(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvttph_epu64(__m512i __W, __mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uqq512_mask(
      (__v8hf)__A, (__v8du)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epu64(__mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uqq512_mask(
      (__v8hf)__A, (__v8du)_mm512_setzero_si512(), (__mmask8)__U,
      0x04);
}




static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvtsh_i32(__m128h __A) {
  return (int)__builtin_ia32_vcvtsh2si32((__v8hf)__A, 0x04);
}




static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_cvtsh_u32(__m128h __A) {
  return (unsigned int)__builtin_ia32_vcvtsh2usi32((__v8hf)__A,
                                                   0x04);
}





static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvtsh_i64(__m128h __A) {
  return (long long)__builtin_ia32_vcvtsh2si64((__v8hf)__A,
                                               0x04);
}




static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_cvtsh_u64(__m128h __A) {
  return (unsigned long long)__builtin_ia32_vcvtsh2usi64(
      (__v8hf)__A, 0x04);
}





static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_cvtu32_sh(__m128h __A, unsigned int __B) {
  __A[0] = __B;
  return __A;
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_cvtu64_sh(__m128h __A, unsigned long long __B) {
  __A[0] = __B;
  return __A;
}





static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvti32_sh(__m128h __A,
                                                              int __B) {
  __A[0] = __B;
  return __A;
}





static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvti64_sh(__m128h __A,
                                                              long long __B) {
  __A[0] = __B;
  return __A;
}





static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvttsh_i32(__m128h __A) {
  return (int)__builtin_ia32_vcvttsh2si32((__v8hf)__A,
                                          0x04);
}





static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_cvttsh_i64(__m128h __A) {
  return (long long)__builtin_ia32_vcvttsh2si64((__v8hf)__A,
                                                0x04);
}





static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_cvttsh_u32(__m128h __A) {
  return (unsigned int)__builtin_ia32_vcvttsh2usi32((__v8hf)__A,
                                                    0x04);
}





static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_cvttsh_u64(__m128h __A) {
  return (unsigned long long)__builtin_ia32_vcvttsh2usi64(
      (__v8hf)__A, 0x04);
}
# 2399 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_cvtxph_ps(__m256h __A) {
  return (__m512)__builtin_ia32_vcvtph2psx512_mask(
      (__v16hf)__A, (__v16sf)_mm512_setzero_ps(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtxph_ps(__m512 __W, __mmask16 __U, __m256h __A) {
  return (__m512)__builtin_ia32_vcvtph2psx512_mask(
      (__v16hf)__A, (__v16sf)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtxph_ps(__mmask16 __U, __m256h __A) {
  return (__m512)__builtin_ia32_vcvtph2psx512_mask(
      (__v16hf)__A, (__v16sf)_mm512_setzero_ps(), (__mmask16)__U,
      0x04);
}
# 2431 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_cvtxps_ph(__m512 __A) {
  return (__m256h)__builtin_ia32_vcvtps2phx512_mask(
      (__v16sf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtxps_ph(__m256h __W, __mmask16 __U, __m512 __A) {
  return (__m256h)__builtin_ia32_vcvtps2phx512_mask(
      (__v16sf)__A, (__v16hf)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtxps_ph(__mmask16 __U, __m512 __A) {
  return (__m256h)__builtin_ia32_vcvtps2phx512_mask(
      (__v16sf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 2510 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_fmadd_ph(__m512h __A,
                                                                __m512h __B,
                                                                __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  (__v32hf)__C, (__mmask32)-1,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fmadd_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  (__v32hf)__C, (__mmask32)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmadd_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask3((__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmadd_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_maskz((__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_fmsub_ph(__m512h __A,
                                                                __m512h __B,
                                                                __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  -(__v32hf)__C, (__mmask32)-1,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fmsub_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  -(__v32hf)__C, (__mmask32)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmsub_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_maskz(
      (__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_fnmadd_ph(__m512h __A,
                                                                 __m512h __B,
                                                                 __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, -(__v32hf)__B,
                                                  (__v32hf)__C, (__mmask32)-1,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask3_fnmadd_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask3(-(__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fnmadd_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_maskz(-(__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_fnmsub_ph(__m512h __A,
                                                                 __m512h __B,
                                                                 __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, -(__v32hf)__B,
                                                  -(__v32hf)__C, (__mmask32)-1,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fnmsub_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_maskz(
      -(__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)__U,
      0x04);
}
# 2633 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_fmaddsub_ph(__m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fmaddsub_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmaddsub_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask3(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmaddsub_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_maskz(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_fmsubadd_ph(__m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask(
      (__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fmsubadd_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask(
      (__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmsubadd_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_maskz(
      (__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)__U,
      0x04);
}






static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmsub_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmsubph512_mask3((__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}






static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmsubadd_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmsubaddph512_mask3(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)__U,
      0x04);
}






static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fnmadd_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, -(__v32hf)__B,
                                                  (__v32hf)__C, (__mmask32)__U,
                                                  0x04);
}
# 2728 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fnmsub_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, -(__v32hf)__B,
                                                  -(__v32hf)__C, (__mmask32)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask3_fnmsub_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmsubph512_mask3(-(__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_fmadd_sh(__m128h __W,
                                                             __m128h __A,
                                                             __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, (__v8hf)__A, (__v8hf)__B,
                                       (__mmask8)-1, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_fmadd_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, (__v8hf)__A, (__v8hf)__B,
                                       (__mmask8)__U, 0x04);
}
# 2767 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmadd_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return __builtin_ia32_vfmaddsh3_maskz((__v8hf)__A, (__v8hf)__B, (__v8hf)__C,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmadd_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return __builtin_ia32_vfmaddsh3_mask3((__v8hf)__W, (__v8hf)__X, (__v8hf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_fmsub_sh(__m128h __W,
                                                             __m128h __A,
                                                             __m128h __B) {
  return (__m128h)__builtin_ia32_vfmaddsh3_mask((__v8hf)__W, (__v8hf)__A,
                                                -(__v8hf)__B, (__mmask8)-1,
                                                0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_fmsub_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_vfmaddsh3_mask((__v8hf)__W, (__v8hf)__A,
                                                -(__v8hf)__B, (__mmask8)__U,
                                                0x04);
}
# 2818 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmsub_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddsh3_maskz((__v8hf)__A, (__v8hf)__B,
                                                 -(__v8hf)__C, (__mmask8)__U,
                                                 0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmsub_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return __builtin_ia32_vfmsubsh3_mask3((__v8hf)__W, (__v8hf)__X, (__v8hf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_fnmadd_sh(__m128h __W,
                                                              __m128h __A,
                                                              __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, -(__v8hf)__A, (__v8hf)__B,
                                       (__mmask8)-1, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmadd_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, -(__v8hf)__A, (__v8hf)__B,
                                       (__mmask8)__U, 0x04);
}
# 2865 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmadd_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return __builtin_ia32_vfmaddsh3_maskz((__v8hf)__A, -(__v8hf)__B, (__v8hf)__C,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmadd_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return __builtin_ia32_vfmaddsh3_mask3((__v8hf)__W, -(__v8hf)__X, (__v8hf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_fnmsub_sh(__m128h __W,
                                                              __m128h __A,
                                                              __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, -(__v8hf)__A, -(__v8hf)__B,
                                       (__mmask8)-1, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmsub_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, -(__v8hf)__A, -(__v8hf)__B,
                                       (__mmask8)__U, 0x04);
}
# 2912 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmsub_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return __builtin_ia32_vfmaddsh3_maskz((__v8hf)__A, -(__v8hf)__B, -(__v8hf)__C,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmsub_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return __builtin_ia32_vfmsubsh3_mask3((__v8hf)__W, -(__v8hf)__X, (__v8hf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_fcmadd_sch(__m128h __A,
                                                               __m128h __B,
                                                               __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcsh_mask((__v4sf)__A, (__v4sf)__B,
                                                 (__v4sf)__C, (__mmask8)-1,
                                                 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask_fcmadd_sch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcsh_round_mask(
      (__v4sf)__A, (__v4sf)(__B), (__v4sf)(__C), __U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fcmadd_sch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcsh_maskz((__v4sf)__A, (__v4sf)__B,
                                                  (__v4sf)__C, (__mmask8)__U,
                                                  0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fcmadd_sch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_vfcmaddcsh_round_mask3(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)__C, __U, 0x04);
}
# 2983 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_fmadd_sch(__m128h __A,
                                                              __m128h __B,
                                                              __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcsh_mask((__v4sf)__A, (__v4sf)__B,
                                                (__v4sf)__C, (__mmask8)-1,
                                                0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmadd_sch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcsh_round_mask(
      (__v4sf)__A, (__v4sf)(__B), (__v4sf)(__C), __U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmadd_sch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcsh_maskz((__v4sf)__A, (__v4sf)__B,
                                                 (__v4sf)__C, (__mmask8)__U,
                                                 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmadd_sch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_vfmaddcsh_round_mask3(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)__C, __U, 0x04);
}
# 3030 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_fcmul_sch(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcsh_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_undefined_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_mask_fcmul_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcsh_mask((__v4sf)__A, (__v4sf)__B,
                                                (__v4sf)__W, (__mmask8)__U,
                                                0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fcmul_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcsh_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 3066 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_fmul_sch(__m128h __A,
                                                             __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcsh_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_undefined_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128))) _mm_mask_fmul_sch(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcsh_mask((__v4sf)__A, (__v4sf)__B,
                                               (__v4sf)__W, (__mmask8)__U,
                                               0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmul_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcsh_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 3104 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_fcmul_pch(__m512h __A,
                                                                 __m512h __B) {
  return (__m512h)__builtin_ia32_vfcmulcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)_mm512_undefined_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fcmul_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_vfcmulcph512_mask((__v16sf)__A, (__v16sf)__B,
                                                   (__v16sf)__W, (__mmask16)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fcmul_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_vfcmulcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)_mm512_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 3140 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_fmul_pch(__m512h __A,
                                                                __m512h __B) {
  return (__m512h)__builtin_ia32_vfmulcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)_mm512_undefined_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fmul_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_vfmulcph512_mask((__v16sf)__A, (__v16sf)__B,
                                                  (__v16sf)__W, (__mmask16)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmul_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_vfmulcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)_mm512_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 3176 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_fcmadd_pch(__m512h __A,
                                                                  __m512h __B,
                                                                  __m512h __C) {
  return (__m512h)__builtin_ia32_vfcmaddcph512_mask3(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fcmadd_pch(__m512h __A, __mmask16 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfcmaddcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask3_fcmadd_pch(__m512h __A, __m512h __B, __m512h __C, __mmask16 __U) {
  return (__m512h)__builtin_ia32_vfcmaddcph512_mask3(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fcmadd_pch(__mmask16 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfcmaddcph512_maskz(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}
# 3225 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512))) _mm512_fmadd_pch(__m512h __A,
                                                                 __m512h __B,
                                                                 __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddcph512_mask3((__v16sf)__A, (__v16sf)__B,
                                                    (__v16sf)__C, (__mmask16)-1,
                                                    0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_fmadd_pch(__m512h __A, __mmask16 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddcph512_mask((__v16sf)__A, (__v16sf)__B,
                                                   (__v16sf)__C, (__mmask16)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask3_fmadd_pch(__m512h __A, __m512h __B, __m512h __C, __mmask16 __U) {
  return (__m512h)__builtin_ia32_vfmaddcph512_mask3(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_maskz_fmadd_pch(__mmask16 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddcph512_maskz(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}
# 3274 "/usr/lib/clang/18/include/avx512fp16intrin.h" 3
static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_reduce_add_ph(__m512h __W) {
  return __builtin_ia32_reduce_fadd_ph512(-0.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_reduce_mul_ph(__m512h __W) {
  return __builtin_ia32_reduce_fmul_ph512(1.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_reduce_max_ph(__m512h __V) {
  return __builtin_ia32_reduce_fmax_ph512(__V);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_reduce_min_ph(__m512h __V) {
  return __builtin_ia32_reduce_fmin_ph512(__V);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_mask_blend_ph(__mmask32 __U, __m512h __A, __m512h __W) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U, (__v32hf)__W,
                                              (__v32hf)__A);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_permutex2var_ph(__m512h __A, __m512i __I, __m512h __B) {
  return (__m512h)__builtin_ia32_vpermi2varhi512((__v32hi)__A, (__v32hi)__I,
                                                 (__v32hi)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,evex512"), __min_vector_width__(512)))
_mm512_permutexvar_ph(__m512i __A, __m512h __B) {
  return (__m512h)__builtin_ia32_permvarhi512((__v32hi)__B, (__v32hi)__A);
}
# 225 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vlfp16intrin.h" 1 3
# 29 "/usr/lib/clang/18/include/avx512vlfp16intrin.h" 3
static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtsh_h(__m128h __a) {
  return __a[0];
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_cvtsh_h(__m256h __a) {
  return __a[0];
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_set_sh(_Float16 __h) {
  return __extension__(__m128h){__h, 0, 0, 0, 0, 0, 0, 0};
}

static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_set1_ph(_Float16 __h) {
  return (__m128h)(__v8hf){__h, __h, __h, __h, __h, __h, __h, __h};
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_set1_ph(_Float16 __h) {
  return (__m256h)(__v16hf){__h, __h, __h, __h, __h, __h, __h, __h,
                            __h, __h, __h, __h, __h, __h, __h, __h};
}

static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_set_ph(_Float16 __h1, _Float16 __h2, _Float16 __h3, _Float16 __h4,
           _Float16 __h5, _Float16 __h6, _Float16 __h7, _Float16 __h8) {
  return (__m128h)(__v8hf){__h8, __h7, __h6, __h5, __h4, __h3, __h2, __h1};
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_set1_pch(_Float16 _Complex h) {
  return (__m256h)_mm256_set1_ps(__builtin_bit_cast(float, h));
}

static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_set1_pch(_Float16 _Complex h) {
  return (__m128h)_mm_set1_ps(__builtin_bit_cast(float, h));
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_set_ph(_Float16 __h1, _Float16 __h2, _Float16 __h3, _Float16 __h4,
              _Float16 __h5, _Float16 __h6, _Float16 __h7, _Float16 __h8,
              _Float16 __h9, _Float16 __h10, _Float16 __h11, _Float16 __h12,
              _Float16 __h13, _Float16 __h14, _Float16 __h15, _Float16 __h16) {
  return (__m256h)(__v16hf){__h16, __h15, __h14, __h13, __h12, __h11,
                            __h10, __h9, __h8, __h7, __h6, __h5,
                            __h4, __h3, __h2, __h1};
}
# 84 "/usr/lib/clang/18/include/avx512vlfp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_add_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)((__v16hf)__A + (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_add_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_add_ph(__A, __B), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_add_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_add_ph(__A, __B), (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_add_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)((__v8hf)__A + (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_add_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_add_ph(__A, __B),
                                              (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_maskz_add_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_add_ph(__A, __B),
                                              (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_sub_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)((__v16hf)__A - (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sub_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_sub_ph(__A, __B), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sub_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_sub_ph(__A, __B), (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_sub_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)((__v8hf)__A - (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_sub_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_sub_ph(__A, __B),
                                              (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_maskz_sub_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_sub_ph(__A, __B),
                                              (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_mul_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)((__v16hf)__A * (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_mul_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_mul_ph(__A, __B), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_mul_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_mul_ph(__A, __B), (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mul_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)((__v8hf)__A * (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_mul_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_mul_ph(__A, __B),
                                              (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_maskz_mul_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_mul_ph(__A, __B),
                                              (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_div_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)((__v16hf)__A / (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_div_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_div_ph(__A, __B), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_div_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_div_ph(__A, __B), (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_div_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)((__v8hf)__A / (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_div_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_div_ph(__A, __B),
                                              (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_maskz_div_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_div_ph(__A, __B),
                                              (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_min_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)__builtin_ia32_minph256((__v16hf)__A, (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_min_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      (__v16hf)__builtin_ia32_minph256((__v16hf)__A, (__v16hf)__B),
      (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_min_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      (__v16hf)__builtin_ia32_minph256((__v16hf)__A, (__v16hf)__B),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_min_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_minph128((__v8hf)__A, (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_min_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)__builtin_ia32_minph128((__v8hf)__A, (__v8hf)__B),
      (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_maskz_min_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)__builtin_ia32_minph128((__v8hf)__A, (__v8hf)__B),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_max_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)__builtin_ia32_maxph256((__v16hf)__A, (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_max_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      (__v16hf)__builtin_ia32_maxph256((__v16hf)__A, (__v16hf)__B),
      (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_max_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      (__v16hf)__builtin_ia32_maxph256((__v16hf)__A, (__v16hf)__B),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_max_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_maxph128((__v8hf)__A, (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_max_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)__builtin_ia32_maxph128((__v8hf)__A, (__v8hf)__B),
      (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_maskz_max_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)__builtin_ia32_maxph128((__v8hf)__A, (__v8hf)__B),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_abs_ph(__m256h __A) {
  return (__m256h)_mm256_and_epi32(_mm256_set1_epi32(0x7FFF7FFF), (__m256i)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_abs_ph(__m128h __A) {
  return (__m128h)_mm_and_epi32(_mm_set1_epi32(0x7FFF7FFF), (__m128i)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_conj_pch(__m256h __A) {
  return (__m256h)_mm256_xor_ps((__m256)__A, _mm256_set1_ps(-0.0f));
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_conj_pch(__m256h __W, __mmask8 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_selectps_256(
      (__mmask8)__U, (__v8sf)_mm256_conj_pch(__A), (__v8sf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_conj_pch(__mmask8 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_selectps_256(
      (__mmask8)__U, (__v8sf)_mm256_conj_pch(__A), (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_conj_pch(__m128h __A) {
  return (__m128h)_mm_xor_ps((__m128)__A, _mm_set1_ps(-0.0f));
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_conj_pch(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A) {
  return (__m128h)__builtin_ia32_selectps_128(
      (__mmask8)__U, (__v4sf)_mm_conj_pch(__A), (__v4sf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_conj_pch(__mmask8 __U, __m128h __A) {
  return (__m128h)__builtin_ia32_selectps_128(
      (__mmask8)__U, (__v4sf)_mm_conj_pch(__A), (__v4sf)_mm_setzero_ps());
}
# 375 "/usr/lib/clang/18/include/avx512vlfp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_rcp_ph(__m256h __A) {
  return (__m256h)__builtin_ia32_rcpph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_undefined_ph(), (__mmask16)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rcp_ph(__m256h __W, __mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_rcpph256_mask((__v16hf)__A, (__v16hf)__W,
                                               (__mmask16)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rcp_ph(__mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_rcpph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_rcp_ph(__m128h __A) {
  return (__m128h)__builtin_ia32_rcpph128_mask(
      (__v8hf)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_rcp_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A) {
  return (__m128h)__builtin_ia32_rcpph128_mask((__v8hf)__A, (__v8hf)__W,
                                               (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_maskz_rcp_ph(__mmask8 __U,
                                                                 __m128h __A) {
  return (__m128h)__builtin_ia32_rcpph128_mask(
      (__v8hf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_rsqrt_ph(__m256h __A) {
  return (__m256h)__builtin_ia32_rsqrtph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_undefined_ph(), (__mmask16)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_rsqrt_ph(__m256h __W, __mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_rsqrtph256_mask((__v16hf)__A, (__v16hf)__W,
                                                 (__mmask16)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_rsqrt_ph(__mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_rsqrtph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_rsqrt_ph(__m128h __A) {
  return (__m128h)__builtin_ia32_rsqrtph128_mask(
      (__v8hf)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_rsqrt_ph(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A) {
  return (__m128h)__builtin_ia32_rsqrtph128_mask((__v8hf)__A, (__v8hf)__W,
                                                 (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_rsqrt_ph(__mmask8 __U, __m128h __A) {
  return (__m128h)__builtin_ia32_rsqrtph128_mask(
      (__v8hf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_getexp_ph(__m128h __A) {
  return (__m128h)__builtin_ia32_getexpph128_mask(
      (__v8hf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_getexp_ph(__m128h __W, __mmask8 __U, __m128h __A) {
  return (__m128h)__builtin_ia32_getexpph128_mask((__v8hf)__A, (__v8hf)__W,
                                                  (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_getexp_ph(__mmask8 __U, __m128h __A) {
  return (__m128h)__builtin_ia32_getexpph128_mask(
      (__v8hf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_getexp_ph(__m256h __A) {
  return (__m256h)__builtin_ia32_getexpph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_getexp_ph(__m256h __W, __mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_getexpph256_mask((__v16hf)__A, (__v16hf)__W,
                                                  (__mmask16)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_getexp_ph(__mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_getexpph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U);
}
# 509 "/usr/lib/clang/18/include/avx512vlfp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_scalef_ph(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_scalefph128_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_scalef_ph(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_scalefph128_mask((__v8hf)__A, (__v8hf)__B,
                                                  (__v8hf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_scalef_ph(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_scalefph128_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_scalef_ph(__m256h __A,
                                                                 __m256h __B) {
  return (__m256h)__builtin_ia32_scalefph256_mask(
      (__v16hf)__A, (__v16hf)__B, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_scalef_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_scalefph256_mask((__v16hf)__A, (__v16hf)__B,
                                                  (__v16hf)__W, (__mmask16)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_scalef_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_scalefph256_mask(
      (__v16hf)__A, (__v16hf)__B, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U);
}
# 603 "/usr/lib/clang/18/include/avx512vlfp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_sqrt_ph(__m128h __a) {
  return __builtin_ia32_sqrtph((__v8hf)__a);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_sqrt_ph(__m128h __W,
                                                                 __mmask8 __U,
                                                                 __m128h __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_sqrt_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_maskz_sqrt_ph(__mmask8 __U,
                                                                  __m128h __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_sqrt_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_sqrt_ph(__m256h __a) {
  return (__m256h)__builtin_ia32_sqrtph256((__v16hf)__a);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_sqrt_ph(__m256h __W, __mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U, (__v16hf)_mm256_sqrt_ph(__A), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_sqrt_ph(__mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_selectph_256((__mmask16)__U,
                                              (__v16hf)_mm256_sqrt_ph(__A),
                                              (__v16hf)_mm256_setzero_ph());
}
# 653 "/usr/lib/clang/18/include/avx512vlfp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtpd_ph(__m128d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph128_mask(
      (__v2df)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_cvtpd_ph(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph128_mask((__v2df)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtpd_ph(__mmask8 __U, __m128d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph128_mask(
      (__v2df)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_cvtpd_ph(__m256d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph256_mask(
      (__v4df)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtpd_ph(__m128h __W, __mmask8 __U, __m256d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph256_mask((__v4df)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_ph(__mmask8 __U, __m256d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph256_mask(
      (__v4df)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtph_pd(__m128h __A) {
  return (__m128d)__builtin_ia32_vcvtph2pd128_mask(
      (__v8hf)__A, (__v2df)_mm_undefined_pd(), (__mmask8)-1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_cvtph_pd(__m128d __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A) {
  return (__m128d)__builtin_ia32_vcvtph2pd128_mask((__v8hf)__A, (__v2df)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtph_pd(__mmask8 __U, __m128h __A) {
  return (__m128d)__builtin_ia32_vcvtph2pd128_mask(
      (__v8hf)__A, (__v2df)_mm_setzero_pd(), (__mmask8)__U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_cvtph_pd(__m128h __A) {
  return (__m256d)__builtin_ia32_vcvtph2pd256_mask(
      (__v8hf)__A, (__v4df)_mm256_undefined_pd(), (__mmask8)-1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtph_pd(__m256d __W, __mmask8 __U, __m128h __A) {
  return (__m256d)__builtin_ia32_vcvtph2pd256_mask((__v8hf)__A, (__v4df)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtph_pd(__mmask8 __U, __m128h __A) {
  return (__m256d)__builtin_ia32_vcvtph2pd256_mask(
      (__v8hf)__A, (__v4df)_mm256_setzero_pd(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtph_epi16(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2w128_mask(
      (__v8hf)__A, (__v8hi)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtph_epi16(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2w128_mask((__v8hf)__A, (__v8hi)__W,
                                                  (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtph_epi16(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2w128_mask(
      (__v8hf)__A, (__v8hi)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtph_epi16(__m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2w256_mask(
      (__v16hf)__A, (__v16hi)_mm256_undefined_si256(), (__mmask16)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtph_epi16(__m256i __W, __mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2w256_mask((__v16hf)__A, (__v16hi)__W,
                                                  (__mmask16)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epi16(__mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2w256_mask(
      (__v16hf)__A, (__v16hi)_mm256_setzero_si256(), (__mmask16)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvttph_epi16(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2w128_mask(
      (__v8hf)__A, (__v8hi)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttph_epi16(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2w128_mask((__v8hf)__A, (__v8hi)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttph_epi16(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2w128_mask(
      (__v8hf)__A, (__v8hi)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvttph_epi16(__m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2w256_mask(
      (__v16hf)__A, (__v16hi)_mm256_undefined_si256(), (__mmask16)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttph_epi16(__m256i __W, __mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2w256_mask((__v16hf)__A, (__v16hi)__W,
                                                   (__mmask16)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epi16(__mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2w256_mask(
      (__v16hf)__A, (__v16hi)_mm256_setzero_si256(), (__mmask16)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtepi16_ph(__m128i __A) {
  return (__m128h) __builtin_convertvector((__v8hi)__A, __v8hf);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi16_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_cvtepi16_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi16_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_cvtepi16_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi16_ph(__m256i __A) {
  return (__m256h) __builtin_convertvector((__v16hi)__A, __v16hf);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi16_ph(__m256h __W, __mmask16 __U, __m256i __A) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U, (__v16hf)_mm256_cvtepi16_ph(__A), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi16_ph(__mmask16 __U, __m256i __A) {
  return (__m256h)__builtin_ia32_selectph_256((__mmask16)__U,
                                              (__v16hf)_mm256_cvtepi16_ph(__A),
                                              (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtph_epu16(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uw128_mask(
      (__v8hf)__A, (__v8hu)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtph_epu16(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uw128_mask((__v8hf)__A, (__v8hu)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtph_epu16(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uw128_mask(
      (__v8hf)__A, (__v8hu)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtph_epu16(__m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uw256_mask(
      (__v16hf)__A, (__v16hu)_mm256_undefined_si256(), (__mmask16)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtph_epu16(__m256i __W, __mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uw256_mask((__v16hf)__A, (__v16hu)__W,
                                                   (__mmask16)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epu16(__mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uw256_mask(
      (__v16hf)__A, (__v16hu)_mm256_setzero_si256(), (__mmask16)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvttph_epu16(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uw128_mask(
      (__v8hf)__A, (__v8hu)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttph_epu16(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uw128_mask((__v8hf)__A, (__v8hu)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttph_epu16(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uw128_mask(
      (__v8hf)__A, (__v8hu)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvttph_epu16(__m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uw256_mask(
      (__v16hf)__A, (__v16hu)_mm256_undefined_si256(), (__mmask16)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttph_epu16(__m256i __W, __mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uw256_mask((__v16hf)__A, (__v16hu)__W,
                                                    (__mmask16)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epu16(__mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uw256_mask(
      (__v16hf)__A, (__v16hu)_mm256_setzero_si256(), (__mmask16)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtepu16_ph(__m128i __A) {
  return (__m128h) __builtin_convertvector((__v8hu)__A, __v8hf);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepu16_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_cvtepu16_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepu16_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_cvtepu16_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu16_ph(__m256i __A) {
  return (__m256h) __builtin_convertvector((__v16hu)__A, __v16hf);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepu16_ph(__m256h __W, __mmask16 __U, __m256i __A) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U, (__v16hf)_mm256_cvtepu16_ph(__A), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepu16_ph(__mmask16 __U, __m256i __A) {
  return (__m256h)__builtin_ia32_selectph_256((__mmask16)__U,
                                              (__v16hf)_mm256_cvtepu16_ph(__A),
                                              (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtph_epi32(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2dq128_mask(
      (__v8hf)__A, (__v4si)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtph_epi32(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2dq128_mask((__v8hf)__A, (__v4si)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtph_epi32(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2dq128_mask(
      (__v8hf)__A, (__v4si)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtph_epi32(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2dq256_mask(
      (__v8hf)__A, (__v8si)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtph_epi32(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2dq256_mask((__v8hf)__A, (__v8si)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epi32(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2dq256_mask(
      (__v8hf)__A, (__v8si)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtph_epu32(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2udq128_mask(
      (__v8hf)__A, (__v4su)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtph_epu32(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2udq128_mask((__v8hf)__A, (__v4su)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtph_epu32(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2udq128_mask(
      (__v8hf)__A, (__v4su)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtph_epu32(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2udq256_mask(
      (__v8hf)__A, (__v8su)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtph_epu32(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2udq256_mask((__v8hf)__A, (__v8su)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epu32(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2udq256_mask(
      (__v8hf)__A, (__v8su)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtepi32_ph(__m128i __A) {
  return (__m128h)__builtin_ia32_vcvtdq2ph128_mask(
      (__v4si)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi32_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtdq2ph128_mask((__v4si)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtdq2ph128_mask(
      (__v4si)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi32_ph(__m256i __A) {
  return (__m128h) __builtin_convertvector((__v8si)__A, __v8hf);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_ph(__m128h __W, __mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm256_cvtepi32_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_ph(__mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm256_cvtepi32_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtepu32_ph(__m128i __A) {
  return (__m128h)__builtin_ia32_vcvtudq2ph128_mask(
      (__v4su)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepu32_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtudq2ph128_mask((__v4su)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepu32_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtudq2ph128_mask(
      (__v4su)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu32_ph(__m256i __A) {
  return (__m128h) __builtin_convertvector((__v8su)__A, __v8hf);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepu32_ph(__m128h __W, __mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm256_cvtepu32_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepu32_ph(__mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm256_cvtepu32_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvttph_epi32(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2dq128_mask(
      (__v8hf)__A, (__v4si)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttph_epi32(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2dq128_mask((__v8hf)__A, (__v4si)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttph_epi32(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2dq128_mask(
      (__v8hf)__A, (__v4si)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvttph_epi32(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2dq256_mask(
      (__v8hf)__A, (__v8si)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttph_epi32(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2dq256_mask((__v8hf)__A, (__v8si)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epi32(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2dq256_mask(
      (__v8hf)__A, (__v8si)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvttph_epu32(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2udq128_mask(
      (__v8hf)__A, (__v4su)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttph_epu32(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2udq128_mask((__v8hf)__A, (__v4su)__W,
                                                     (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttph_epu32(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2udq128_mask(
      (__v8hf)__A, (__v4su)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvttph_epu32(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2udq256_mask(
      (__v8hf)__A, (__v8su)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttph_epu32(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2udq256_mask((__v8hf)__A, (__v8su)__W,
                                                     (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epu32(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2udq256_mask(
      (__v8hf)__A, (__v8su)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtepi64_ph(__m128i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph128_mask(
      (__v2di)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepi64_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph128_mask((__v2di)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph128_mask(
      (__v2di)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepi64_ph(__m256i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph256_mask(
      (__v4di)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_ph(__m128h __W, __mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph256_mask((__v4di)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_ph(__mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph256_mask(
      (__v4di)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtph_epi64(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2qq128_mask(
      (__v8hf)__A, (__v2di)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtph_epi64(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2qq128_mask((__v8hf)__A, (__v2di)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtph_epi64(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2qq128_mask(
      (__v8hf)__A, (__v2di)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtph_epi64(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2qq256_mask(
      (__v8hf)__A, (__v4di)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtph_epi64(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2qq256_mask((__v8hf)__A, (__v4di)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epi64(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2qq256_mask(
      (__v8hf)__A, (__v4di)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtepu64_ph(__m128i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph128_mask(
      (__v2du)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtepu64_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph128_mask((__v2du)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtepu64_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph128_mask(
      (__v2du)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtepu64_ph(__m256i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph256_mask(
      (__v4du)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtepu64_ph(__m128h __W, __mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph256_mask((__v4du)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtepu64_ph(__mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph256_mask(
      (__v4du)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtph_epu64(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uqq128_mask(
      (__v8hf)__A, (__v2du)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtph_epu64(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uqq128_mask((__v8hf)__A, (__v2du)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtph_epu64(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uqq128_mask(
      (__v8hf)__A, (__v2du)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvtph_epu64(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uqq256_mask(
      (__v8hf)__A, (__v4du)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtph_epu64(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uqq256_mask((__v8hf)__A, (__v4du)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epu64(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uqq256_mask(
      (__v8hf)__A, (__v4du)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvttph_epi64(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2qq128_mask(
      (__v8hf)__A, (__v2di)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttph_epi64(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2qq128_mask((__v8hf)__A, (__v2di)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttph_epi64(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2qq128_mask(
      (__v8hf)__A, (__v2di)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvttph_epi64(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2qq256_mask(
      (__v8hf)__A, (__v4di)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttph_epi64(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2qq256_mask((__v8hf)__A, (__v4di)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epi64(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2qq256_mask(
      (__v8hf)__A, (__v4di)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvttph_epu64(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uqq128_mask(
      (__v8hf)__A, (__v2du)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvttph_epu64(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uqq128_mask((__v8hf)__A, (__v2du)__W,
                                                     (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvttph_epu64(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uqq128_mask(
      (__v8hf)__A, (__v2du)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_cvttph_epu64(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uqq256_mask(
      (__v8hf)__A, (__v4du)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvttph_epu64(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uqq256_mask((__v8hf)__A, (__v4du)__W,
                                                     (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epu64(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uqq256_mask(
      (__v8hf)__A, (__v4du)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtxph_ps(__m128h __A) {
  return (__m128)__builtin_ia32_vcvtph2psx128_mask(
      (__v8hf)__A, (__v4sf)_mm_undefined_ps(), (__mmask8)-1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_cvtxph_ps(__m128 __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A) {
  return (__m128)__builtin_ia32_vcvtph2psx128_mask((__v8hf)__A, (__v4sf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtxph_ps(__mmask8 __U, __m128h __A) {
  return (__m128)__builtin_ia32_vcvtph2psx128_mask(
      (__v8hf)__A, (__v4sf)_mm_setzero_ps(), (__mmask8)__U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_cvtxph_ps(__m128h __A) {
  return (__m256)__builtin_ia32_vcvtph2psx256_mask(
      (__v8hf)__A, (__v8sf)_mm256_undefined_ps(), (__mmask8)-1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtxph_ps(__m256 __W, __mmask8 __U, __m128h __A) {
  return (__m256)__builtin_ia32_vcvtph2psx256_mask((__v8hf)__A, (__v8sf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtxph_ps(__mmask8 __U, __m128h __A) {
  return (__m256)__builtin_ia32_vcvtph2psx256_mask(
      (__v8hf)__A, (__v8sf)_mm256_setzero_ps(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_cvtxps_ph(__m128 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx128_mask(
      (__v4sf)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_cvtxps_ph(__m128h __W,
                                                                   __mmask8 __U,
                                                                   __m128 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx128_mask((__v4sf)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtxps_ph(__mmask8 __U, __m128 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx128_mask(
      (__v4sf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_cvtxps_ph(__m256 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx256_mask(
      (__v8sf)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtxps_ph(__m128h __W, __mmask8 __U, __m256 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx256_mask((__v8sf)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtxps_ph(__mmask8 __U, __m256 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx256_mask(
      (__v8sf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fmadd_ph(__m128h __A,
                                                             __m128h __B,
                                                             __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B,
                                          (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_fmadd_ph(__m128h __A,
                                                                  __mmask8 __U,
                                                                  __m128h __B,
                                                                  __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmadd_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmadd_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fmsub_ph(__m128h __A,
                                                             __m128h __B,
                                                             __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B,
                                          -(__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_fmsub_ph(__m128h __A,
                                                                  __mmask8 __U,
                                                                  __m128h __B,
                                                                  __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, _mm_fmsub_ph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmsub_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, _mm_fmsub_ph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmadd_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph(-(__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmadd_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph(-(__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fnmsub_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph(-(__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_fmadd_ph(__m256h __A,
                                                                __m256h __B,
                                                                __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B,
                                             (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmadd_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmadd_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmadd_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_fmsub_ph(__m256h __A,
                                                                __m256h __B,
                                                                __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B,
                                             -(__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmsub_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmsub_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fnmadd_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256(-(__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fnmadd_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256(-(__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fnmsub_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256(-(__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fmaddsub_ph(__m128h __A,
                                                                __m128h __B,
                                                                __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B,
                                             (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmaddsub_ph(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmaddsub_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmaddsub_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fmsubadd_ph(__m128h __A,
                                                                __m128h __B,
                                                                __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B,
                                             -(__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmsubadd_ph(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmsubadd_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_fmaddsub_ph(__m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B,
                                                (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmaddsub_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmaddsub_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmaddsub_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_fmsubadd_ph(__m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B,
                                                -(__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmsubadd_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmsubadd_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmsub_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmsub_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmsubadd_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmsubadd_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fnmadd_ph(__m128h __A,
                                                              __m128h __B,
                                                              __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B,
                                          (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmadd_ph(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B, (__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_fnmadd_ph(__m256h __A,
                                                                 __m256h __B,
                                                                 __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B,
                                             (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fnmadd_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B, (__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fnmsub_ph(__m128h __A,
                                                              __m128h __B,
                                                              __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B,
                                          -(__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fnmsub_ph(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fnmsub_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_fnmsub_ph(__m256h __A,
                                                                 __m256h __B,
                                                                 __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B,
                                             -(__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fnmsub_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fnmsub_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fcmul_pch(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcph128_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fcmul_pch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                   (__v4sf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fcmul_pch(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcph128_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm256_fcmul_pch(__m256h __A,
                                                                 __m256h __B) {
  return (__m256h)__builtin_ia32_vfcmulcph256_mask(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)_mm256_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fcmul_pch(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_vfcmulcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                   (__v8sf)__W, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fcmul_pch(__mmask8 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_vfcmulcph256_mask(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)_mm256_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fcmadd_pch(__m128h __A,
                                                               __m128h __B,
                                                               __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                    (__v4sf)__C, (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fcmadd_pch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectps_128(
      __U,
      __builtin_ia32_vfcmaddcph128_mask((__v4sf)__A, (__v4sf)(__m128h)__B,
                                        (__v4sf)__C, (__mmask8)__U),
      (__v4sf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fcmadd_pch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_vfcmaddcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                    (__v4sf)__C, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fcmadd_pch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcph128_maskz(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)__C, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_fcmadd_pch(__m256h __A,
                                                                  __m256h __B,
                                                                  __m256h __C) {
  return (__m256h)__builtin_ia32_vfcmaddcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                    (__v8sf)__C, (__mmask8)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fcmadd_pch(__m256h __A, __mmask8 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectps_256(
      __U,
      __builtin_ia32_vfcmaddcph256_mask((__v8sf)__A, (__v8sf)__B, (__v8sf)__C,
                                        (__mmask8)__U),
      (__v8sf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fcmadd_pch(__m256h __A, __m256h __B, __m256h __C, __mmask8 __U) {
  return (__m256h)__builtin_ia32_vfcmaddcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                    (__v8sf)__C, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fcmadd_pch(__mmask8 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_vfcmaddcph256_maskz(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)__C, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fmul_pch(__m128h __A,
                                                             __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcph128_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_fmul_pch(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                  (__v4sf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmul_pch(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcph128_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_fmul_pch(__m256h __A,
                                                                __m256h __B) {
  return (__m256h)__builtin_ia32_vfmulcph256_mask(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)_mm256_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmul_pch(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_vfmulcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                  (__v8sf)__W, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmul_pch(__mmask8 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_vfmulcph256_mask(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)_mm256_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_fmadd_pch(__m128h __A,
                                                              __m128h __B,
                                                              __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                   (__v4sf)__C, (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask_fmadd_pch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectps_128(
      __U,
      __builtin_ia32_vfmaddcph128_mask((__v4sf)__A, (__v4sf)__B, (__v4sf)__C,
                                       (__mmask8)__U),
      (__v4sf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_mask3_fmadd_pch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_vfmaddcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                   (__v4sf)__C, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_maskz_fmadd_pch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcph128_maskz((__v4sf)__A, (__v4sf)__B,
                                                    (__v4sf)__C, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256))) _mm256_fmadd_pch(__m256h __A,
                                                                 __m256h __B,
                                                                 __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                   (__v8sf)__C, (__mmask8)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_fmadd_pch(__m256h __A, __mmask8 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectps_256(
      __U,
      __builtin_ia32_vfmaddcph256_mask((__v8sf)__A, (__v8sf)__B, (__v8sf)__C,
                                       (__mmask8)__U),
      (__v8sf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask3_fmadd_pch(__m256h __A, __m256h __B, __m256h __C, __mmask8 __U) {
  return (__m256h)__builtin_ia32_vfmaddcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                   (__v8sf)__C, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_fmadd_pch(__mmask8 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddcph256_maskz((__v8sf)__A, (__v8sf)__B,
                                                    (__v8sf)__C, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128))) _mm_mask_blend_ph(__mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __W) {
  return (__m128h)__builtin_ia32_selectph_128((__mmask8)__U, (__v8hf)__W,
                                              (__v8hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_mask_blend_ph(__mmask16 __U, __m256h __A, __m256h __W) {
  return (__m256h)__builtin_ia32_selectph_256((__mmask16)__U, (__v16hf)__W,
                                              (__v16hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_permutex2var_ph(__m128h __A, __m128i __I, __m128h __B) {
  return (__m128h)__builtin_ia32_vpermi2varhi128((__v8hi)__A, (__v8hi)__I,
                                                 (__v8hi)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_permutex2var_ph(__m256h __A, __m256i __I, __m256h __B) {
  return (__m256h)__builtin_ia32_vpermi2varhi256((__v16hi)__A, (__v16hi)__I,
                                                 (__v16hi)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_permutexvar_ph(__m128i __A, __m128h __B) {
  return (__m128h)__builtin_ia32_permvarhi128((__v8hi)__B, (__v8hi)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_permutexvar_ph(__m256i __A, __m256h __B) {
  return (__m256h)__builtin_ia32_permvarhi256((__v16hi)__B, (__v16hi)__A);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_add_ph(__m256h __W) {
  return __builtin_ia32_reduce_fadd_ph256(-0.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_mul_ph(__m256h __W) {
  return __builtin_ia32_reduce_fmul_ph256(1.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_max_ph(__m256h __V) {
  return __builtin_ia32_reduce_fmax_ph256(__V);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(256)))
_mm256_reduce_min_ph(__m256h __V) {
  return __builtin_ia32_reduce_fmin_ph256(__V);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_reduce_add_ph(__m128h __W) {
  return __builtin_ia32_reduce_fadd_ph128(-0.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_reduce_mul_ph(__m128h __W) {
  return __builtin_ia32_reduce_fmul_ph128(1.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_reduce_max_ph(__m128h __V) {
  return __builtin_ia32_reduce_fmax_ph128(__V);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16,avx512vl,no-evex512"), __min_vector_width__(128)))
_mm_reduce_min_ph(__m128h __V) {
  return __builtin_ia32_reduce_fmin_ph128(__V);
}
# 230 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512bf16intrin.h" 1 3
# 18 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
typedef __bf16 __v32bf __attribute__((__vector_size__(64), __aligned__(64)));
typedef __bf16 __m512bh __attribute__((__vector_size__(64), __aligned__(64)));
typedef __bf16 __bfloat16 __attribute__((deprecated("use __bf16 instead")));
# 39 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,no-evex512"))) _mm_cvtsbh_ss(__bf16 __A) {
  return __builtin_ia32_cvtsbf162ss_32(__A);
}
# 55 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m512bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_cvtne2ps_pbh(__m512 __A, __m512 __B) {
  return (__m512bh)__builtin_ia32_cvtne2ps2bf16_512((__v16sf) __A,
                                                    (__v16sf) __B);
}
# 78 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m512bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtne2ps_pbh(__m512bh __W, __mmask32 __U, __m512 __A, __m512 __B) {
  return (__m512bh)__builtin_ia32_selectpbf_512((__mmask32)__U,
                                        (__v32bf)_mm512_cvtne2ps_pbh(__A, __B),
                                        (__v32bf)__W);
}
# 100 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m512bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtne2ps_pbh(__mmask32 __U, __m512 __A, __m512 __B) {
  return (__m512bh)__builtin_ia32_selectpbf_512((__mmask32)__U,
                                        (__v32bf)_mm512_cvtne2ps_pbh(__A, __B),
                                        (__v32bf)_mm512_setzero_si512());
}
# 116 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_cvtneps_pbh(__m512 __A) {
  return (__m256bh)__builtin_ia32_cvtneps2bf16_512_mask((__v16sf)__A,
                                              (__v16bf)_mm256_undefined_si256(),
                                              (__mmask16)-1);
}
# 137 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtneps_pbh(__m256bh __W, __mmask16 __U, __m512 __A) {
  return (__m256bh)__builtin_ia32_cvtneps2bf16_512_mask((__v16sf)__A,
                                                        (__v16bf)__W,
                                                        (__mmask16)__U);
}
# 156 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtneps_pbh(__mmask16 __U, __m512 __A) {
  return (__m256bh)__builtin_ia32_cvtneps2bf16_512_mask((__v16sf)__A,
                                                (__v16bf)_mm256_setzero_si256(),
                                                (__mmask16)__U);
}
# 177 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_dpbf16_ps(__m512 __D, __m512bh __A, __m512bh __B) {
  return (__m512)__builtin_ia32_dpbf16ps_512((__v16sf) __D,
                                             (__v32bf) __A,
                                             (__v32bf) __B);
}
# 201 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_mask_dpbf16_ps(__m512 __D, __mmask16 __U, __m512bh __A, __m512bh __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                       (__v16sf)_mm512_dpbf16_ps(__D, __A, __B),
                                       (__v16sf)__D);
}
# 225 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_maskz_dpbf16_ps(__mmask16 __U, __m512 __D, __m512bh __A, __m512bh __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                       (__v16sf)_mm512_dpbf16_ps(__D, __A, __B),
                                       (__v16sf)_mm512_setzero_si512());
}
# 239 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512))) _mm512_cvtpbh_ps(__m256bh __A) {
  return _mm512_castsi512_ps((__m512i)_mm512_slli_epi32(
      (__m512i)_mm512_cvtepi16_epi32((__m256i)__A), 16));
}
# 254 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_maskz_cvtpbh_ps(__mmask16 __U, __m256bh __A) {
  return _mm512_castsi512_ps((__m512i)_mm512_slli_epi32(
      (__m512i)_mm512_maskz_cvtepi16_epi32((__mmask16)__U, (__m256i)__A), 16));
}
# 272 "/usr/lib/clang/18/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16,evex512"), __min_vector_width__(512)))
_mm512_mask_cvtpbh_ps(__m512 __S, __mmask16 __U, __m256bh __A) {
  return _mm512_castsi512_ps((__m512i)_mm512_mask_slli_epi32(
      (__m512i)__S, (__mmask16)__U,
      (__m512i)_mm512_cvtepi16_epi32((__m256i)__A), 16));
}
# 235 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 1 3
# 39 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_cvtne2ps_pbh(__m128 __A, __m128 __B) {
  return (__m128bh)__builtin_ia32_cvtne2ps2bf16_128((__v4sf) __A,
                                                    (__v4sf) __B);
}
# 62 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtne2ps_pbh(__m128bh __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128bh)__builtin_ia32_selectpbf_128((__mmask8)__U,
                                             (__v8bf)_mm_cvtne2ps_pbh(__A, __B),
                                             (__v8bf)__W);
}
# 84 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtne2ps_pbh(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128bh)__builtin_ia32_selectpbf_128((__mmask8)__U,
                                             (__v8bf)_mm_cvtne2ps_pbh(__A, __B),
                                             (__v8bf)_mm_setzero_si128());
}
# 103 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_cvtne2ps_pbh(__m256 __A, __m256 __B) {
  return (__m256bh)__builtin_ia32_cvtne2ps2bf16_256((__v8sf) __A,
                                                    (__v8sf) __B);
}
# 126 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtne2ps_pbh(__m256bh __W, __mmask16 __U, __m256 __A, __m256 __B) {
  return (__m256bh)__builtin_ia32_selectpbf_256((__mmask16)__U,
                                         (__v16bf)_mm256_cvtne2ps_pbh(__A, __B),
                                         (__v16bf)__W);
}
# 148 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtne2ps_pbh(__mmask16 __U, __m256 __A, __m256 __B) {
  return (__m256bh)__builtin_ia32_selectpbf_256((__mmask16)__U,
                                         (__v16bf)_mm256_cvtne2ps_pbh(__A, __B),
                                         (__v16bf)_mm256_setzero_si256());
}
# 183 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtneps_pbh(__m128bh __W, __mmask8 __U, __m128 __A) {
  return (__m128bh)__builtin_ia32_cvtneps2bf16_128_mask((__v4sf) __A,
                                                        (__v8bf)__W,
                                                        (__mmask8)__U);
}
# 203 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtneps_pbh(__mmask8 __U, __m128 __A) {
  return (__m128bh)__builtin_ia32_cvtneps2bf16_128_mask((__v4sf) __A,
                                                    (__v8bf)_mm_setzero_si128(),
                                                    (__mmask8)__U);
}
# 236 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtneps_pbh(__m128bh __W, __mmask8 __U, __m256 __A) {
  return (__m128bh)__builtin_ia32_cvtneps2bf16_256_mask((__v8sf)__A,
                                                        (__v8bf)__W,
                                                        (__mmask8)__U);
}
# 255 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtneps_pbh(__mmask8 __U, __m256 __A) {
  return (__m128bh)__builtin_ia32_cvtneps2bf16_256_mask((__v8sf)__A,
                                                    (__v8bf)_mm_setzero_si128(),
                                                    (__mmask8)__U);
}
# 276 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_dpbf16_ps(__m128 __D, __m128bh __A, __m128bh __B) {
  return (__m128)__builtin_ia32_dpbf16ps_128((__v4sf)__D,
                                             (__v8bf)__A,
                                             (__v8bf)__B);
}
# 300 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_mask_dpbf16_ps(__m128 __D, __mmask8 __U, __m128bh __A, __m128bh __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                           (__v4sf)_mm_dpbf16_ps(__D, __A, __B),
                                           (__v4sf)__D);
}
# 324 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_dpbf16_ps(__mmask8 __U, __m128 __D, __m128bh __A, __m128bh __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                           (__v4sf)_mm_dpbf16_ps(__D, __A, __B),
                                           (__v4sf)_mm_setzero_si128());
}
# 345 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_dpbf16_ps(__m256 __D, __m256bh __A, __m256bh __B) {
  return (__m256)__builtin_ia32_dpbf16ps_256((__v8sf)__D,
                                             (__v16bf)__A,
                                             (__v16bf)__B);
}
# 369 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_mask_dpbf16_ps(__m256 __D, __mmask8 __U, __m256bh __A, __m256bh __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                        (__v8sf)_mm256_dpbf16_ps(__D, __A, __B),
                                        (__v8sf)__D);
}
# 393 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_dpbf16_ps(__mmask8 __U, __m256 __D, __m256bh __A, __m256bh __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                        (__v8sf)_mm256_dpbf16_ps(__D, __A, __B),
                                        (__v8sf)_mm256_setzero_si256());
}
# 410 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __bf16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128))) _mm_cvtness_sbh(float __A) {
  __v4sf __V = {__A, 0, 0, 0};
  __v8bf __R = __builtin_ia32_cvtneps2bf16_128_mask(
      (__v4sf)__V, (__v8bf)_mm_undefined_si128(), (__mmask8)-1);
  return (__bf16)__R[0];
}
# 424 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128))) _mm_cvtpbh_ps(__m128bh __A) {
  return _mm_castsi128_ps(
      (__m128i)_mm_slli_epi32((__m128i)_mm_cvtepi16_epi32((__m128i)__A), 16));
}
# 436 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256))) _mm256_cvtpbh_ps(__m128bh __A) {
  return _mm256_castsi256_ps((__m256i)_mm256_slli_epi32(
      (__m256i)_mm256_cvtepi16_epi32((__m128i)__A), 16));
}
# 451 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_maskz_cvtpbh_ps(__mmask8 __U, __m128bh __A) {
  return _mm_castsi128_ps((__m128i)_mm_slli_epi32(
      (__m128i)_mm_maskz_cvtepi16_epi32((__mmask8)__U, (__m128i)__A), 16));
}
# 467 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_cvtpbh_ps(__mmask8 __U, __m128bh __A) {
  return _mm256_castsi256_ps((__m256i)_mm256_slli_epi32(
      (__m256i)_mm256_maskz_cvtepi16_epi32((__mmask8)__U, (__m128i)__A), 16));
}
# 486 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(128)))
_mm_mask_cvtpbh_ps(__m128 __S, __mmask8 __U, __m128bh __A) {
  return _mm_castsi128_ps((__m128i)_mm_mask_slli_epi32(
      (__m128i)__S, (__mmask8)__U, (__m128i)_mm_cvtepi16_epi32((__m128i)__A),
      16));
}
# 506 "/usr/lib/clang/18/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bf16,no-evex512"), __min_vector_width__(256)))
_mm256_mask_cvtpbh_ps(__m256 __S, __mmask8 __U, __m128bh __A) {
  return _mm256_castsi256_ps((__m256i)_mm256_mask_slli_epi32(
      (__m256i)__S, (__mmask8)__U, (__m256i)_mm256_cvtepi16_epi32((__m128i)__A),
      16));
}
# 240 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/pkuintrin.h" 1 3
# 20 "/usr/lib/clang/18/include/pkuintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("pku")))
_rdpkru_u32(void)
{
  return __builtin_ia32_rdpkru();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("pku")))
_wrpkru(unsigned int __val)
{
  __builtin_ia32_wrpkru(__val);
}
# 245 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/vpclmulqdqintrin.h" 1 3
# 250 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/vaesintrin.h" 1 3
# 26 "/usr/lib/clang/18/include/vaesintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes"), __min_vector_width__(256)))
 _mm256_aesenc_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesenc256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes"), __min_vector_width__(256)))
 _mm256_aesdec_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesdec256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes"), __min_vector_width__(256)))
 _mm256_aesenclast_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesenclast256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes"), __min_vector_width__(256)))
 _mm256_aesdeclast_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesdeclast256((__v4di) __A,
              (__v4di) __B);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512,vaes"), __min_vector_width__(512)))
 _mm512_aesenc_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesenc512((__v8di) __A,
              (__v8di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512,vaes"), __min_vector_width__(512)))
 _mm512_aesdec_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesdec512((__v8di) __A,
              (__v8di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512,vaes"), __min_vector_width__(512)))
 _mm512_aesenclast_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesenclast512((__v8di) __A,
              (__v8di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512,vaes"), __min_vector_width__(512)))
 _mm512_aesdeclast_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesdeclast512((__v8di) __A,
              (__v8di) __B);
}
# 255 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/gfniintrin.h" 1 3
# 59 "/usr/lib/clang/18/include/gfniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("gfni,no-evex512"), __min_vector_width__(128)))
_mm_gf2p8mul_epi8(__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vgf2p8mulb_v16qi((__v16qi) __A,
              (__v16qi) __B);
}
# 77 "/usr/lib/clang/18/include/gfniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,gfni,no-evex512"), __min_vector_width__(256)))
_mm256_gf2p8mul_epi8(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vgf2p8mulb_v32qi((__v32qi) __A,
              (__v32qi) __B);
}
# 114 "/usr/lib/clang/18/include/gfniintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,evex512,gfni"), __min_vector_width__(512)))
_mm512_gf2p8mul_epi8(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vgf2p8mulb_v64qi((__v64qi) __A,
              (__v64qi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512,gfni"), __min_vector_width__(512)))
_mm512_mask_gf2p8mul_epi8(__m512i __S, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_selectb_512(__U,
              (__v64qi) _mm512_gf2p8mul_epi8(__A, __B),
              (__v64qi) __S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,evex512,gfni"), __min_vector_width__(512)))
_mm512_maskz_gf2p8mul_epi8(__mmask64 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_gf2p8mul_epi8((__m512i)_mm512_setzero_si512(),
              __U, __A, __B);
}
# 173 "/usr/lib/clang/18/include/gfniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni,no-evex512"), __min_vector_width__(128)))
_mm_mask_gf2p8mul_epi8(__m128i __S, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_selectb_128(__U,
              (__v16qi) _mm_gf2p8mul_epi8(__A, __B),
              (__v16qi) __S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni,no-evex512"), __min_vector_width__(128)))
_mm_maskz_gf2p8mul_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_gf2p8mul_epi8((__m128i)_mm_setzero_si128(),
              __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni,no-evex512"), __min_vector_width__(256)))
_mm256_mask_gf2p8mul_epi8(__m256i __S, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_selectb_256(__U,
              (__v32qi) _mm256_gf2p8mul_epi8(__A, __B),
              (__v32qi) __S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni,no-evex512"), __min_vector_width__(256)))
_mm256_maskz_gf2p8mul_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_gf2p8mul_epi8((__m256i)_mm256_setzero_si256(),
              __U, __A, __B);
}
# 260 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 1 3
# 55 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbssd_epi32(__m128i __W,
                                                                 __m128i __A,
                                                                 __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbssd128((__v4si)__W, (__v4si)__A,
                                             (__v4si)__B);
}
# 92 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbssd_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbssd256((__v8si)__W, (__v8si)__A,
                                             (__v8si)__B);
}
# 129 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbssds_epi32(__m128i __W,
                                                                  __m128i __A,
                                                                  __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbssds128((__v4si)__W, (__v4si)__A,
                                              (__v4si)__B);
}
# 167 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbssds_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbssds256((__v8si)__W, (__v8si)__A,
                                              (__v8si)__B);
}
# 203 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbsud_epi32(__m128i __W,
                                                                 __m128i __A,
                                                                 __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbsud128((__v4si)__W, (__v4si)__A,
                                             (__v4si)__B);
}
# 240 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbsud_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbsud256((__v8si)__W, (__v8si)__A,
                                             (__v8si)__B);
}
# 277 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbsuds_epi32(__m128i __W,
                                                                  __m128i __A,
                                                                  __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbsuds128((__v4si)__W, (__v4si)__A,
                                              (__v4si)__B);
}
# 315 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbsuds_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbsuds256((__v8si)__W, (__v8si)__A,
                                              (__v8si)__B);
}
# 351 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbuud_epi32(__m128i __W,
                                                                 __m128i __A,
                                                                 __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbuud128((__v4si)__W, (__v4si)__A,
                                             (__v4si)__B);
}
# 388 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbuud_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbuud256((__v8si)__W, (__v8si)__A,
                                             (__v8si)__B);
}
# 425 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbuuds_epi32(__m128i __W,
                                                                  __m128i __A,
                                                                  __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbuuds128((__v4si)__W, (__v4si)__A,
                                              (__v4si)__B);
}
# 463 "/usr/lib/clang/18/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbuuds_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbuuds256((__v8si)__W, (__v8si)__A,
                                              (__v8si)__B);
}
# 265 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avxneconvertintrin.h" 1 3
# 56 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_bcstnebf16_ps(const void *__A) {
  return (__m128)__builtin_ia32_vbcstnebf162ps128((const __bf16 *)__A);
}
# 89 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_bcstnebf16_ps(const void *__A) {
  return (__m256)__builtin_ia32_vbcstnebf162ps256((const __bf16 *)__A);
}
# 122 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_bcstnesh_ps(const void *__A) {
  return (__m128)__builtin_ia32_vbcstnesh2ps128((const _Float16 *)__A);
}
# 155 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_bcstnesh_ps(const void *__A) {
  return (__m256)__builtin_ia32_vbcstnesh2ps256((const _Float16 *)__A);
}
# 188 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneebf16_ps(const __m128bh *__A) {
  return (__m128)__builtin_ia32_vcvtneebf162ps128((const __v8bf *)__A);
}
# 221 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneebf16_ps(const __m256bh *__A) {
  return (__m256)__builtin_ia32_vcvtneebf162ps256((const __v16bf *)__A);
}
# 254 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneeph_ps(const __m128h *__A) {
  return (__m128)__builtin_ia32_vcvtneeph2ps128((const __v8hf *)__A);
}
# 287 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneeph_ps(const __m256h *__A) {
  return (__m256)__builtin_ia32_vcvtneeph2ps256((const __v16hf *)__A);
}
# 320 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneobf16_ps(const __m128bh *__A) {
  return (__m128)__builtin_ia32_vcvtneobf162ps128((const __v8bf *)__A);
}
# 353 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneobf16_ps(const __m256bh *__A) {
  return (__m256)__builtin_ia32_vcvtneobf162ps256((const __v16bf *)__A);
}
# 386 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneoph_ps(const __m128h *__A) {
  return (__m128)__builtin_ia32_vcvtneoph2ps128((const __v8hf *)__A);
}
# 419 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneoph_ps(const __m256h *__A) {
  return (__m256)__builtin_ia32_vcvtneoph2ps256((const __v16hf *)__A);
}
# 447 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneps_avx_pbh(__m128 __A) {
  return (__m128bh)__builtin_ia32_vcvtneps2bf16128((__v4sf)__A);
}
# 475 "/usr/lib/clang/18/include/avxneconvertintrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneps_avx_pbh(__m256 __A) {
  return (__m128bh)__builtin_ia32_vcvtneps2bf16256((__v8sf)__A);
}
# 270 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/sha512intrin.h" 1 3
# 63 "/usr/lib/clang/18/include/sha512intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("sha512"), __min_vector_width__(256)))
_mm256_sha512msg1_epi64(__m256i __A, __m128i __B) {
  return (__m256i)__builtin_ia32_vsha512msg1((__v4du)__A, (__v2du)__B);
}
# 111 "/usr/lib/clang/18/include/sha512intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("sha512"), __min_vector_width__(256)))
_mm256_sha512msg2_epi64(__m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vsha512msg2((__v4du)__A, (__v4du)__B);
}
# 192 "/usr/lib/clang/18/include/sha512intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("sha512"), __min_vector_width__(256)))
_mm256_sha512rnds2_epi64(__m256i __A, __m256i __B, __m128i __C) {
  return (__m256i)__builtin_ia32_vsha512rnds2((__v4du)__A, (__v4du)__B,
                                              (__v2du)__C);
}
# 275 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/sm3intrin.h" 1 3
# 72 "/usr/lib/clang/18/include/sm3intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sm3"), __min_vector_width__(128))) _mm_sm3msg1_epi32(__m128i __A,
                                                                  __m128i __B,
                                                                  __m128i __C) {
  return (__m128i)__builtin_ia32_vsm3msg1((__v4su)__A, (__v4su)__B,
                                          (__v4su)__C);
}
# 129 "/usr/lib/clang/18/include/sm3intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sm3"), __min_vector_width__(128))) _mm_sm3msg2_epi32(__m128i __A,
                                                                  __m128i __B,
                                                                  __m128i __C) {
  return (__m128i)__builtin_ia32_vsm3msg2((__v4su)__A, (__v4su)__B,
                                          (__v4su)__C);
}
# 280 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/sm4intrin.h" 1 3
# 285 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 1 3
# 56 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(128))) _mm_dpwsud_epi32(__m128i __W,
                                                                 __m128i __A,
                                                                 __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpwsud128((__v4si)__W, (__v4si)__A,
                                             (__v4si)__B);
}
# 93 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(256)))
_mm256_dpwsud_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpwsud256((__v8si)__W, (__v8si)__A,
                                             (__v8si)__B);
}
# 130 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(128))) _mm_dpwsuds_epi32(__m128i __W,
                                                                  __m128i __A,
                                                                  __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpwsuds128((__v4si)__W, (__v4si)__A,
                                              (__v4si)__B);
}
# 168 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(256)))
_mm256_dpwsuds_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpwsuds256((__v8si)__W, (__v8si)__A,
                                              (__v8si)__B);
}
# 204 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(128))) _mm_dpwusd_epi32(__m128i __W,
                                                                 __m128i __A,
                                                                 __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpwusd128((__v4si)__W, (__v4si)__A,
                                             (__v4si)__B);
}
# 241 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(256)))
_mm256_dpwusd_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpwusd256((__v8si)__W, (__v8si)__A,
                                             (__v8si)__B);
}
# 278 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(128))) _mm_dpwusds_epi32(__m128i __W,
                                                                  __m128i __A,
                                                                  __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpwusds128((__v4si)__W, (__v4si)__A,
                                              (__v4si)__B);
}
# 316 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(256)))
_mm256_dpwusds_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpwusds256((__v8si)__W, (__v8si)__A,
                                              (__v8si)__B);
}
# 352 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(128))) _mm_dpwuud_epi32(__m128i __W,
                                                                 __m128i __A,
                                                                 __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpwuud128((__v4si)__W, (__v4si)__A,
                                             (__v4si)__B);
}
# 389 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(256)))
_mm256_dpwuud_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpwuud256((__v8si)__W, (__v8si)__A,
                                             (__v8si)__B);
}
# 426 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(128))) _mm_dpwuuds_epi32(__m128i __W,
                                                                  __m128i __A,
                                                                  __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpwuuds128((__v4si)__W, (__v4si)__A,
                                              (__v4si)__B);
}
# 464 "/usr/lib/clang/18/include/avxvnniint16intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint16"), __min_vector_width__(256)))
_mm256_dpwuuds_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpwuuds256((__v8si)__W, (__v8si)__A,
                                              (__v8si)__B);
}
# 290 "/usr/lib/clang/18/include/immintrin.h" 2 3
# 301 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("rdpid")))
_rdpid_u32(void) {
  return __builtin_ia32_rdpid();
}
# 318 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdrnd")))
_rdrand16_step(unsigned short *__p)
{
  return (int)__builtin_ia32_rdrand16_step(__p);
}
# 333 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdrnd")))
_rdrand32_step(unsigned int *__p)
{
  return (int)__builtin_ia32_rdrand32_step(__p);
}
# 348 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdrnd")))
_rdrand64_step(unsigned long long *__p)
{

  return (int)__builtin_ia32_rdrand64_step(__p);
# 367 "/usr/lib/clang/18/include/immintrin.h" 3
}
# 380 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readfsbase_u32(void)
{
  return __builtin_ia32_rdfsbase32();
}
# 393 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readfsbase_u64(void)
{
  return __builtin_ia32_rdfsbase64();
}
# 406 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readgsbase_u32(void)
{
  return __builtin_ia32_rdgsbase32();
}
# 419 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readgsbase_u64(void)
{
  return __builtin_ia32_rdgsbase64();
}
# 433 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writefsbase_u32(unsigned int __V)
{
  __builtin_ia32_wrfsbase32(__V);
}
# 447 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writefsbase_u64(unsigned long long __V)
{
  __builtin_ia32_wrfsbase64(__V);
}
# 461 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writegsbase_u32(unsigned int __V)
{
  __builtin_ia32_wrgsbase32(__V);
}
# 475 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writegsbase_u64(unsigned long long __V)
{
  __builtin_ia32_wrgsbase64(__V);
}
# 502 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_loadbe_i16(void const * __P) {
  struct __loadu_i16 {
    unsigned short __v;
  } __attribute__((__packed__, __may_alias__));
  return (short)__builtin_bswap16(((const struct __loadu_i16*)__P)->__v);
}
# 520 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_storebe_i16(void * __P, short __D) {
  struct __storeu_i16 {
    unsigned short __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_i16*)__P)->__v = __builtin_bswap16((unsigned short)__D);
}
# 537 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_loadbe_i32(void const * __P) {
  struct __loadu_i32 {
    unsigned int __v;
  } __attribute__((__packed__, __may_alias__));
  return (int)__builtin_bswap32(((const struct __loadu_i32*)__P)->__v);
}
# 555 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_storebe_i32(void * __P, int __D) {
  struct __storeu_i32 {
    unsigned int __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_i32*)__P)->__v = __builtin_bswap32((unsigned int)__D);
}
# 573 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_loadbe_i64(void const * __P) {
  struct __loadu_i64 {
    unsigned long long __v;
  } __attribute__((__packed__, __may_alias__));
  return (long long)__builtin_bswap64(((const struct __loadu_i64*)__P)->__v);
}
# 591 "/usr/lib/clang/18/include/immintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_storebe_i64(void * __P, long long __D) {
  struct __storeu_i64 {
    unsigned long long __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_i64*)__P)->__v = __builtin_bswap64((unsigned long long)__D);
}





# 1 "/usr/lib/clang/18/include/rtmintrin.h" 1 3
# 29 "/usr/lib/clang/18/include/rtmintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("rtm")))
_xbegin(void)
{
  return (unsigned int)__builtin_ia32_xbegin();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("rtm")))
_xend(void)
{
  __builtin_ia32_xend();
}
# 604 "/usr/lib/clang/18/include/immintrin.h" 2 3
# 1 "/usr/lib/clang/18/include/xtestintrin.h" 1 3
# 21 "/usr/lib/clang/18/include/xtestintrin.h" 3
static __inline__ int
    __attribute__((__always_inline__, __nodebug__, __target__("rtm")))
    _xtest(void) {
  return __builtin_ia32_xtest();
}
# 605 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/shaintrin.h" 1 3
# 69 "/usr/lib/clang/18/include/shaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha1nexte_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha1nexte((__v4si)__X, (__v4si)__Y);
}
# 89 "/usr/lib/clang/18/include/shaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha1msg1_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha1msg1((__v4si)__X, (__v4si)__Y);
}
# 109 "/usr/lib/clang/18/include/shaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha1msg2_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha1msg2((__v4si)__X, (__v4si)__Y);
}
# 141 "/usr/lib/clang/18/include/shaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha256rnds2_epu32(__m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i)__builtin_ia32_sha256rnds2((__v4si)__X, (__v4si)__Y, (__v4si)__Z);
}
# 161 "/usr/lib/clang/18/include/shaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha256msg1_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha256msg1((__v4si)__X, (__v4si)__Y);
}
# 181 "/usr/lib/clang/18/include/shaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha256msg2_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha256msg2((__v4si)__X, (__v4si)__Y);
}
# 610 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/fxsrintrin.h" 1 3
# 29 "/usr/lib/clang/18/include/fxsrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxsave(void *__p)
{
  __builtin_ia32_fxsave(__p);
}
# 47 "/usr/lib/clang/18/include/fxsrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxrstor(void *__p)
{
  __builtin_ia32_fxrstor(__p);
}
# 64 "/usr/lib/clang/18/include/fxsrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxsave64(void *__p)
{
  __builtin_ia32_fxsave64(__p);
}
# 82 "/usr/lib/clang/18/include/fxsrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxrstor64(void *__p)
{
  __builtin_ia32_fxrstor64(__p);
}
# 615 "/usr/lib/clang/18/include/immintrin.h" 2 3



# 1 "/usr/lib/clang/18/include/xsaveintrin.h" 1 3
# 24 "/usr/lib/clang/18/include/xsaveintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xsave(void *__p, unsigned long long __m) {
  __builtin_ia32_xsave(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xrstor(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstor(__p, __m);
}
# 49 "/usr/lib/clang/18/include/xsaveintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xsave64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsave64(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xrstor64(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstor64(__p, __m);
}
# 619 "/usr/lib/clang/18/include/immintrin.h" 2 3



# 1 "/usr/lib/clang/18/include/xsaveoptintrin.h" 1 3
# 20 "/usr/lib/clang/18/include/xsaveoptintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaveopt")))
_xsaveopt(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaveopt(__p, __m);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaveopt")))
_xsaveopt64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaveopt64(__p, __m);
}
# 623 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/xsavecintrin.h" 1 3
# 45 "/usr/lib/clang/18/include/xsavecintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsavec")))
_xsavec(void *__p, unsigned long long __m) {
  __builtin_ia32_xsavec(__p, __m);
}
# 76 "/usr/lib/clang/18/include/xsavecintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsavec")))
_xsavec64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsavec64(__p, __m);
}
# 628 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/xsavesintrin.h" 1 3
# 20 "/usr/lib/clang/18/include/xsavesintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xsaves(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaves(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xrstors(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstors(__p, __m);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xrstors64(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstors64(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xsaves64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaves64(__p, __m);
}
# 633 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/cetintrin.h" 1 3
# 21 "/usr/lib/clang/18/include/cetintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _incsspd(int __a) {
  __builtin_ia32_incsspd((unsigned int)__a);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _incsspq(unsigned long long __a) {
  __builtin_ia32_incsspq(__a);
}



static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _inc_ssp(unsigned int __a) {
  __builtin_ia32_incsspq(__a);
}






static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspd(unsigned int __a) {
  return __builtin_ia32_rdsspd(__a);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspd_i32(void) {
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wuninitialized"
  unsigned int t;
  return __builtin_ia32_rdsspd(t);
#pragma clang diagnostic pop
}


static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspq(unsigned long long __a) {
  return __builtin_ia32_rdsspq(__a);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspq_i64(void) {
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wuninitialized"
  unsigned long long t;
  return __builtin_ia32_rdsspq(t);
#pragma clang diagnostic pop
}



static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _get_ssp(void) {
  return __builtin_ia32_rdsspq(0);
}






static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _saveprevssp(void) {
  __builtin_ia32_saveprevssp();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rstorssp(void * __p) {
  __builtin_ia32_rstorssp(__p);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrssd(unsigned int __a, void * __p) {
  __builtin_ia32_wrssd(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrssq(unsigned long long __a, void * __p) {
  __builtin_ia32_wrssq(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrussd(unsigned int __a, void * __p) {
  __builtin_ia32_wrussd(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrussq(unsigned long long __a, void * __p) {
  __builtin_ia32_wrussq(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _setssbsy(void) {
  __builtin_ia32_setssbsy();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _clrssbsy(void * __p) {
  __builtin_ia32_clrssbsy(__p);
}
# 638 "/usr/lib/clang/18/include/immintrin.h" 2 3



# 1 "/usr/lib/clang/18/include/adcintrin.h" 1 3
# 54 "/usr/lib/clang/18/include/adcintrin.h" 3
static __inline unsigned char __attribute__((__always_inline__, __nodebug__)) _addcarry_u32(unsigned char __cf,
                                                        unsigned int __x,
                                                        unsigned int __y,
                                                        unsigned int *__p) {
  return __builtin_ia32_addcarryx_u32(__cf, __x, __y, __p);
}
# 85 "/usr/lib/clang/18/include/adcintrin.h" 3
static __inline unsigned char __attribute__((__always_inline__, __nodebug__)) _subborrow_u32(unsigned char __cf,
                                                         unsigned int __x,
                                                         unsigned int __y,
                                                         unsigned int *__p) {
  return __builtin_ia32_subborrow_u32(__cf, __x, __y, __p);
}
# 116 "/usr/lib/clang/18/include/adcintrin.h" 3
static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_addcarry_u64(unsigned char __cf, unsigned long long __x,
              unsigned long long __y, unsigned long long *__p) {
  return __builtin_ia32_addcarryx_u64(__cf, __x, __y, __p);
}
# 146 "/usr/lib/clang/18/include/adcintrin.h" 3
static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_subborrow_u64(unsigned char __cf, unsigned long long __x,
               unsigned long long __y, unsigned long long *__p) {
  return __builtin_ia32_subborrow_u64(__cf, __x, __y, __p);
}
# 642 "/usr/lib/clang/18/include/immintrin.h" 2 3



# 1 "/usr/lib/clang/18/include/adxintrin.h" 1 3
# 57 "/usr/lib/clang/18/include/adxintrin.h" 3
static __inline unsigned char __attribute__((__always_inline__, __nodebug__, __target__("adx"))) _addcarryx_u32(unsigned char __cf,
                                                         unsigned int __x,
                                                         unsigned int __y,
                                                         unsigned int *__p) {
  return __builtin_ia32_addcarryx_u32(__cf, __x, __y, __p);
}
# 88 "/usr/lib/clang/18/include/adxintrin.h" 3
static __inline unsigned char __attribute__((__always_inline__, __nodebug__, __target__("adx")))
_addcarryx_u64(unsigned char __cf, unsigned long long __x,
               unsigned long long __y, unsigned long long *__p) {
  return __builtin_ia32_addcarryx_u64(__cf, __x, __y, __p);
}
# 646 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/rdseedintrin.h" 1 3
# 41 "/usr/lib/clang/18/include/rdseedintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdseed")))
_rdseed16_step(unsigned short *__p)
{
  return (int) __builtin_ia32_rdseed16_step(__p);
}
# 68 "/usr/lib/clang/18/include/rdseedintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdseed")))
_rdseed32_step(unsigned int *__p)
{
  return (int) __builtin_ia32_rdseed32_step(__p);
}
# 96 "/usr/lib/clang/18/include/rdseedintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdseed")))
_rdseed64_step(unsigned long long *__p)
{
  return (int) __builtin_ia32_rdseed64_step(__p);
}
# 651 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/wbnoinvdintrin.h" 1 3
# 17 "/usr/lib/clang/18/include/wbnoinvdintrin.h" 3
static __inline__ void
  __attribute__((__always_inline__, __nodebug__, __target__("wbnoinvd")))
_wbnoinvd (void)
{
  __builtin_ia32_wbnoinvd ();
}
# 656 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/cldemoteintrin.h" 1 3
# 28 "/usr/lib/clang/18/include/cldemoteintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("cldemote")))
_cldemote(const void * __P) {
  __builtin_ia32_cldemote(__P);
}
# 661 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/waitpkgintrin.h" 1 3
# 20 "/usr/lib/clang/18/include/waitpkgintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("waitpkg")))
_umonitor (void * __address)
{
  __builtin_ia32_umonitor (__address);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("waitpkg")))
_umwait (unsigned int __control, unsigned long long __counter)
{
  return __builtin_ia32_umwait (__control,
    (unsigned int)(__counter >> 32), (unsigned int)__counter);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("waitpkg")))
_tpause (unsigned int __control, unsigned long long __counter)
{
  return __builtin_ia32_tpause (__control,
    (unsigned int)(__counter >> 32), (unsigned int)__counter);
}
# 666 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/movdirintrin.h" 1 3
# 17 "/usr/lib/clang/18/include/movdirintrin.h" 3
static __inline__ void
__attribute__((__always_inline__, __nodebug__, __target__("movdiri")))
_directstoreu_u32 (void *__dst, unsigned int __value)
{
  __builtin_ia32_directstore_u32((unsigned int *)__dst, (unsigned int)__value);
}




static __inline__ void
__attribute__((__always_inline__, __nodebug__, __target__("movdiri")))
_directstoreu_u64 (void *__dst, unsigned long __value)
{
  __builtin_ia32_directstore_u64((unsigned long *)__dst, __value);
}
# 42 "/usr/lib/clang/18/include/movdirintrin.h" 3
static __inline__ void
__attribute__((__always_inline__, __nodebug__, __target__("movdir64b")))
_movdir64b (void *__dst __attribute__((align_value(64))), const void *__src)
{
  __builtin_ia32_movdir64b(__dst, __src);
}
# 671 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/pconfigintrin.h" 1 3
# 25 "/usr/lib/clang/18/include/pconfigintrin.h" 3
static __inline unsigned int __attribute__((__always_inline__, __nodebug__, __target__("pconfig")))
_pconfig_u32(unsigned int __leaf, long unsigned int __d[])
{
  unsigned int __result;
  __asm__ ("pconfig"
           : "=a" (__result), "=b" (__d[0]), "=c" (__d[1]), "=d" (__d[2])
           : "a" (__leaf), "b" (__d[0]), "c" (__d[1]), "d" (__d[2])
           : "cc");
  return __result;
}
# 676 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/sgxintrin.h" 1 3
# 23 "/usr/lib/clang/18/include/sgxintrin.h" 3
static __inline unsigned int __attribute__((__always_inline__, __nodebug__, __target__("sgx")))
_enclu_u32(unsigned int __leaf, long unsigned int __d[])
{
  unsigned int __result;
  __asm__ ("enclu"
           : "=a" (__result), "=b" (__d[0]), "=c" (__d[1]), "=d" (__d[2])
           : "a" (__leaf), "b" (__d[0]), "c" (__d[1]), "d" (__d[2])
           : "cc");
  return __result;
}

static __inline unsigned int __attribute__((__always_inline__, __nodebug__, __target__("sgx")))
_encls_u32(unsigned int __leaf, long unsigned int __d[])
{
  unsigned int __result;
  __asm__ ("encls"
           : "=a" (__result), "=b" (__d[0]), "=c" (__d[1]), "=d" (__d[2])
           : "a" (__leaf), "b" (__d[0]), "c" (__d[1]), "d" (__d[2])
           : "cc");
  return __result;
}

static __inline unsigned int __attribute__((__always_inline__, __nodebug__, __target__("sgx")))
_enclv_u32(unsigned int __leaf, long unsigned int __d[])
{
  unsigned int __result;
  __asm__ ("enclv"
           : "=a" (__result), "=b" (__d[0]), "=c" (__d[1]), "=d" (__d[2])
           : "a" (__leaf), "b" (__d[0]), "c" (__d[1]), "d" (__d[2])
           : "cc");
  return __result;
}
# 681 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/ptwriteintrin.h" 1 3
# 21 "/usr/lib/clang/18/include/ptwriteintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("ptwrite")))
_ptwrite32(unsigned int __value) {
  __builtin_ia32_ptwrite32(__value);
}



static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("ptwrite")))
_ptwrite64(unsigned long long __value) {
  __builtin_ia32_ptwrite64(__value);
}
# 686 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/invpcidintrin.h" 1 3
# 17 "/usr/lib/clang/18/include/invpcidintrin.h" 3
static __inline__ void
  __attribute__((__always_inline__, __nodebug__, __target__("invpcid")))
_invpcid(unsigned int __type, void *__descriptor) {
  __builtin_ia32_invpcid(__type, __descriptor);
}
# 691 "/usr/lib/clang/18/include/immintrin.h" 2 3



# 1 "/usr/lib/clang/18/include/amxfp16intrin.h" 1 3
# 695 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/keylockerintrin.h" 1 3
# 95 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_loadiwkey (unsigned int __ctl, __m128i __intkey,
               __m128i __enkey_lo, __m128i __enkey_hi) {
  __builtin_ia32_loadiwkey (__intkey, __enkey_lo, __enkey_hi, __ctl);
}
# 130 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_encodekey128_u32(unsigned int __htype, __m128i __key, void *__h) {
  return __builtin_ia32_encodekey128_u32(__htype, (__v2di)__key, __h);
}
# 167 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_encodekey256_u32(unsigned int __htype, __m128i __key_lo, __m128i __key_hi,
                     void *__h) {
  return __builtin_ia32_encodekey256_u32(__htype, (__v2di)__key_lo,
                                         (__v2di)__key_hi, __h);
}
# 206 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_aesenc128kl_u8(__m128i* __odata, __m128i __idata, const void *__h) {
  return __builtin_ia32_aesenc128kl_u8((__v2di *)__odata, (__v2di)__idata, __h);
}
# 245 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_aesenc256kl_u8(__m128i* __odata, __m128i __idata, const void *__h) {
  return __builtin_ia32_aesenc256kl_u8((__v2di *)__odata, (__v2di)__idata, __h);
}
# 284 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_aesdec128kl_u8(__m128i* __odata, __m128i __idata, const void *__h) {
  return __builtin_ia32_aesdec128kl_u8((__v2di *)__odata, (__v2di)__idata, __h);
}
# 323 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_aesdec256kl_u8(__m128i* __odata, __m128i __idata, const void *__h) {
  return __builtin_ia32_aesdec256kl_u8((__v2di *)__odata, (__v2di)__idata, __h);
}
# 381 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl,widekl"), __min_vector_width__(128)))
_mm_aesencwide128kl_u8(__m128i __odata[8], const __m128i __idata[8], const void* __h) {
  return __builtin_ia32_aesencwide128kl_u8((__v2di *)__odata,
                                           (const __v2di *)__idata, __h);
}
# 427 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl,widekl"), __min_vector_width__(128)))
_mm_aesencwide256kl_u8(__m128i __odata[8], const __m128i __idata[8], const void* __h) {
  return __builtin_ia32_aesencwide256kl_u8((__v2di *)__odata,
                                           (const __v2di *)__idata, __h);
}
# 473 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl,widekl"), __min_vector_width__(128)))
_mm_aesdecwide128kl_u8(__m128i __odata[8], const __m128i __idata[8], const void* __h) {
  return __builtin_ia32_aesdecwide128kl_u8((__v2di *)__odata,
                                           (const __v2di *)__idata, __h);
}
# 519 "/usr/lib/clang/18/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl,widekl"), __min_vector_width__(128)))
_mm_aesdecwide256kl_u8(__m128i __odata[8], const __m128i __idata[8], const void* __h) {
  return __builtin_ia32_aesdecwide256kl_u8((__v2di *)__odata,
                                           (const __v2di *)__idata, __h);
}
# 700 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/amxintrin.h" 1 3
# 41 "/usr/lib/clang/18/include/amxintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
_tile_loadconfig(const void *__config) {
  __builtin_ia32_tile_loadconfig(__config);
}
# 57 "/usr/lib/clang/18/include/amxintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
_tile_storeconfig(void *__config) {
  __builtin_ia32_tile_storeconfig(__config);
}







static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("amx-tile"))) _tile_release(void) {
  __builtin_ia32_tilerelease();
}
# 234 "/usr/lib/clang/18/include/amxintrin.h" 3
typedef int _tile1024i __attribute__((__vector_size__(1024), __aligned__(64)));


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_loadd_internal(unsigned short m, unsigned short n, const void *base,
                     long unsigned int stride) {
  return __builtin_ia32_tileloadd64_internal(m, n, base,
                                             (long unsigned int)(stride));
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_loaddt1_internal(unsigned short m, unsigned short n, const void *base,
                       long unsigned int stride) {
  return __builtin_ia32_tileloaddt164_internal(m, n, base,
                                               (long unsigned int)(stride));
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_dpbssd_internal(unsigned short m, unsigned short n, unsigned short k,
                      _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbssd_internal(m, n, k, dst, src1, src2);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_dpbsud_internal(unsigned short m, unsigned short n, unsigned short k,
                      _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbsud_internal(m, n, k, dst, src1, src2);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_dpbusd_internal(unsigned short m, unsigned short n, unsigned short k,
                      _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbusd_internal(m, n, k, dst, src1, src2);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_dpbuud_internal(unsigned short m, unsigned short n, unsigned short k,
                      _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbuud_internal(m, n, k, dst, src1, src2);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_stored_internal(unsigned short m, unsigned short n, void *base,
                      long unsigned int stride, _tile1024i tile) {
  return __builtin_ia32_tilestored64_internal(m, n, base,
                                              (long unsigned int)(stride), tile);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-bf16")))
_tile_dpbf16ps_internal(unsigned short m, unsigned short n, unsigned short k,
                        _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbf16ps_internal(m, n, k, dst, src1, src2);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-fp16")))
_tile_dpfp16ps_internal(unsigned short m, unsigned short n, unsigned short k,
                        _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpfp16ps_internal(m, n, k, dst, src1, src2);
}





typedef struct __tile1024i_str {
  const unsigned short row;
  const unsigned short col;
  _tile1024i tile;
} __tile1024i;
# 325 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
static __inline__ void __tile_loadd(__tile1024i *dst, const void *base,
                                    long unsigned int stride) {
  dst->tile = _tile_loadd_internal(dst->row, dst->col, base, stride);
}
# 346 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
static __inline__ void __tile_stream_loadd(__tile1024i *dst, const void *base,
                                           long unsigned int stride) {
  dst->tile = _tile_loaddt1_internal(dst->row, dst->col, base, stride);
}
# 368 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
static __inline__ void __tile_dpbssd(__tile1024i *dst, __tile1024i src0,
                                     __tile1024i src1) {
  dst->tile = _tile_dpbssd_internal(src0.row, src1.col, src0.col, dst->tile,
                                    src0.tile, src1.tile);
}
# 391 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
static __inline__ void __tile_dpbsud(__tile1024i *dst, __tile1024i src0,
                                     __tile1024i src1) {
  dst->tile = _tile_dpbsud_internal(src0.row, src1.col, src0.col, dst->tile,
                                    src0.tile, src1.tile);
}
# 414 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
static __inline__ void __tile_dpbusd(__tile1024i *dst, __tile1024i src0,
                                     __tile1024i src1) {
  dst->tile = _tile_dpbusd_internal(src0.row, src1.col, src0.col, dst->tile,
                                    src0.tile, src1.tile);
}
# 437 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
static __inline__ void __tile_dpbuud(__tile1024i *dst, __tile1024i src0,
                                     __tile1024i src1) {
  dst->tile = _tile_dpbuud_internal(src0.row, src1.col, src0.col, dst->tile,
                                    src0.tile, src1.tile);
}
# 455 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
static __inline__ void __tile_stored(void *base, long unsigned int stride,
                                     __tile1024i src) {
  _tile_stored_internal(src.row, src.col, base, stride, src.tile);
}
# 469 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
static __inline__ void __tile_zero(__tile1024i *dst) {
  dst->tile = __builtin_ia32_tilezero_internal(dst->row, dst->col);
}
# 489 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-bf16")))
static __inline__ void __tile_dpbf16ps(__tile1024i *dst, __tile1024i src0,
                                       __tile1024i src1) {
  dst->tile = _tile_dpbf16ps_internal(src0.row, src1.col, src0.col, dst->tile,
                                      src0.tile, src1.tile);
}
# 511 "/usr/lib/clang/18/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-fp16")))
static __inline__ void __tile_dpfp16ps(__tile1024i *dst, __tile1024i src0,
                                       __tile1024i src1) {
  dst->tile = _tile_dpfp16ps_internal(src0.row, src1.col, src0.col, dst->tile,
                                      src0.tile, src1.tile);
}
# 705 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/amxcomplexintrin.h" 1 3
# 110 "/usr/lib/clang/18/include/amxcomplexintrin.h" 3
static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-complex")))
_tile_cmmimfp16ps_internal(unsigned short m, unsigned short n, unsigned short k,
                           _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tcmmimfp16ps_internal(m, n, k, dst, src1, src2);
}

static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-complex")))
_tile_cmmrlfp16ps_internal(unsigned short m, unsigned short n, unsigned short k,
                           _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tcmmrlfp16ps_internal(m, n, k, dst, src1, src2);
}
# 138 "/usr/lib/clang/18/include/amxcomplexintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-complex")))
static void __tile_cmmimfp16ps(__tile1024i *dst, __tile1024i src0,
                               __tile1024i src1) {
  dst->tile = _tile_cmmimfp16ps_internal(src0.row, src1.col, src0.col,
                                         dst->tile, src0.tile, src1.tile);
}
# 161 "/usr/lib/clang/18/include/amxcomplexintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-complex")))
static void __tile_cmmrlfp16ps(__tile1024i *dst, __tile1024i src0,
                               __tile1024i src1) {
  dst->tile = _tile_cmmrlfp16ps_internal(src0.row, src1.col, src0.col,
                                         dst->tile, src0.tile, src1.tile);
}
# 710 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vp2intersectintrin.h" 1 3
# 51 "/usr/lib/clang/18/include/avx512vp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vp2intersect,evex512"), __min_vector_width__(512)))
_mm512_2intersect_epi32(__m512i __a, __m512i __b, __mmask16 *__m0, __mmask16 *__m1) {
  __builtin_ia32_vp2intersect_d_512((__v16si)__a, (__v16si)__b, __m0, __m1);
}
# 71 "/usr/lib/clang/18/include/avx512vp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vp2intersect,evex512"), __min_vector_width__(512)))
_mm512_2intersect_epi64(__m512i __a, __m512i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_q_512((__v8di)__a, (__v8di)__b, __m0, __m1);
}
# 715 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/avx512vlvp2intersectintrin.h" 1 3
# 55 "/usr/lib/clang/18/include/avx512vlvp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vp2intersect,no-evex512"), __min_vector_width__(256)))
_mm256_2intersect_epi32(__m256i __a, __m256i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_d_256((__v8si)__a, (__v8si)__b, __m0, __m1);
}
# 75 "/usr/lib/clang/18/include/avx512vlvp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vp2intersect,no-evex512"), __min_vector_width__(256)))
_mm256_2intersect_epi64(__m256i __a, __m256i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_q_256((__v4di)__a, (__v4di)__b, __m0, __m1);
}
# 95 "/usr/lib/clang/18/include/avx512vlvp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vp2intersect,no-evex512"), __min_vector_width__(128)))
_mm_2intersect_epi32(__m128i __a, __m128i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_d_128((__v4si)__a, (__v4si)__b, __m0, __m1);
}
# 115 "/usr/lib/clang/18/include/avx512vlvp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vp2intersect,no-evex512"), __min_vector_width__(128)))
_mm_2intersect_epi64(__m128i __a, __m128i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_q_128((__v2di)__a, (__v2di)__b, __m0, __m1);
}
# 720 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/enqcmdintrin.h" 1 3
# 35 "/usr/lib/clang/18/include/enqcmdintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("enqcmd")))
_enqcmd (void *__dst, const void *__src)
{
  return __builtin_ia32_enqcmd(__dst, __src);
}
# 55 "/usr/lib/clang/18/include/enqcmdintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("enqcmd")))
_enqcmds (void *__dst, const void *__src)
{
  return __builtin_ia32_enqcmds(__dst, __src);
}
# 725 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/serializeintrin.h" 1 3
# 23 "/usr/lib/clang/18/include/serializeintrin.h" 3
static __inline__ void
__attribute__((__always_inline__, __nodebug__, __target__("serialize")))
_serialize (void)
{
  __builtin_ia32_serialize ();
}
# 730 "/usr/lib/clang/18/include/immintrin.h" 2 3




# 1 "/usr/lib/clang/18/include/tsxldtrkintrin.h" 1 3
# 31 "/usr/lib/clang/18/include/tsxldtrkintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("tsxldtrk")))
_xsusldtrk (void)
{
    __builtin_ia32_xsusldtrk();
}
# 48 "/usr/lib/clang/18/include/tsxldtrkintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("tsxldtrk")))
_xresldtrk (void)
{
    __builtin_ia32_xresldtrk();
}
# 735 "/usr/lib/clang/18/include/immintrin.h" 2 3
# 16 "/usr/lib/clang/18/include/x86intrin.h" 2 3



# 1 "/usr/lib/clang/18/include/mm3dnow.h" 1 3
# 14 "/usr/lib/clang/18/include/mm3dnow.h" 3
# 1 "/usr/lib/clang/18/include/prfchwintrin.h" 1 3
# 28 "/usr/lib/clang/18/include/prfchwintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__))
_m_prefetch(void *__P)
{
  __builtin_prefetch (__P, 0, 3 );
}
# 49 "/usr/lib/clang/18/include/prfchwintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__))
_m_prefetchw(volatile const void *__P)
{
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wcast-qual"
  __builtin_prefetch ((const void*)__P, 1, 3 );
#pragma clang diagnostic pop
}
# 15 "/usr/lib/clang/18/include/mm3dnow.h" 2 3

typedef float __v2sf __attribute__((__vector_size__(8)));




static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("3dnow")))
_m_femms(void) {
  __builtin_ia32_femms();
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pavgusb(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pavgusb((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pf2id(__m64 __m) {
  return (__m64)__builtin_ia32_pf2id((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfacc(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfacc((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfadd(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfadd((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfcmpeq(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfcmpeq((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfcmpge(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfcmpge((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfcmpgt(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfcmpgt((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfmax(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfmax((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfmin(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfmin((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfmul(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfmul((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrcp(__m64 __m) {
  return (__m64)__builtin_ia32_pfrcp((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrcpit1(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfrcpit1((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrcpit2(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfrcpit2((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrsqrt(__m64 __m) {
  return (__m64)__builtin_ia32_pfrsqrt((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrsqrtit1(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfrsqit1((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfsub(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfsub((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfsubr(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfsubr((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pi2fd(__m64 __m) {
  return (__m64)__builtin_ia32_pi2fd((__v2si)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pmulhrw(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pmulhrw((__v4hi)__m1, (__v4hi)__m2);
}





static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pf2iw(__m64 __m) {
  return (__m64)__builtin_ia32_pf2iw((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pfnacc(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfnacc((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pfpnacc(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfpnacc((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pi2fw(__m64 __m) {
  return (__m64)__builtin_ia32_pi2fw((__v2si)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pswapdsf(__m64 __m) {
  return (__m64)__builtin_ia32_pswapdsf((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pswapdsi(__m64 __m) {
  return (__m64)__builtin_ia32_pswapdsi((__v2si)__m);
}
# 20 "/usr/lib/clang/18/include/x86intrin.h" 2 3




# 1 "/usr/lib/clang/18/include/prfchwintrin.h" 1 3
# 25 "/usr/lib/clang/18/include/x86intrin.h" 2 3




# 1 "/usr/lib/clang/18/include/ammintrin.h" 1 3
# 69 "/usr/lib/clang/18/include/ammintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4a"), __min_vector_width__(128)))
_mm_extract_si64(__m128i __x, __m128i __y)
{
  return (__m128i)__builtin_ia32_extrq((__v2di)__x, (__v16qi)__y);
}
# 139 "/usr/lib/clang/18/include/ammintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4a"), __min_vector_width__(128)))
_mm_insert_si64(__m128i __x, __m128i __y)
{
  return (__m128i)__builtin_ia32_insertq((__v2di)__x, (__v2di)__y);
}
# 157 "/usr/lib/clang/18/include/ammintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse4a"), __min_vector_width__(128)))
_mm_stream_sd(void *__p, __m128d __a)
{
  __builtin_ia32_movntsd((double *)__p, (__v2df)__a);
}
# 175 "/usr/lib/clang/18/include/ammintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse4a"), __min_vector_width__(128)))
_mm_stream_ss(void *__p, __m128 __a)
{
  __builtin_ia32_movntss((float *)__p, (__v4sf)__a);
}
# 30 "/usr/lib/clang/18/include/x86intrin.h" 2 3




# 1 "/usr/lib/clang/18/include/fma4intrin.h" 1 3
# 23 "/usr/lib/clang/18/include/fma4intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_macc_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_macc_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_macc_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_macc_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmacc_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmacc_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmacc_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmacc_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_maddsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_maddsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msubadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msubadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_macc_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_macc_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_msub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_msub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_nmacc_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_nmacc_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_nmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_nmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, -(__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_maddsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_maddsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_msubadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_msubadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}
# 35 "/usr/lib/clang/18/include/x86intrin.h" 2 3




# 1 "/usr/lib/clang/18/include/xopintrin.h" 1 3
# 17 "/usr/lib/clang/18/include/xopintrin.h" 3
# 1 "/usr/lib/clang/18/include/fma4intrin.h" 1 3
# 18 "/usr/lib/clang/18/include/xopintrin.h" 2 3





static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccs_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacssww((__v8hi)__A, (__v8hi)__B, (__v8hi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_macc_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsww((__v8hi)__A, (__v8hi)__B, (__v8hi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccsd_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccd_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccs_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacssdd((__v4si)__A, (__v4si)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_macc_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsdd((__v4si)__A, (__v4si)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccslo_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacssdql((__v4si)__A, (__v4si)__B, (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_macclo_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsdql((__v4si)__A, (__v4si)__B, (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccshi_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacssdqh((__v4si)__A, (__v4si)__B, (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_macchi_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsdqh((__v4si)__A, (__v4si)__B, (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maddsd_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmadcsswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maddd_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmadcswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddw_epi8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddbw((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddd_epi8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddbd((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epi8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddbq((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddd_epi16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddwd((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epi16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddwq((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epi32(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphadddq((__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddw_epu8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddubw((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddd_epu8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddubd((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epu8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddubq((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddd_epu16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphadduwd((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epu16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphadduwq((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epu32(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddudq((__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_hsubw_epi8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphsubbw((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_hsubd_epi16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphsubwd((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_hsubq_epi32(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphsubdq((__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_cmov_si128(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)(((__v2du)__A & (__v2du)__C) | ((__v2du)__B & ~(__v2du)__C));
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(256)))
_mm256_cmov_si256(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)(((__v4du)__A & (__v4du)__C) | ((__v4du)__B & ~(__v4du)__C));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_perm_epi8(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpperm((__v16qi)__A, (__v16qi)__B, (__v16qi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_rot_epi8(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vprotb((__v16qi)__A, (__v16qi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_rot_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vprotw((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_rot_epi32(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vprotd((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_rot_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vprotq((__v2di)__A, (__v2di)__B);
}
# 239 "/usr/lib/clang/18/include/xopintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_shl_epi8(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshlb((__v16qi)__A, (__v16qi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_shl_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshlw((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_shl_epi32(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshld((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_shl_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshlq((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_sha_epi8(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshab((__v16qi)__A, (__v16qi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_sha_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshaw((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_sha_epi32(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshad((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_sha_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshaq((__v2di)__A, (__v2di)__B);
}
# 328 "/usr/lib/clang/18/include/xopintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (7)));
}
# 731 "/usr/lib/clang/18/include/xopintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_frcz_ss(__m128 __A)
{
  return (__m128)__builtin_ia32_vfrczss((__v4sf)__A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_frcz_sd(__m128d __A)
{
  return (__m128d)__builtin_ia32_vfrczsd((__v2df)__A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_frcz_ps(__m128 __A)
{
  return (__m128)__builtin_ia32_vfrczps((__v4sf)__A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_frcz_pd(__m128d __A)
{
  return (__m128d)__builtin_ia32_vfrczpd((__v2df)__A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(256)))
_mm256_frcz_ps(__m256 __A)
{
  return (__m256)__builtin_ia32_vfrczps256((__v8sf)__A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(256)))
_mm256_frcz_pd(__m256d __A)
{
  return (__m256d)__builtin_ia32_vfrczpd256((__v4df)__A);
}
# 40 "/usr/lib/clang/18/include/x86intrin.h" 2 3




# 1 "/usr/lib/clang/18/include/tbmintrin.h" 1 3
# 24 "/usr/lib/clang/18/include/tbmintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcfill_u32(unsigned int __a)
{
  return __a & (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blci_u32(unsigned int __a)
{
  return __a | ~(__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcic_u32(unsigned int __a)
{
  return ~__a & (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcmsk_u32(unsigned int __a)
{
  return __a ^ (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcs_u32(unsigned int __a)
{
  return __a | (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blsfill_u32(unsigned int __a)
{
  return __a | (__a - 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blsic_u32(unsigned int __a)
{
  return ~__a | (__a - 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__t1mskc_u32(unsigned int __a)
{
  return ~__a | (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__tzmsk_u32(unsigned int __a)
{
  return ~__a & (__a - 1);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcfill_u64(unsigned long long __a)
{
  return __a & (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blci_u64(unsigned long long __a)
{
  return __a | ~(__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcic_u64(unsigned long long __a)
{
  return ~__a & (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcmsk_u64(unsigned long long __a)
{
  return __a ^ (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcs_u64(unsigned long long __a)
{
  return __a | (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blsfill_u64(unsigned long long __a)
{
  return __a | (__a - 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blsic_u64(unsigned long long __a)
{
  return ~__a | (__a - 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__t1mskc_u64(unsigned long long __a)
{
  return ~__a | (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__tzmsk_u64(unsigned long long __a)
{
  return ~__a & (__a - 1);
}
# 45 "/usr/lib/clang/18/include/x86intrin.h" 2 3




# 1 "/usr/lib/clang/18/include/lwpintrin.h" 1 3
# 31 "/usr/lib/clang/18/include/lwpintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("lwp")))
__llwpcb (void *__addr)
{
  __builtin_ia32_llwpcb(__addr);
}
# 46 "/usr/lib/clang/18/include/lwpintrin.h" 3
static __inline__ void* __attribute__((__always_inline__, __nodebug__, __target__("lwp")))
__slwpcb (void)
{
  return __builtin_ia32_slwpcb();
}
# 50 "/usr/lib/clang/18/include/x86intrin.h" 2 3




# 1 "/usr/lib/clang/18/include/mwaitxintrin.h" 1 3
# 35 "/usr/lib/clang/18/include/mwaitxintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mwaitx")))
_mm_monitorx(void * __p, unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_monitorx(__p, __extensions, __hints);
}
# 54 "/usr/lib/clang/18/include/mwaitxintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mwaitx")))
_mm_mwaitx(unsigned __extensions, unsigned __hints, unsigned __clock)
{
  __builtin_ia32_mwaitx(__extensions, __hints, __clock);
}
# 55 "/usr/lib/clang/18/include/x86intrin.h" 2 3




# 1 "/usr/lib/clang/18/include/clzerointrin.h" 1 3
# 30 "/usr/lib/clang/18/include/clzerointrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("clzero")))
_mm_clzero (void * __line)
{
  __builtin_ia32_clzero ((void *)__line);
}
# 60 "/usr/lib/clang/18/include/x86intrin.h" 2 3




# 1 "/usr/lib/clang/18/include/rdpruintrin.h" 1 3
# 30 "/usr/lib/clang/18/include/rdpruintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("rdpru")))
__rdpru (int reg_id)
{
  return __builtin_ia32_rdpru(reg_id);
}
# 65 "/usr/lib/clang/18/include/x86intrin.h" 2 3
# 2 "./dgemm.h" 2

# 1 "/usr/include/omp.h" 1 3 4
# 18 "/usr/include/omp.h" 3 4
# 1 "/usr/include/stddef.h" 1 3 4
# 42 "/usr/include/stddef.h" 3 4
typedef __ptrdiff_t ptrdiff_t;
# 67 "/usr/include/stddef.h" 3 4
typedef __max_align_t max_align_t;
# 19 "/usr/include/omp.h" 2 3 4

# 1 "/usr/include/stdint.h" 1 3 4
# 35 "/usr/include/stdint.h" 3 4
# 1 "/usr/include/machine/_stdint.h" 1 3 4




# 1 "/usr/include/x86/_stdint.h" 1 3 4
# 6 "/usr/include/machine/_stdint.h" 2 3 4
# 36 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/sys/_stdint.h" 1 3 4
# 34 "/usr/include/sys/_stdint.h" 3 4
typedef __int8_t int8_t;




typedef __int16_t int16_t;




typedef __int32_t int32_t;




typedef __int64_t int64_t;




typedef __uint8_t uint8_t;




typedef __uint16_t uint16_t;




typedef __uint32_t uint32_t;




typedef __uint64_t uint64_t;




typedef __intptr_t intptr_t;



typedef __uintptr_t uintptr_t;



typedef __intmax_t intmax_t;



typedef __uintmax_t uintmax_t;
# 37 "/usr/include/stdint.h" 2 3 4

typedef __int_least8_t int_least8_t;
typedef __int_least16_t int_least16_t;
typedef __int_least32_t int_least32_t;
typedef __int_least64_t int_least64_t;

typedef __uint_least8_t uint_least8_t;
typedef __uint_least16_t uint_least16_t;
typedef __uint_least32_t uint_least32_t;
typedef __uint_least64_t uint_least64_t;

typedef __int_fast8_t int_fast8_t;
typedef __int_fast16_t int_fast16_t;
typedef __int_fast32_t int_fast32_t;
typedef __int_fast64_t int_fast64_t;

typedef __uint_fast8_t uint_fast8_t;
typedef __uint_fast16_t uint_fast16_t;
typedef __uint_fast32_t uint_fast32_t;
typedef __uint_fast64_t uint_fast64_t;
# 21 "/usr/include/omp.h" 2 3 4
# 49 "/usr/include/omp.h" 3 4
    typedef enum omp_sched_t {
        omp_sched_static = 1,
        omp_sched_dynamic = 2,
        omp_sched_guided = 3,
        omp_sched_auto = 4,
        omp_sched_monotonic = 0x80000000
    } omp_sched_t;


    extern void omp_set_num_threads (int);
    extern void omp_set_dynamic (int);
    extern void omp_set_nested (int);
    extern void omp_set_max_active_levels (int);
    extern void omp_set_schedule (omp_sched_t, int);


    extern int omp_get_num_threads (void);
    extern int omp_get_dynamic (void);
    extern int omp_get_nested (void);
    extern int omp_get_max_threads (void);
    extern int omp_get_thread_num (void);
    extern int omp_get_num_procs (void);
    extern int omp_in_parallel (void);
    extern int omp_in_final (void);
    extern int omp_get_active_level (void);
    extern int omp_get_level (void);
    extern int omp_get_ancestor_thread_num (int);
    extern int omp_get_team_size (int);
    extern int omp_get_thread_limit (void);
    extern int omp_get_max_active_levels (void);
    extern void omp_get_schedule (omp_sched_t *, int *);
    extern int omp_get_max_task_priority (void);


    typedef struct omp_lock_t {
        void * _lk;
    } omp_lock_t;

    extern void omp_init_lock (omp_lock_t *);
    extern void omp_set_lock (omp_lock_t *);
    extern void omp_unset_lock (omp_lock_t *);
    extern void omp_destroy_lock (omp_lock_t *);
    extern int omp_test_lock (omp_lock_t *);


    typedef struct omp_nest_lock_t {
        void * _lk;
    } omp_nest_lock_t;

    extern void omp_init_nest_lock (omp_nest_lock_t *);
    extern void omp_set_nest_lock (omp_nest_lock_t *);
    extern void omp_unset_nest_lock (omp_nest_lock_t *);
    extern void omp_destroy_nest_lock (omp_nest_lock_t *);
    extern int omp_test_nest_lock (omp_nest_lock_t *);


    typedef enum omp_sync_hint_t {
        omp_sync_hint_none = 0,
        omp_lock_hint_none = omp_sync_hint_none,
        omp_sync_hint_uncontended = 1,
        omp_lock_hint_uncontended = omp_sync_hint_uncontended,
        omp_sync_hint_contended = (1<<1),
        omp_lock_hint_contended = omp_sync_hint_contended,
        omp_sync_hint_nonspeculative = (1<<2),
        omp_lock_hint_nonspeculative = omp_sync_hint_nonspeculative,
        omp_sync_hint_speculative = (1<<3),
        omp_lock_hint_speculative = omp_sync_hint_speculative,
        kmp_lock_hint_hle = (1<<16),
        kmp_lock_hint_rtm = (1<<17),
        kmp_lock_hint_adaptive = (1<<18)
    } omp_sync_hint_t;


    typedef omp_sync_hint_t omp_lock_hint_t;


    extern void omp_init_lock_with_hint(omp_lock_t *, omp_lock_hint_t);
    extern void omp_init_nest_lock_with_hint(omp_nest_lock_t *, omp_lock_hint_t);


    extern double omp_get_wtime (void);
    extern double omp_get_wtick (void);


    extern int omp_get_default_device (void);
    extern void omp_set_default_device (int);
    extern int omp_is_initial_device (void);
    extern int omp_get_num_devices (void);
    extern int omp_get_num_teams (void);
    extern int omp_get_team_num (void);
    extern int omp_get_cancellation (void);


    extern int omp_get_initial_device (void);
    extern void* omp_target_alloc(size_t, int);
    extern void omp_target_free(void *, int);
    extern int omp_target_is_present(const void *, int);
    extern int omp_target_memcpy(void *, const void *, size_t, size_t, size_t, int, int);
    extern int omp_target_memcpy_rect(void *, const void *, size_t, int, const size_t *,
                                            const size_t *, const size_t *, const size_t *, const size_t *, int, int);
    extern int omp_target_associate_ptr(const void *, const void *, size_t, size_t, int);
    extern int omp_target_disassociate_ptr(const void *, int);


    extern int omp_get_device_num (void);
    typedef void * omp_depend_t;


    typedef intptr_t omp_intptr_t;


    typedef enum omp_interop_property {
        omp_ipr_fr_id = -1,
        omp_ipr_fr_name = -2,
        omp_ipr_vendor = -3,
        omp_ipr_vendor_name = -4,
        omp_ipr_device_num = -5,
        omp_ipr_platform = -6,
        omp_ipr_device = -7,
        omp_ipr_device_context = -8,
        omp_ipr_targetsync = -9,
        omp_ipr_first = -9
    } omp_interop_property_t;



    typedef enum omp_interop_rc {
        omp_irc_no_value = 1,
        omp_irc_success = 0,
        omp_irc_empty = -1,
        omp_irc_out_of_range = -2,
        omp_irc_type_int = -3,
        omp_irc_type_ptr = -4,
        omp_irc_type_str = -5,
        omp_irc_other = -6
    } omp_interop_rc_t;

    typedef enum omp_interop_fr {
        omp_ifr_cuda = 1,
        omp_ifr_cuda_driver = 2,
        omp_ifr_opencl = 3,
        omp_ifr_sycl = 4,
        omp_ifr_hip = 5,
        omp_ifr_level_zero = 6,
        omp_ifr_last = 7
    } omp_interop_fr_t;

    typedef void * omp_interop_t;




    extern int omp_get_num_interop_properties(const omp_interop_t);



    extern omp_intptr_t omp_get_interop_int(const omp_interop_t, omp_interop_property_t, int *);



    extern void * omp_get_interop_ptr(const omp_interop_t, omp_interop_property_t, int *);



    extern const char * omp_get_interop_str(const omp_interop_t, omp_interop_property_t, int *);



    extern const char * omp_get_interop_name(const omp_interop_t, omp_interop_property_t);



    extern const char * omp_get_interop_type_desc(const omp_interop_t, omp_interop_property_t);



    extern const char * omp_get_interop_rc_desc(const omp_interop_t, omp_interop_rc_t);






    extern int omp_target_memcpy_async(void *, const void *, size_t, size_t, size_t, int,
                                             int, int, omp_depend_t *);



    extern int omp_target_memcpy_rect_async(void *, const void *, size_t, int, const size_t *,
                                             const size_t *, const size_t *, const size_t *, const size_t *, int, int,
                                             int, omp_depend_t *);


    extern void * omp_target_memset(void *, int, size_t, int);
    extern void * omp_target_memset_async(void *, int, size_t, int, int, omp_depend_t *);




    extern void * omp_get_mapped_ptr(const void *, int);
    extern int omp_target_is_accessible(const void *, size_t, int);


    extern int kmp_get_stacksize (void);
    extern void kmp_set_stacksize (int);
    extern size_t kmp_get_stacksize_s (void);
    extern void kmp_set_stacksize_s (size_t);
    extern int kmp_get_blocktime (void);
    extern int kmp_get_library (void);
    extern void kmp_set_blocktime (int);
    extern void kmp_set_library (int);
    extern void kmp_set_library_serial (void);
    extern void kmp_set_library_turnaround (void);
    extern void kmp_set_library_throughput (void);
    extern void kmp_set_defaults (char const *);
    extern void kmp_set_disp_num_buffers (int);


    typedef void * kmp_affinity_mask_t;

    extern int kmp_set_affinity (kmp_affinity_mask_t *);
    extern int kmp_get_affinity (kmp_affinity_mask_t *);
    extern int kmp_get_affinity_max_proc (void);
    extern void kmp_create_affinity_mask (kmp_affinity_mask_t *);
    extern void kmp_destroy_affinity_mask (kmp_affinity_mask_t *);
    extern int kmp_set_affinity_mask_proc (int, kmp_affinity_mask_t *);
    extern int kmp_unset_affinity_mask_proc (int, kmp_affinity_mask_t *);
    extern int kmp_get_affinity_mask_proc (int, kmp_affinity_mask_t *);


    typedef enum omp_proc_bind_t {
        omp_proc_bind_false = 0,
        omp_proc_bind_true = 1,
        omp_proc_bind_master = 2,
        omp_proc_bind_close = 3,
        omp_proc_bind_spread = 4
    } omp_proc_bind_t;

    extern omp_proc_bind_t omp_get_proc_bind (void);


    extern int omp_get_num_places (void);
    extern int omp_get_place_num_procs (int);
    extern void omp_get_place_proc_ids (int, int *);
    extern int omp_get_place_num (void);
    extern int omp_get_partition_num_places (void);
    extern void omp_get_partition_place_nums (int *);

    extern void * kmp_malloc (size_t);
    extern void * kmp_aligned_malloc (size_t, size_t);
    extern void * kmp_calloc (size_t, size_t);
    extern void * kmp_realloc (void *, size_t);
    extern void kmp_free (void *);

    extern void kmp_set_warnings_on(void);
    extern void kmp_set_warnings_off(void);


    typedef enum omp_control_tool_result_t {
        omp_control_tool_notool = -2,
        omp_control_tool_nocallback = -1,
        omp_control_tool_success = 0,
        omp_control_tool_ignored = 1
    } omp_control_tool_result_t;

    typedef enum omp_control_tool_t {
        omp_control_tool_start = 1,
        omp_control_tool_pause = 2,
        omp_control_tool_flush = 3,
        omp_control_tool_end = 4
    } omp_control_tool_t;

    extern int omp_control_tool(int, int, void*);


    typedef uintptr_t omp_uintptr_t;

    typedef enum {
        omp_atk_sync_hint = 1,
        omp_atk_alignment = 2,
        omp_atk_access = 3,
        omp_atk_pool_size = 4,
        omp_atk_fallback = 5,
        omp_atk_fb_data = 6,
        omp_atk_pinned = 7,
        omp_atk_partition = 8
    } omp_alloctrait_key_t;

    typedef enum {
        omp_atv_false = 0,
        omp_atv_true = 1,
        omp_atv_contended = 3,
        omp_atv_uncontended = 4,
        omp_atv_serialized = 5,
        omp_atv_sequential = omp_atv_serialized,
        omp_atv_private = 6,
        omp_atv_all = 7,
        omp_atv_thread = 8,
        omp_atv_pteam = 9,
        omp_atv_cgroup = 10,
        omp_atv_default_mem_fb = 11,
        omp_atv_null_fb = 12,
        omp_atv_abort_fb = 13,
        omp_atv_allocator_fb = 14,
        omp_atv_environment = 15,
        omp_atv_nearest = 16,
        omp_atv_blocked = 17,
        omp_atv_interleaved = 18
    } omp_alloctrait_value_t;


    typedef struct {
        omp_alloctrait_key_t key;
        omp_uintptr_t value;
    } omp_alloctrait_t;
# 394 "/usr/include/omp.h" 3 4
    typedef enum omp_allocator_handle_t

    {
      omp_null_allocator = 0,
      omp_default_mem_alloc = 1,
      omp_large_cap_mem_alloc = 2,
      omp_const_mem_alloc = 3,
      omp_high_bw_mem_alloc = 4,
      omp_low_lat_mem_alloc = 5,
      omp_cgroup_mem_alloc = 6,
      omp_pteam_mem_alloc = 7,
      omp_thread_mem_alloc = 8,
      llvm_omp_target_host_mem_alloc = 100,
      llvm_omp_target_shared_mem_alloc = 101,
      llvm_omp_target_device_mem_alloc = 102,
      KMP_ALLOCATOR_MAX_HANDLE = 0xffffffffffffffff
    } omp_allocator_handle_t;



    typedef enum omp_memspace_handle_t

    {
      omp_default_mem_space = 0,
      omp_large_cap_mem_space = 1,
      omp_const_mem_space = 2,
      omp_high_bw_mem_space = 3,
      omp_low_lat_mem_space = 4,
      llvm_omp_target_host_mem_space = 100,
      llvm_omp_target_shared_mem_space = 101,
      llvm_omp_target_device_mem_space = 102,
      KMP_MEMSPACE_MAX_HANDLE = 0xffffffffffffffff
    } omp_memspace_handle_t;

    extern omp_allocator_handle_t omp_init_allocator(omp_memspace_handle_t m,
                                                       int ntraits, omp_alloctrait_t traits[]);
    extern void omp_destroy_allocator(omp_allocator_handle_t allocator);

    extern void omp_set_default_allocator(omp_allocator_handle_t a);
    extern omp_allocator_handle_t omp_get_default_allocator(void);
# 447 "/usr/include/omp.h" 3 4
    extern void * omp_alloc(size_t size, omp_allocator_handle_t a);
    extern void * omp_aligned_alloc(size_t align, size_t size,
                                                         omp_allocator_handle_t a);
    extern void * omp_calloc(size_t nmemb, size_t size, omp_allocator_handle_t a);
    extern void * omp_aligned_calloc(size_t align, size_t nmemb, size_t size,
                                                          omp_allocator_handle_t a);
    extern void * omp_realloc(void *ptr, size_t size, omp_allocator_handle_t allocator,
                                                   omp_allocator_handle_t free_allocator);
    extern void omp_free(void *ptr, omp_allocator_handle_t a);



    extern void ompc_set_affinity_format(char const *);
    extern size_t ompc_get_affinity_format(char *, size_t);
    extern void ompc_display_affinity(char const *);
    extern size_t ompc_capture_affinity(char *, size_t, char const *);






    typedef enum omp_event_handle_t { KMP_EVENT_MAX_HANDLE = 0xffffffffffffffff } omp_event_handle_t;

    extern void omp_fulfill_event ( omp_event_handle_t event );


    typedef enum omp_pause_resource_t {
      omp_pause_resume = 0,
      omp_pause_soft = 1,
      omp_pause_hard = 2
    } omp_pause_resource_t;
    extern int omp_pause_resource(omp_pause_resource_t, int);
    extern int omp_pause_resource_all(omp_pause_resource_t);

    extern int omp_get_supported_active_levels(void);


    extern void omp_set_num_teams(int num_teams);
    extern int omp_get_max_teams(void);
    extern void omp_set_teams_thread_limit(int limit);
    extern int omp_get_teams_thread_limit(void);


    extern void omp_display_env(int verbose);


#pragma omp begin declare variant match(device={kind(host)})
    static inline int omp_is_initial_device(void) { return 1; }
#pragma omp end declare variant
#pragma omp begin declare variant match(device={kind(nohost)})
    static inline int omp_is_initial_device(void) { return 0; }
#pragma omp end declare variant



    extern int omp_in_explicit_task(void);


    extern void *llvm_omp_target_dynamic_shared_alloc(void);







    typedef int omp_int_t;
    typedef double omp_wtime_t;
# 4 "./dgemm.h" 2






void mm2d(double *c[], double *a[], double *b[]);
void mm(int n, double* a, double* b, double* c);
void mmasmu(int n, double* a, double* b, double* c);
void mmasmu2(double* a, double* b, double* c);
void mmasm(int n, double* a, double* b, double* c);
void mmasmlu(int n, double* a, double* b, double* c);
void block(int n, int si, int sj, int sk, double* a, double* b, double* c);
void mmcb(int n, double* a, double* b, double* c);
void mmomp(int n, double* a, double* b, double* c);
# 2 "mmomp.c" 2



void mmomp(int n, double* a, double* b, double* c)
{
#pragma omp parallel for
   for (int i=0; i<n; i+=(32))
   for (int j=0; j<n; j+=(32))
   for (int k=0; k<n; k+=(32))
      block(n, i, j, k, a, b, c);
}
